{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f33954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import gc\n",
    "from typing import List, Dict, Set, Tuple, Any, Optional, Union\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from rapidfuzz import fuzz\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import lru_cache\n",
    "from datetime import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1ee9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPU/CPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Initialize SpaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    print(\"Downloading SpaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_lg\")\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "#st_model = SentenceTransformer(\"nasa-impact/nasa-ibm-st.38m\")\n",
    "st_model = SentenceTransformer(\"nasa-impact/nasa-smd-ibm-st-v2\")\n",
    "st_model.eval()\n",
    "st_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Memory management\n",
    "def optimize_memory():\n",
    "    # Free up memory resources\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a525fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScienceClassifier:\n",
    "    # Science document classifier with model loading and caching\n",
    "    _instance = None\n",
    "\n",
    "    @classmethod\n",
    "    def get_instance(cls):\n",
    "        # Get or create singleton instance\n",
    "        if cls._instance is None:\n",
    "            cls._instance = cls()\n",
    "        return cls._instance\n",
    "\n",
    "    def __init__(self):\n",
    "        # Model configurations\n",
    "        self.models = {\n",
    "            \"research_area\": {\n",
    "                \"name\": \"arminmehrabian/nasa-impact-nasa-smd-ibm-st-v2-classification-finetuned\",\n",
    "                \"label_map\": {\n",
    "                    0: \"Agriculture\",\n",
    "                    1: \"Air Quality\",\n",
    "                    2: \"Atmospheric/Ocean Indicators\",\n",
    "                    3: \"Cryospheric Indicators\",\n",
    "                    4: \"Droughts\",\n",
    "                    5: \"Earthquakes\",\n",
    "                    6: \"Ecosystems\",\n",
    "                    7: \"Energy Production/Use\",\n",
    "                    8: \"Environmental Impacts\",\n",
    "                    9: \"Floods\",\n",
    "                    10: \"Greenhouse Gases\",\n",
    "                    11: \"Habitat Conversion/Fragmentation\",\n",
    "                    12: \"Heat\",\n",
    "                    13: \"Land Surface/Agriculture Indicators\",\n",
    "                    14: \"Public Health\",\n",
    "                    15: \"Severe Storms\",\n",
    "                    16: \"Sun-Earth Interactions\",\n",
    "                    17: \"Validation\",\n",
    "                    18: \"Volcanic Eruptions\",\n",
    "                    19: \"Water Quality\",\n",
    "                    20: \"Wildfires\",\n",
    "                },\n",
    "            },\n",
    "            \"science_keywords\": {\"name\": \"nasa-impact/science-keyword-classification\"},\n",
    "            \"division\": {\n",
    "                \"name\": \"nasa-impact/division-classifier\",\n",
    "                \"label_map\": {\n",
    "                    0: \"Astrophysics\",\n",
    "                    1: \"Biological and Physical Sciences\",\n",
    "                    2: \"Earth Science\",\n",
    "                    3: \"Heliophysics\",\n",
    "                    4: \"Planetary Science\",\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        self.classification_cache = {}\n",
    "        self._load_models()\n",
    "\n",
    "    def _load_models(self):\n",
    "        # Load classification models\n",
    "        self.classifiers = {}\n",
    "        for task, config in self.models.items():\n",
    "            try:\n",
    "                print(f\"Loading {task} model from {config['name']}...\")\n",
    "                with torch.no_grad():\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(config[\"name\"])\n",
    "                    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                        config[\"name\"]\n",
    "                    )\n",
    "                    model.eval()\n",
    "\n",
    "                self.classifiers[task] = {\n",
    "                    \"pipe\": pipeline(\n",
    "                        \"text-classification\",\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        device=device,\n",
    "                        batch_size=32,\n",
    "                    ),\n",
    "                    \"config\": config,\n",
    "                }\n",
    "                print(f\"Successfully loaded {task} model\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {task} model: {str(e)}\")\n",
    "\n",
    "    def _prepare_text(self, publication: Dict) -> str:\n",
    "        # Extract and combine text from publication for classification\n",
    "        # Extract title\n",
    "        title = \"\"\n",
    "        if \"title\" in publication:\n",
    "            if isinstance(publication[\"title\"], list) and publication[\"title\"]:\n",
    "                title = publication[\"title\"][0]\n",
    "            elif isinstance(publication[\"title\"], str):\n",
    "                title = publication[\"title\"]\n",
    "\n",
    "        abstract = publication.get(\"abstract\", \"\")\n",
    "        keywords = \" \".join(publication.get(\"keywords\", []))\n",
    "        return \" \".join([title, abstract, keywords])\n",
    "\n",
    "    def _get_cache_key(self, publication: Dict) -> str:\n",
    "        # Generate caches for publication\n",
    "        if \"DOI\" in publication and publication[\"DOI\"]:\n",
    "            return f\"doi:{publication['DOI']}\"\n",
    "\n",
    "        title = \"\"\n",
    "        if \"title\" in publication:\n",
    "            if isinstance(publication[\"title\"], list) and publication[\"title\"]:\n",
    "                title = publication[\"title\"][0]\n",
    "            elif isinstance(publication[\"title\"], str):\n",
    "                title = publication.get(\"title\", \"\")\n",
    "\n",
    "        return f\"title:{title}\"\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def classify(self, publication: Dict) -> Dict:\n",
    "        # Run classification on publications with caching\n",
    "        # Check cache\n",
    "        cache_key = self._get_cache_key(publication)\n",
    "        if cache_key in self.classification_cache:\n",
    "            return self.classification_cache[cache_key]\n",
    "\n",
    "        text = self._prepare_text(publication)\n",
    "        results = {\"research_areas\": [], \"science_keywords\": [], \"division\": None}\n",
    "\n",
    "        # Research Area Classification\n",
    "        if self.classifiers.get(\"research_area\"):\n",
    "            try:\n",
    "                res_area = self.classifiers[\"research_area\"][\"pipe\"](\n",
    "                    text, top_k=3, truncation=True, max_length=512\n",
    "                )\n",
    "                results[\"research_areas\"] = [\n",
    "                    {\n",
    "                        \"label\": self.models[\"research_area\"][\"label_map\"][\n",
    "                            int(pred[\"label\"].replace(\"LABEL_\", \"\"))\n",
    "                        ],\n",
    "                        \"score\": float(pred[\"score\"]),\n",
    "                    }\n",
    "                    for pred in res_area\n",
    "                ]\n",
    "            except Exception as e:\n",
    "                print(f\"Research area classification failed: {str(e)}\")\n",
    "\n",
    "        # Science Keywords Classification\n",
    "        if self.classifiers.get(\"science_keywords\"):\n",
    "            try:\n",
    "                science_keywords = self.classifiers[\"science_keywords\"][\"pipe\"](\n",
    "                    text, truncation=True, max_length=512, top_k=10\n",
    "                )\n",
    "\n",
    "                for pred in science_keywords:\n",
    "                    if pred[\"score\"] > 0.35 and len(pred[\"label\"]) >= 4:\n",
    "                        results[\"science_keywords\"].append(\n",
    "                            {\"label\": pred[\"label\"], \"score\": float(pred[\"score\"])}\n",
    "                        )\n",
    "            except Exception as e:\n",
    "                print(f\"Science keyword classification failed: {str(e)}\")\n",
    "\n",
    "        # Division Classification\n",
    "        if self.classifiers.get(\"division\"):\n",
    "            try:\n",
    "                division_result = self.classifiers[\"division\"][\"pipe\"](\n",
    "                    text, top_k=1, truncation=True, max_length=512\n",
    "                )\n",
    "\n",
    "                if division_result:\n",
    "                    division = division_result[0]\n",
    "                    if \"score\" in division:\n",
    "                        results[\"division\"] = {\n",
    "                            \"label\": division[\"label\"],\n",
    "                            \"score\": float(division[\"score\"]),\n",
    "                        }\n",
    "            except Exception as e:\n",
    "                print(f\"Division classification failed: {str(e)}\")\n",
    "\n",
    "        # Cache results\n",
    "        self.classification_cache[cache_key] = results\n",
    "        return results\n",
    "\n",
    "\n",
    "class ModelContextManager:\n",
    "    # Context validation with model profiles\n",
    "\n",
    "    def __init__(self, curated_publications: List[Dict]):\n",
    "        self.st_model = st_model\n",
    "        self.model_profiles = self._build_model_profiles(curated_publications)\n",
    "        self.profile_cache = {}\n",
    "        self.corpus_term_frequencies = self._build_corpus_term_frequencies(\n",
    "            curated_publications\n",
    "        )\n",
    "        self.model_tfidf_terms = self._calculate_model_tfidf_terms(curated_publications)\n",
    "\n",
    "    def _build_model_profiles(self, publications: List[Dict]) -> Dict[str, Dict]:\n",
    "        # Build model context profiles from curated publications\n",
    "        model_texts = defaultdict(list)\n",
    "        model_terms = defaultdict(set)\n",
    "\n",
    "        # Collect texts and terms\n",
    "        for pub in publications:\n",
    "            model = pub.get(\"model\")\n",
    "            if model:\n",
    "                prepared_text = ScienceClassifier.get_instance()._prepare_text(pub)\n",
    "                model_texts[model].append(prepared_text)\n",
    "                model_terms[model].update(self._extract_key_terms(prepared_text))\n",
    "\n",
    "        # Create embeddings\n",
    "        final_profiles = {}\n",
    "        for model, texts in model_texts.items():\n",
    "            aggregated_text = \" \".join(texts[: min(100, len(texts))])\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                embedding = self.st_model.encode(\n",
    "                    aggregated_text, convert_to_tensor=True\n",
    "                )\n",
    "\n",
    "            final_profiles[model] = {\n",
    "                \"embedding\": embedding.cpu().numpy(),\n",
    "                \"terms\": set(sorted(model_terms[model], key=lambda x: -len(x))[:25]),\n",
    "                \"text_count\": len(texts),\n",
    "            }\n",
    "\n",
    "        optimize_memory()\n",
    "        return final_profiles\n",
    "\n",
    "    def _build_corpus_term_frequencies(\n",
    "        self, publications: List[Dict]\n",
    "    ) -> Dict[str, int]:\n",
    "        # Build term frequency dictionary for entire corpus\n",
    "        corpus_terms = Counter()\n",
    "\n",
    "        for pub in publications:\n",
    "            prepared_text = ScienceClassifier.get_instance()._prepare_text(pub)\n",
    "            words = re.findall(r\"\\b[a-z]{4,}\\b\", prepared_text.lower())\n",
    "            corpus_terms.update(words)\n",
    "\n",
    "        return corpus_terms\n",
    "\n",
    "    def _calculate_model_tfidf_terms(\n",
    "        self, publications: List[Dict]\n",
    "    ) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        # Calculate TF-IDF terms for each model\n",
    "        model_term_counts = defaultdict(Counter)\n",
    "        model_doc_counts = defaultdict(int)\n",
    "\n",
    "        # Count terms by model\n",
    "        for pub in publications:\n",
    "            model = pub.get(\"model\")\n",
    "            if model:\n",
    "                model_doc_counts[model] += 1\n",
    "                prepared_text = ScienceClassifier.get_instance()._prepare_text(pub)\n",
    "                words = re.findall(r\"\\b[a-z]{4,}\\b\", prepared_text.lower())\n",
    "                model_term_counts[model].update(words)\n",
    "\n",
    "        # Calculate total document count\n",
    "        total_docs = sum(model_doc_counts.values())\n",
    "\n",
    "        # Calculate TF-IDF for each term in each model\n",
    "        model_tfidf_terms = {}\n",
    "        for model, term_counts in model_term_counts.items():\n",
    "            tfidf_scores = {}\n",
    "            model_doc_count = model_doc_counts[model]\n",
    "\n",
    "            for term, count in term_counts.items():\n",
    "                # Term frequency in this model\n",
    "                tf = count / sum(term_counts.values())\n",
    "\n",
    "                # Inverse document frequency (add 1 to avoid division by zero)\n",
    "                term_doc_count = sum(\n",
    "                    1 for m, tc in model_term_counts.items() if term in tc\n",
    "                )\n",
    "                idf = np.log((total_docs + 1) / (term_doc_count + 1))\n",
    "\n",
    "                # TF-IDF score\n",
    "                tfidf = tf * idf\n",
    "\n",
    "                # Only keep terms with sufficient frequency and length\n",
    "                if count >= 3 and len(term) >= 4:\n",
    "                    tfidf_scores[term] = tfidf\n",
    "\n",
    "            # Sort by TF-IDF score and take top terms\n",
    "            sorted_terms = sorted(\n",
    "                tfidf_scores.items(), key=lambda x: x[1], reverse=True\n",
    "            )\n",
    "            model_tfidf_terms[model] = sorted_terms[\n",
    "                :30\n",
    "            ]  # Keep top 30 distinctive terms\n",
    "\n",
    "        return model_tfidf_terms\n",
    "\n",
    "    def _extract_key_terms(self, text: str) -> Set[str]:\n",
    "        # Extract key terms from text\n",
    "        words = re.findall(r\"\\b[a-z]{4,}\\b\", text.lower())\n",
    "        word_counts = Counter(words)\n",
    "        return {word for word, count in word_counts.items() if count >= 2}\n",
    "\n",
    "    @lru_cache(maxsize=5000)\n",
    "    def _get_pub_profile(self, prepared_text: str) -> Dict:\n",
    "        # Get publication profile with caching\n",
    "        if prepared_text in self.profile_cache:\n",
    "            return self.profile_cache[prepared_text]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            embedding = self.st_model.encode(prepared_text)\n",
    "\n",
    "        profile = {\n",
    "            \"embedding\": embedding,\n",
    "            \"terms\": self._extract_key_terms(prepared_text),\n",
    "        }\n",
    "\n",
    "        self.profile_cache[prepared_text] = profile\n",
    "        return profile\n",
    "\n",
    "    def get_model_specific_terms(self) -> Dict[str, List[str]]:\n",
    "        # Return model-specific terminology based on TF-IDF analysis\n",
    "        model_terms = {}\n",
    "        for model, tfidf_terms in self.model_tfidf_terms.items():\n",
    "            model_terms[model] = [term for term, score in tfidf_terms]\n",
    "        return model_terms\n",
    "\n",
    "    def get_context_scores(self, publication: Dict) -> Dict[str, float]:\n",
    "        # Get context validation scores\n",
    "        prepared_text = ScienceClassifier.get_instance()._prepare_text(publication)\n",
    "        pub_profile = self._get_pub_profile(prepared_text)\n",
    "\n",
    "        scores = {}\n",
    "        for model, model_profile in self.model_profiles.items():\n",
    "            # Basic term overlap score\n",
    "            pub_terms = pub_profile[\"terms\"]\n",
    "            model_terms = model_profile[\"terms\"]\n",
    "\n",
    "            if not pub_terms or not model_terms:\n",
    "                term_overlap = 0.0\n",
    "            else:\n",
    "                intersection = len(pub_terms.intersection(model_terms))\n",
    "                union = len(pub_terms.union(model_terms))\n",
    "                term_overlap = intersection / union if union > 0 else 0.0\n",
    "\n",
    "            # TF-IDF term match score\n",
    "            model_tfidf_terms = self.model_tfidf_terms.get(model, [])\n",
    "            tfidf_term_set = {term for term, score in model_tfidf_terms}\n",
    "            tfidf_match_count = len(pub_terms.intersection(tfidf_term_set))\n",
    "            tfidf_match_score = (\n",
    "                min(1.0, tfidf_match_count / 5) if tfidf_term_set else 0.0\n",
    "            )\n",
    "\n",
    "            # Semantic similarity\n",
    "            semantic_sim = cosine_similarity(\n",
    "                [pub_profile[\"embedding\"]], [model_profile[\"embedding\"]]\n",
    "            )[0][0]\n",
    "\n",
    "            # Combined score (with TF-IDF term matches)\n",
    "            scores[model] = (\n",
    "                0.5 * semantic_sim + 0.3 * term_overlap + 0.2 * tfidf_match_score\n",
    "            )\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class RelevanceRanker:\n",
    "    # Rank publications by relevance to models\n",
    "\n",
    "    def __init__(self, model_descriptions: Dict[str, str]):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_descriptions = model_descriptions\n",
    "        self.description_cache = {}\n",
    "        self.result_cache = {}\n",
    "\n",
    "        # Load model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"nasa-impact/nasa-smd-ibm-ranker\"\n",
    "        )\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"nasa-impact/nasa-smd-ibm-ranker\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            if self.device.type != \"cuda\":\n",
    "                self.model = self.model.float()\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def _safe_prepare_model_text(self, model_id: str) -> str:\n",
    "        # Prepare model text with caching\n",
    "        if model_id not in self.description_cache:\n",
    "            desc = self.model_descriptions.get(model_id, \"\")[:380]\n",
    "            self.description_cache[model_id] = re.sub(r\"\\s+\", \" \", desc)\n",
    "        return self.description_cache[model_id]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def batch_rank(self, query: str, model_ids: List[str]) -> Dict[str, float]:\n",
    "        # Rank models by relevance to query\n",
    "        if not model_ids:\n",
    "            return {}\n",
    "\n",
    "        # Check cache\n",
    "        cache_key = f\"{hash(query)}_{hash(tuple(sorted(model_ids)))}\"\n",
    "        if cache_key in self.result_cache:\n",
    "            return self.result_cache[cache_key]\n",
    "\n",
    "        query = query[:400]  # Truncate query\n",
    "        batch_texts = [self._safe_prepare_model_text(mid) for mid in model_ids]\n",
    "\n",
    "        try:\n",
    "            # Prepare inputs\n",
    "            inputs = self.tokenizer(\n",
    "                [query] * len(batch_texts),\n",
    "                batch_texts,\n",
    "                padding=\"longest\" if self.device.type == \"cuda\" else True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "\n",
    "            outputs = self.model(**inputs)\n",
    "            scores = F.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()\n",
    "            result = dict(zip(model_ids, scores.tolist()))\n",
    "\n",
    "            # Cache results\n",
    "            self.result_cache[cache_key] = result\n",
    "            return result\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Ranker fallback to CPU: {str(e)}\")\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.model = self.model.to(self.device).float()\n",
    "            return self.batch_rank(query, model_ids)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Ranker failed: {str(e)}\")\n",
    "            return {mid: 0.0 for mid in model_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdbafc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiled regex patterns for efficiency\n",
    "HTML_TAG_PATTERN = re.compile(r\"<[^>]+>\")\n",
    "HYPHEN_UNDERSCORE_PATTERN = re.compile(r\"[-_]\")\n",
    "SPECIAL_CHAR_PATTERN = re.compile(r\"[^a-zA-Z0-9]\")\n",
    "\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # lean and normalize text for matching\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = HTML_TAG_PATTERN.sub(\" \", text)\n",
    "    text = HYPHEN_UNDERSCORE_PATTERN.sub(\" \", text)\n",
    "    return SPECIAL_CHAR_PATTERN.sub(\" \", text).lower()\n",
    "\n",
    "\n",
    "# DOI normalization cache\n",
    "_doi_cache = {}\n",
    "\n",
    "\n",
    "def normalize_doi(doi: str) -> str:\n",
    "    # Normalize DOI strings\n",
    "    if not doi:\n",
    "        return \"\"\n",
    "\n",
    "    if doi in _doi_cache:\n",
    "        return _doi_cache[doi]\n",
    "\n",
    "    doi = doi.lower().replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    if \"doi.org/\" in doi:\n",
    "        doi = doi.replace(\"doi.org/\", \"\")\n",
    "    doi = doi.replace(\",\", \".\")\n",
    "    result = doi.strip(\"/ \\n\\r\\t\")\n",
    "\n",
    "    _doi_cache[doi] = result\n",
    "    return result\n",
    "\n",
    "\n",
    "# Fuzzy matching cache\n",
    "_fuzzy_match_cache = {}\n",
    "\n",
    "\n",
    "def fuzzy_keyword_match(\n",
    "    text: str, keyword: str, threshold: float = 90.0\n",
    ") -> Tuple[bool, float]:\n",
    "    # Fuzzy keyword matching\n",
    "    cache_key = f\"{hash(text)}_{keyword}_{threshold}\"\n",
    "    if cache_key in _fuzzy_match_cache:\n",
    "        return _fuzzy_match_cache[cache_key]\n",
    "\n",
    "    # Check for exact match first\n",
    "    keyword_lower = keyword.lower()\n",
    "    if keyword_lower in text.lower():\n",
    "        result = (True, 100.0)\n",
    "        _fuzzy_match_cache[cache_key] = result\n",
    "        return result\n",
    "\n",
    "    # Use direct fuzzy matching for shorter texts\n",
    "    if len(text) < 1000:\n",
    "        score = fuzz.token_sort_ratio(text.lower(), keyword_lower)\n",
    "        result = (score >= threshold, score)\n",
    "        _fuzzy_match_cache[cache_key] = result\n",
    "        return result\n",
    "\n",
    "    # For longer texts, use spaCy\n",
    "    doc = nlp(text[:2000])\n",
    "\n",
    "    # Process relevant parts\n",
    "    chunks = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "    entities = [ent.text.lower() for ent in doc.ents]\n",
    "    tokens = [\n",
    "        token.text.lower()\n",
    "        for token in doc\n",
    "        if token.is_alpha and not token.is_stop and len(token.text) > 3\n",
    "    ]\n",
    "\n",
    "    all_spans = chunks[:50] + entities[:50] + tokens[:100]\n",
    "\n",
    "    # Find best match\n",
    "    best_score = 0.0\n",
    "    for span in all_spans:\n",
    "        if len(span) < 3:\n",
    "            continue\n",
    "        score = fuzz.token_sort_ratio(span, keyword_lower)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "\n",
    "    result = (best_score >= threshold, best_score)\n",
    "    _fuzzy_match_cache[cache_key] = result\n",
    "    return result\n",
    "\n",
    "\n",
    "# Publication date extraction function\n",
    "def extract_publication_date(publication: Dict) -> Optional[str]:\n",
    "    # Extract publication date in yyyy-mm format\n",
    "    try:\n",
    "        # Try to get from 'published' field first\n",
    "        if \"published\" in publication and \"date-parts\" in publication[\"published\"]:\n",
    "            date_parts = publication[\"published\"][\"date-parts\"][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "\n",
    "        # Try published-online\n",
    "        if (\n",
    "            \"published-online\" in publication\n",
    "            and \"date-parts\" in publication[\"published-online\"]\n",
    "        ):\n",
    "            date_parts = publication[\"published-online\"][\"date-parts\"][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "\n",
    "        # Try published-print\n",
    "        if (\n",
    "            \"published-print\" in publication\n",
    "            and \"date-parts\" in publication[\"published-print\"]\n",
    "        ):\n",
    "            date_parts = publication[\"published-print\"][\"date-parts\"][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "\n",
    "        # Try indexed\n",
    "        if \"indexed\" in publication and \"date-parts\" in publication[\"indexed\"]:\n",
    "            date_parts = publication[\"indexed\"][\"date-parts\"][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "\n",
    "        # Try created\n",
    "        if \"created\" in publication and \"date-parts\" in publication[\"created\"]:\n",
    "            date_parts = publication[\"created\"][\"date-parts\"][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "\n",
    "        # Try to parse issue date if year is present\n",
    "        if \"issued\" in publication and \"date-parts\" in publication[\"issued\"]:\n",
    "            date_parts = publication[\"issued\"][\"date-parts\"][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "            elif len(date_parts) >= 1:\n",
    "                return f\"{date_parts[0]:04d}-01\"  # Default to January if only year\n",
    "\n",
    "        # Try publication year\n",
    "        if \"year\" in publication:\n",
    "            return f\"{publication['year']:04d}-01\"  # Default to January\n",
    "\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting publication date: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Publication characteristics cache\n",
    "_characteristics_cache = {}\n",
    "\n",
    "\n",
    "def extract_publication_characteristics(publication: Dict) -> Dict[str, Any]:\n",
    "    # Extract publication characteristics with caching\n",
    "    cache_key = str(hash(json.dumps(publication, sort_keys=True)[:1000]))\n",
    "    if cache_key in _characteristics_cache:\n",
    "        return _characteristics_cache[cache_key]\n",
    "\n",
    "    characteristics = {}\n",
    "\n",
    "    # Extract title\n",
    "    title = \"\"\n",
    "    if \"title\" in publication:\n",
    "        if isinstance(publication[\"title\"], list) and publication[\"title\"]:\n",
    "            title = publication[\"title\"][0]\n",
    "        else:\n",
    "            title = publication.get(\"title\", \"\")\n",
    "\n",
    "    # Extract abstract\n",
    "    abstract = publication.get(\"abstract\", \"\")\n",
    "\n",
    "    # Basic metrics\n",
    "    characteristics[\"title_length\"] = len(title.split())\n",
    "    characteristics[\"abstract_length\"] = len(abstract.split())\n",
    "    characteristics[\"total_length\"] = (\n",
    "        characteristics[\"title_length\"] + characteristics[\"abstract_length\"]\n",
    "    )\n",
    "\n",
    "    # Keywords metrics\n",
    "    keywords = publication.get(\"keywords\", [])\n",
    "    characteristics[\"keyword_count\"] = len(keywords)\n",
    "\n",
    "    # Process with spaCy if abstract is present\n",
    "    if abstract and len(abstract) > 10 and len(abstract) < 10000:\n",
    "        doc = nlp(abstract[:2000])\n",
    "\n",
    "        # Part-of-speech distributions\n",
    "        pos_counts = Counter([token.pos_ for token in doc])\n",
    "        doc_len = len(doc) or 1\n",
    "\n",
    "        characteristics[\"noun_ratio\"] = pos_counts.get(\"NOUN\", 0) / doc_len\n",
    "        characteristics[\"verb_ratio\"] = pos_counts.get(\"VERB\", 0) / doc_len\n",
    "        characteristics[\"adj_ratio\"] = pos_counts.get(\"ADJ\", 0) / doc_len\n",
    "\n",
    "        # Named entity analysis\n",
    "        entity_types = [ent.label_ for ent in doc.ents]\n",
    "        entity_counter = Counter(entity_types)\n",
    "\n",
    "        characteristics[\"entity_count\"] = len(doc.ents)\n",
    "        characteristics[\"org_count\"] = entity_counter.get(\"ORG\", 0)\n",
    "        characteristics[\"person_count\"] = entity_counter.get(\"PERSON\", 0)\n",
    "        characteristics[\"date_count\"] = entity_counter.get(\"DATE\", 0)\n",
    "\n",
    "        # Readability metrics\n",
    "        sentences = list(doc.sents)\n",
    "        if sentences:\n",
    "            sent_lengths = [len(sent) for sent in sentences]\n",
    "            characteristics[\"avg_sentence_length\"] = np.mean(sent_lengths)\n",
    "            characteristics[\"sentence_count\"] = len(sentences)\n",
    "        else:\n",
    "            characteristics[\"avg_sentence_length\"] = 0\n",
    "            characteristics[\"sentence_count\"] = 0\n",
    "\n",
    "    # Publication year\n",
    "    if \"year\" in publication:\n",
    "        characteristics[\"year\"] = publication.get(\"year\")\n",
    "\n",
    "    # Author metrics\n",
    "    if \"authors\" in publication:\n",
    "        authors = publication.get(\"authors\", [])\n",
    "        if isinstance(authors, list):\n",
    "            characteristics[\"author_count\"] = len(authors)\n",
    "        else:\n",
    "            characteristics[\"author_count\"] = 1 if authors else 0\n",
    "\n",
    "    _characteristics_cache[cache_key] = characteristics\n",
    "    return characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f262c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_embeddings_cache = {}\n",
    "_curated_models_cache = {}\n",
    "_model_keywords_cache = {}\n",
    "_model_descriptions_cache = {}\n",
    "\n",
    "\n",
    "def initialize_model_embeddings(\n",
    "    model_descriptions: Dict[str, str],\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    # Create embeddings for model descriptions with batch processing\n",
    "    if _model_embeddings_cache and len(_model_embeddings_cache) == len(\n",
    "        model_descriptions\n",
    "    ):\n",
    "        return _model_embeddings_cache\n",
    "\n",
    "    embeddings = {}\n",
    "    batch_size = 32\n",
    "\n",
    "    model_list = list(model_descriptions.items())\n",
    "\n",
    "    for i in range(0, len(model_list), batch_size):\n",
    "        batch = model_list[i : i + batch_size]\n",
    "        models, texts = zip(*batch)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            batch_embeddings = st_model.encode(list(texts), convert_to_tensor=False)\n",
    "\n",
    "        for j, model in enumerate(models):\n",
    "            embeddings[model] = batch_embeddings[j]\n",
    "\n",
    "    _model_embeddings_cache.update(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_curated_models(curated_path: str) -> Dict[str, str]:\n",
    "    # Load curated DOI-model mappings\n",
    "    if curated_path in _curated_models_cache:\n",
    "        return _curated_models_cache[curated_path]\n",
    "\n",
    "    try:\n",
    "        with open(curated_path) as f:\n",
    "            curated = json.load(f)\n",
    "\n",
    "        mapping = {}\n",
    "        for entry in curated:\n",
    "            if \"doi\" in entry and \"model\" in entry:\n",
    "                normalized_doi = normalize_doi(entry[\"doi\"])\n",
    "                if normalized_doi:\n",
    "                    mapping[normalized_doi] = entry[\"model\"]\n",
    "\n",
    "        _curated_models_cache[curated_path] = mapping\n",
    "        return mapping\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading curated models from {curated_path}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def load_model_keywords(keywords_path: str) -> Dict[str, List[str]]:\n",
    "    # Load model keywords\n",
    "    if keywords_path in _model_keywords_cache:\n",
    "        return _model_keywords_cache[keywords_path]\n",
    "\n",
    "    try:\n",
    "        with open(keywords_path) as f:\n",
    "            keywords = json.load(f)\n",
    "\n",
    "        _model_keywords_cache[keywords_path] = keywords\n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model keywords from {keywords_path}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def load_model_descriptions(descriptions_path: str) -> Dict[str, str]:\n",
    "    # Load model descriptions\n",
    "    if descriptions_path in _model_descriptions_cache:\n",
    "        return _model_descriptions_cache[descriptions_path]\n",
    "\n",
    "    try:\n",
    "        with open(descriptions_path) as f:\n",
    "            descriptions = json.load(f)\n",
    "\n",
    "        _model_descriptions_cache[descriptions_path] = descriptions\n",
    "        return descriptions\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model descriptions from {descriptions_path}: {str(e)}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56159f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model thresholds - optimized based on F1 score\n",
    "MODEL_THRESHOLDS = {\n",
    "    \"ECCO\": 0.10,\n",
    "    \"RAPID\": 0.50,\n",
    "    \"ISSM\": 0.40,\n",
    "    \"CMS-Flux\": 0.75,\n",
    "    \"CARDAMOM\": 0.70,\n",
    "    \"MOMO-CHEM\": 0.95,\n",
    "}\n",
    "\n",
    "\n",
    "# Affinity caches\n",
    "_keyword_affinity_cache = None\n",
    "_research_area_affinity_cache = None\n",
    "_division_affinity_cache = None\n",
    "\n",
    "\n",
    "def derive_data_driven_affinities(curated_publications_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Derives data-driven affinities from statistical analysis of co-occurrence patterns\n",
    "    in the curated dataset. For each research area, science keyword, and division,\n",
    "    this function calculates normalized frequency distributions across models\n",
    "    and transforms these distributions into affinity multipliers.\n",
    "\n",
    "    Args:\n",
    "        curated_publications_path: Path to the curated publications JSON file\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing science keyword to model affinities\n",
    "    \"\"\"\n",
    "    print(\"Deriving data-driven affinities from curated dataset...\")\n",
    "\n",
    "    try:\n",
    "        # Load curated publications\n",
    "        with open(curated_publications_path, \"r\") as f:\n",
    "            curated_publications = json.load(f)\n",
    "\n",
    "        # Initialize science classifier\n",
    "        science_classifier = ScienceClassifier.get_instance()\n",
    "\n",
    "        # Count model occurrences and classifications\n",
    "        model_counts = Counter()\n",
    "        model_keyword_counts = defaultdict(Counter)\n",
    "        model_research_area_counts = defaultdict(Counter)\n",
    "        model_division_counts = defaultdict(Counter)\n",
    "\n",
    "        # Analyze each publication\n",
    "        for publication in curated_publications:\n",
    "            model = publication.get(\"model\")\n",
    "            if not model:\n",
    "                continue\n",
    "\n",
    "            # Count model occurrences\n",
    "            model_counts[model] += 1\n",
    "\n",
    "            # Get science classifications\n",
    "            classifications = science_classifier.classify(publication)\n",
    "\n",
    "            # Extract science keywords\n",
    "            for keyword_entry in classifications.get(\"science_keywords\", []):\n",
    "                keyword = keyword_entry.get(\"label\")\n",
    "                score = keyword_entry.get(\"score\", 0)\n",
    "\n",
    "                if keyword and score >= 0.35:  # Apply same threshold as classifier\n",
    "                    model_keyword_counts[keyword][model] += 1\n",
    "\n",
    "            # Extract research areas\n",
    "            for area_entry in classifications.get(\"research_areas\", []):\n",
    "                area = area_entry.get(\"label\")\n",
    "                score = area_entry.get(\"score\", 0)\n",
    "\n",
    "                if area and score >= 0.3:  # Apply reasonable threshold\n",
    "                    model_research_area_counts[area][model] += 1\n",
    "\n",
    "            # Extract division\n",
    "            division_entry = classifications.get(\"division\")\n",
    "            if division_entry:\n",
    "                division = division_entry.get(\"label\")\n",
    "                score = division_entry.get(\"score\", 0)\n",
    "\n",
    "                if division and score >= 0.5:  # Apply reasonable threshold\n",
    "                    model_division_counts[division][model] += 1\n",
    "\n",
    "        # Calculate total publications per model\n",
    "        total_models = sum(model_counts.values())\n",
    "        model_proportions = {\n",
    "            model: count / total_models for model, count in model_counts.items()\n",
    "        }\n",
    "\n",
    "        # Calculate keyword affinities\n",
    "        keyword_affinities = {}\n",
    "\n",
    "        for keyword, model_counts in model_keyword_counts.items():\n",
    "            # Only consider keywords with sufficient data\n",
    "            if sum(model_counts.values()) >= 3:\n",
    "                keyword_affinities[keyword] = {}\n",
    "\n",
    "                for model, count in model_counts.items():\n",
    "                    # Skip if total model count is too low\n",
    "                    if model_counts[model] < 2:\n",
    "                        continue\n",
    "\n",
    "                    # Calculate the probability of this model given the keyword\n",
    "                    prob_model_given_keyword = count / sum(model_counts.values())\n",
    "\n",
    "                    # Calculate the base probability of the model in the dataset\n",
    "                    base_prob_model = model_proportions.get(\n",
    "                        model, 0.01\n",
    "                    )  # Avoid division by zero\n",
    "\n",
    "                    # Calculate affinity as the ratio of these probabilities\n",
    "                    # Apply a transformation to make it a useful multiplier\n",
    "                    affinity = prob_model_given_keyword / base_prob_model\n",
    "\n",
    "                    # Apply thresholds and scaling\n",
    "                    if affinity > 1.05:  # Only include meaningful affinities\n",
    "                        # Cap very high affinities to avoid overconfidence\n",
    "                        affinity = min(1.35, affinity)\n",
    "                        # Round to 2 decimal places\n",
    "                        affinity = round(affinity, 2)\n",
    "                        keyword_affinities[keyword][model] = affinity\n",
    "\n",
    "        # Calculate research area affinities (using the same approach)\n",
    "        research_area_affinities = {}\n",
    "\n",
    "        for area, model_counts in model_research_area_counts.items():\n",
    "            if sum(model_counts.values()) >= 3:\n",
    "                research_area_affinities[area] = {}\n",
    "\n",
    "                for model, count in model_counts.items():\n",
    "                    if model_counts[model] < 2:\n",
    "                        continue\n",
    "\n",
    "                    prob_model_given_area = count / sum(model_counts.values())\n",
    "                    base_prob_model = model_proportions.get(model, 0.01)\n",
    "\n",
    "                    affinity = prob_model_given_area / base_prob_model\n",
    "\n",
    "                    if affinity > 1.05:\n",
    "                        affinity = min(\n",
    "                            1.7, affinity\n",
    "                        )  # Allow slightly higher values for research areas\n",
    "                        affinity = round(affinity, 2)\n",
    "                        research_area_affinities[area][model] = affinity\n",
    "\n",
    "        # Calculate division affinities (using the same approach)\n",
    "        division_affinities = {}\n",
    "\n",
    "        for division, model_counts in model_division_counts.items():\n",
    "            if sum(model_counts.values()) >= 3:\n",
    "                division_affinities[division] = {}\n",
    "\n",
    "                for model, count in model_counts.items():\n",
    "                    if model_counts[model] < 2:\n",
    "                        continue\n",
    "\n",
    "                    prob_model_given_division = count / sum(model_counts.values())\n",
    "                    base_prob_model = model_proportions.get(model, 0.01)\n",
    "\n",
    "                    affinity = prob_model_given_division / base_prob_model\n",
    "\n",
    "                    if affinity > 1.05:\n",
    "                        affinity = min(1.3, affinity)\n",
    "                        affinity = round(affinity, 2)\n",
    "                        division_affinities[division][model] = affinity\n",
    "\n",
    "        # Save the science keyword affinities to file\n",
    "        try:\n",
    "            with open(\"./data_driven_affinities.json\", \"w\") as f:\n",
    "                json.dump(keyword_affinities, f, indent=2)\n",
    "            print(\n",
    "                f\"Saved data-driven keyword affinities with {len(keyword_affinities)} entries\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data-driven affinities: {str(e)}\")\n",
    "\n",
    "        # Update global research area and division affinities\n",
    "        global _research_area_affinity_cache, _division_affinity_cache\n",
    "        _research_area_affinity_cache = research_area_affinities\n",
    "        _division_affinity_cache = division_affinities\n",
    "\n",
    "        print(\n",
    "            f\"Derived affinities for {len(keyword_affinities)} keywords, {len(research_area_affinities)} research areas, and {len(division_affinities)} divisions\"\n",
    "        )\n",
    "\n",
    "        return keyword_affinities\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error deriving data-driven affinities: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def get_science_keyword_model_affinities():\n",
    "    # Get science keyword affinities\n",
    "    global _keyword_affinity_cache\n",
    "\n",
    "    if _keyword_affinity_cache is not None:\n",
    "        return _keyword_affinity_cache\n",
    "\n",
    "    try:\n",
    "        with open(\"./data_driven_affinities.json\", \"r\") as f:\n",
    "            _keyword_affinity_cache = json.load(f)\n",
    "            return _keyword_affinity_cache\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(\"Error loading affinity data. Using default empty affinities.\")\n",
    "        _keyword_affinity_cache = {}\n",
    "        return _keyword_affinity_cache\n",
    "\n",
    "\n",
    "def get_research_area_model_affinities():\n",
    "    # Get research area affinities\n",
    "    global _research_area_affinity_cache\n",
    "\n",
    "    if _research_area_affinity_cache is not None:\n",
    "        return _research_area_affinity_cache\n",
    "\n",
    "    _research_area_affinity_cache = {\n",
    "        \"Atmospheric/Ocean Indicators\": {\n",
    "            \"ECCO\": 1.7,\n",
    "            \"MOMO-CHEM\": 1.4,\n",
    "            \"CMS-Flux\": 1.1,\n",
    "            \"ISSM\": 1.1,\n",
    "        },\n",
    "        \"Greenhouse Gases\": {\n",
    "            \"CARDAMOM\": 1.6,\n",
    "            \"CMS-Flux\": 1.8,\n",
    "            \"MOMO-CHEM\": 1.55,\n",
    "            \"ECCO\": 1.15,\n",
    "        },\n",
    "        \"Ecosystems\": {\"CARDAMOM\": 1.6, \"CMS-Flux\": 1.2, \"ECCO\": 1.25},\n",
    "        \"Land Surface/Agriculture Indicators\": {\n",
    "            \"CARDAMOM\": 1.4,\n",
    "            \"CMS-Flux\": 1.3,\n",
    "            \"ECCO\": 1.1,\n",
    "            \"ISSM\": 1.4,\n",
    "            \"RAPID\": 1.4,\n",
    "        },\n",
    "        \"Validation\": {\n",
    "            \"CMS-Flux\": 1.2,\n",
    "            \"ECCO\": 1.4,\n",
    "            \"ISSM\": 1.2,\n",
    "            \"MOMO-CHEM\": 1.15,\n",
    "            \"RAPID\": 1.25,\n",
    "        },\n",
    "        \"Cryospheric Indicators\": {\"ECCO\": 1.35, \"ISSM\": 1.9},\n",
    "        \"Air Quality\": {\"CMS-Flux\": 1.4, \"MOMO-CHEM\": 1.9},\n",
    "        \"Floods\": {\"ISSM\": 1.15, \"RAPID\": 1.6},\n",
    "        \"Environmental Impacts\": {\"MOMO-CHEM\": 1.25},\n",
    "        \"Severe Storms\": {\"ECCO\": 1.2},\n",
    "        \"Earthquakes\": {\"ECCO\": 1.05},\n",
    "        \"Droughts\": {\"CMS-Flux\": 1.2, \"RAPID\": 1.4},\n",
    "    }\n",
    "\n",
    "    return _research_area_affinity_cache\n",
    "\n",
    "\n",
    "def get_division_model_affinities():\n",
    "    # Get division affinities\n",
    "    global _division_affinity_cache\n",
    "\n",
    "    if _division_affinity_cache is not None:\n",
    "        return _division_affinity_cache\n",
    "\n",
    "    _division_affinity_cache = {\n",
    "        \"Earth Science\": {\n",
    "            \"ECCO\": 1.3,\n",
    "            \"RAPID\": 1.2,\n",
    "            \"CMS-Flux\": 1.15,\n",
    "            \"MOMO-CHEM\": 1.1,\n",
    "            \"ISSM\": 1.15,\n",
    "            \"CARDAMOM\": 1.15,\n",
    "        },\n",
    "        \"Biological and Physical Sciences\": {\n",
    "            \"CARDAMOM\": 1.2,\n",
    "            \"CMS-Flux\": 1.1,\n",
    "        },\n",
    "        \"Heliophysics\": {\n",
    "            \"MOMO-CHEM\": 1.15,\n",
    "        },\n",
    "        \"Planetary Science\": {\n",
    "            \"ISSM\": 1.1,\n",
    "        },\n",
    "        \"Astrophysics\": {},\n",
    "    }\n",
    "\n",
    "    return _division_affinity_cache\n",
    "\n",
    "\n",
    "def get_model_specific_thresholds():\n",
    "    # Get model-specific thresholds\n",
    "    return MODEL_THRESHOLDS.copy()\n",
    "\n",
    "\n",
    "def analyze_threshold_performance(\n",
    "    results: List[Dict],\n",
    "    model_thresholds: Dict[str, float] = None,\n",
    "    overall_threshold: float = 0.4,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze model performance with custom thresholds\n",
    "\n",
    "    Args:\n",
    "        results: List of publication results with ground truth\n",
    "        model_thresholds: Custom threshold values by model (defaults to MODEL_THRESHOLDS)\n",
    "        overall_threshold: Default threshold for models without specific threshold\n",
    "\n",
    "    Returns:\n",
    "        Dict containing performance metrics using the specified thresholds\n",
    "    \"\"\"\n",
    "    if model_thresholds is None:\n",
    "        model_thresholds = MODEL_THRESHOLDS.copy()\n",
    "\n",
    "    # Extract publications with ground truth\n",
    "    publications_with_truth = [\n",
    "        result\n",
    "        for result in results\n",
    "        if \"models\" in result and result[\"models\"] and \"confidence_scores\" in result\n",
    "    ]\n",
    "\n",
    "    if not publications_with_truth:\n",
    "        return {\"error\": \"No publications with ground truth for evaluation\"}\n",
    "\n",
    "    # Get all unique models\n",
    "    all_models = set()\n",
    "    for pub in publications_with_truth:\n",
    "        all_models.update(pub.get(\"models\", []))\n",
    "        for model in pub.get(\"confidence_scores\", {}).keys():\n",
    "            all_models.add(model)\n",
    "    all_models = sorted(list(all_models))\n",
    "\n",
    "    # Collect performance metrics\n",
    "    model_metrics = {}\n",
    "    overall_metrics = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}  # Added tn for MCC\n",
    "\n",
    "    for model in all_models:\n",
    "        # For each model, collect predictions using custom thresholds\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # Apply model-specific threshold or fall back to overall\n",
    "        threshold = model_thresholds.get(model, overall_threshold)\n",
    "\n",
    "        for pub in publications_with_truth:\n",
    "            is_true_match = 1 if model in pub.get(\"models\", []) else 0\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            is_predicted = 1 if confidence >= threshold else 0\n",
    "\n",
    "            y_true.append(is_true_match)\n",
    "            y_pred.append(is_predicted)\n",
    "\n",
    "        # Calculate metrics\n",
    "        tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n",
    "        fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n",
    "        fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n",
    "        tn = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0)  # Added for MCC\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)  # Calculate MCC\n",
    "\n",
    "        model_metrics[model] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"mcc\": mcc,  # Added MCC\n",
    "            \"tp\": tp,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tn\": tn,  # Added for MCC\n",
    "            \"threshold\": threshold,\n",
    "        }\n",
    "\n",
    "        # Accumulate for overall metrics\n",
    "        overall_metrics[\"tp\"] += tp\n",
    "        overall_metrics[\"fp\"] += fp\n",
    "        overall_metrics[\"fn\"] += fn\n",
    "        overall_metrics[\"tn\"] += tn  # Added for MCC\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    overall_precision = (\n",
    "        overall_metrics[\"tp\"] / (overall_metrics[\"tp\"] + overall_metrics[\"fp\"])\n",
    "        if (overall_metrics[\"tp\"] + overall_metrics[\"fp\"]) > 0\n",
    "        else 0\n",
    "    )\n",
    "    overall_recall = (\n",
    "        overall_metrics[\"tp\"] / (overall_metrics[\"tp\"] + overall_metrics[\"fn\"])\n",
    "        if (overall_metrics[\"tp\"] + overall_metrics[\"fn\"]) > 0\n",
    "        else 0\n",
    "    )\n",
    "    overall_f1 = (\n",
    "        2 * overall_precision * overall_recall / (overall_precision + overall_recall)\n",
    "        if (overall_precision + overall_recall) > 0\n",
    "        else 0\n",
    "    )\n",
    "    \n",
    "    # Calculate overall MCC\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    for pub in publications_with_truth:\n",
    "        for model in all_models:\n",
    "            is_true_match = 1 if model in pub.get(\"models\", []) else 0\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            threshold = model_thresholds.get(model, overall_threshold)\n",
    "            is_predicted = 1 if confidence >= threshold else 0\n",
    "            \n",
    "            all_y_true.append(is_true_match)\n",
    "            all_y_pred.append(is_predicted)\n",
    "    \n",
    "    overall_mcc = matthews_corrcoef(all_y_true, all_y_pred)\n",
    "\n",
    "    return {\n",
    "        \"per_model\": model_metrics,\n",
    "        \"overall\": {\n",
    "            \"precision\": overall_precision,\n",
    "            \"recall\": overall_recall,\n",
    "            \"f1\": overall_f1,\n",
    "            \"mcc\": overall_mcc,  # Added MCC\n",
    "            \"tp\": overall_metrics[\"tp\"],\n",
    "            \"fp\": overall_metrics[\"fp\"],\n",
    "            \"fn\": overall_metrics[\"fn\"],\n",
    "            \"tn\": overall_metrics[\"tn\"],  # Added for MCC\n",
    "        },\n",
    "        \"thresholds\": {\n",
    "            \"model_specific\": model_thresholds,\n",
    "            \"overall_default\": overall_threshold,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def find_optimal_thresholds(\n",
    "    results: List[Dict], threshold_range=None, step=0.05\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Find optimal thresholds for each model based on F1 score and MCC\n",
    "\n",
    "    Args:\n",
    "        results: List of publication results with ground truth\n",
    "        threshold_range: Optional range of thresholds to test (min, max)\n",
    "        step: Step size for threshold values\n",
    "\n",
    "    Returns:\n",
    "        Dict containing optimal thresholds for each model and overall\n",
    "    \"\"\"\n",
    "    if threshold_range is None:\n",
    "        threshold_range = (0.1, 0.95)\n",
    "\n",
    "    # Generate threshold values to test\n",
    "    thresholds = np.arange(threshold_range[0], threshold_range[1] + step, step)\n",
    "\n",
    "    # Extract publications with ground truth\n",
    "    publications_with_truth = [\n",
    "        result\n",
    "        for result in results\n",
    "        if \"models\" in result and result[\"models\"] and \"confidence_scores\" in result\n",
    "    ]\n",
    "\n",
    "    if not publications_with_truth:\n",
    "        return {\"error\": \"No publications with ground truth for evaluation\"}\n",
    "\n",
    "    # Get all unique models\n",
    "    all_models = set()\n",
    "    for pub in publications_with_truth:\n",
    "        all_models.update(pub.get(\"models\", []))\n",
    "        for model in pub.get(\"confidence_scores\", {}).keys():\n",
    "            all_models.add(model)\n",
    "    all_models = sorted(list(all_models))\n",
    "\n",
    "    # Find optimal thresholds for each model\n",
    "    optimal_thresholds = {}\n",
    "\n",
    "    for model in all_models:\n",
    "        best_f1 = -1\n",
    "        best_f1_threshold = 0.4  # Default\n",
    "        best_f1_metrics = {}\n",
    "        \n",
    "        best_mcc = -2  # MCC ranges from -1 to 1, so -2 is safe as initialization\n",
    "        best_mcc_threshold = 0.4  # Default\n",
    "        best_mcc_metrics = {}\n",
    "\n",
    "        # Extract data for this model\n",
    "        model_data = []\n",
    "        for pub in publications_with_truth:\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            is_true_match = 1 if model in pub.get(\"models\", []) else 0\n",
    "            model_data.append((confidence, is_true_match))\n",
    "\n",
    "        # Test each threshold\n",
    "        for threshold in thresholds:\n",
    "            # Calculate metrics at this threshold\n",
    "            tp = sum(1 for conf, true in model_data if true == 1 and conf >= threshold)\n",
    "            fp = sum(1 for conf, true in model_data if true == 0 and conf >= threshold)\n",
    "            fn = sum(1 for conf, true in model_data if true == 1 and conf < threshold)\n",
    "            tn = sum(1 for conf, true in model_data if true == 0 and conf < threshold)  # Added for MCC\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = (\n",
    "                2 * precision * recall / (precision + recall)\n",
    "                if (precision + recall) > 0\n",
    "                else 0\n",
    "            )\n",
    "            \n",
    "            # Calculate MCC\n",
    "            y_true = [true for _, true in model_data]\n",
    "            y_pred = [1 if conf >= threshold else 0 for conf, _ in model_data]\n",
    "            mcc = matthews_corrcoef(y_true, y_pred)  # Calculate MCC\n",
    "\n",
    "            # Update if this is the best F1 score\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_f1_threshold = threshold\n",
    "                best_f1_metrics = {\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1\": f1,\n",
    "                    \"mcc\": mcc,  # Include MCC\n",
    "                    \"tp\": tp,\n",
    "                    \"fp\": fp,\n",
    "                    \"fn\": fn,\n",
    "                    \"tn\": tn,  # Added for MCC\n",
    "                }\n",
    "            \n",
    "            # Update if this is the best MCC\n",
    "            if mcc > best_mcc:\n",
    "                best_mcc = mcc\n",
    "                best_mcc_threshold = threshold\n",
    "                best_mcc_metrics = {\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"f1\": f1,\n",
    "                    \"mcc\": mcc,\n",
    "                    \"tp\": tp,\n",
    "                    \"fp\": fp,\n",
    "                    \"fn\": fn,\n",
    "                    \"tn\": tn,\n",
    "                }\n",
    "\n",
    "        optimal_thresholds[model] = {\n",
    "            \"threshold_f1\": float(best_f1_threshold),\n",
    "            \"f1\": best_f1,\n",
    "            \"metrics_f1\": best_f1_metrics,\n",
    "            \"threshold_mcc\": float(best_mcc_threshold),  # Added MCC threshold\n",
    "            \"mcc\": best_mcc,  # Added best MCC value\n",
    "            \"metrics_mcc\": best_mcc_metrics,  # Added MCC metrics\n",
    "        }\n",
    "\n",
    "    # Find optimal overall threshold\n",
    "    all_data = []\n",
    "    for pub in publications_with_truth:\n",
    "        for model in all_models:\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            is_true_match = 1 if model in pub.get(\"models\", []) else 0\n",
    "            all_data.append((confidence, is_true_match))\n",
    "\n",
    "    best_overall_f1 = -1\n",
    "    best_overall_f1_threshold = 0.4  # Default\n",
    "    best_overall_f1_metrics = {}\n",
    "    \n",
    "    best_overall_mcc = -2  # MCC initialization\n",
    "    best_overall_mcc_threshold = 0.4  # Default\n",
    "    best_overall_mcc_metrics = {}\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Calculate metrics at this threshold\n",
    "        tp = sum(1 for conf, true in all_data if true == 1 and conf >= threshold)\n",
    "        fp = sum(1 for conf, true in all_data if true == 0 and conf >= threshold)\n",
    "        fn = sum(1 for conf, true in all_data if true == 1 and conf < threshold)\n",
    "        tn = sum(1 for conf, true in all_data if true == 0 and conf < threshold)  # Added for MCC\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "        \n",
    "        # Calculate MCC\n",
    "        y_true = [true for _, true in all_data]\n",
    "        y_pred = [1 if conf >= threshold else 0 for conf, _ in all_data]\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "        # Update if this is the best F1 score\n",
    "        if f1 > best_overall_f1:\n",
    "            best_overall_f1 = f1\n",
    "            best_overall_f1_threshold = threshold\n",
    "            best_overall_f1_metrics = {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"mcc\": mcc,  # Include MCC in F1 metrics\n",
    "                \"tp\": tp,\n",
    "                \"fp\": fp,\n",
    "                \"fn\": fn,\n",
    "                \"tn\": tn,\n",
    "            }\n",
    "        \n",
    "        # Update if this is the best MCC\n",
    "        if mcc > best_overall_mcc:\n",
    "            best_overall_mcc = mcc\n",
    "            best_overall_mcc_threshold = threshold\n",
    "            best_overall_mcc_metrics = {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"mcc\": mcc,\n",
    "                \"tp\": tp,\n",
    "                \"fp\": fp,\n",
    "                \"fn\": fn,\n",
    "                \"tn\": tn,\n",
    "            }\n",
    "\n",
    "    # Create model threshold dictionaries (both F1 and MCC-based)\n",
    "    model_threshold_dict_f1 = {\n",
    "        model: data[\"threshold_f1\"] for model, data in optimal_thresholds.items()\n",
    "    }\n",
    "    \n",
    "    model_threshold_dict_mcc = {\n",
    "        model: data[\"threshold_mcc\"] for model, data in optimal_thresholds.items()\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"per_model\": optimal_thresholds,\n",
    "        \"overall\": {\n",
    "            \"threshold_f1\": float(best_overall_f1_threshold),\n",
    "            \"f1\": best_overall_f1,\n",
    "            \"metrics_f1\": best_overall_f1_metrics,\n",
    "            \"threshold_mcc\": float(best_overall_mcc_threshold),  # Added MCC threshold\n",
    "            \"mcc\": best_overall_mcc,  # Added best MCC value\n",
    "            \"metrics_mcc\": best_overall_mcc_metrics,  # Added MCC metrics\n",
    "        },\n",
    "        \"model_thresholds_f1\": model_threshold_dict_f1,\n",
    "        \"model_thresholds_mcc\": model_threshold_dict_mcc,  # Added MCC thresholds\n",
    "    }\n",
    "\n",
    "\n",
    "# Semantic matching cache\n",
    "_semantic_match_cache = {}\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def semantic_match(\n",
    "    publication_text: str,\n",
    "    model_embeddings: Dict[str, np.ndarray],\n",
    "    threshold: float = 0.5,\n",
    "    model_thresholds: Dict[str, float] = None,\n",
    ") -> Dict[str, float]:\n",
    "    # Semantic matching\n",
    "    cache_key = f\"{hash(publication_text)}_{hash(str(threshold))}\"\n",
    "    if cache_key in _semantic_match_cache:\n",
    "        return _semantic_match_cache[cache_key]\n",
    "\n",
    "    if model_thresholds is None:\n",
    "        model_thresholds = get_model_specific_thresholds()\n",
    "\n",
    "    pub_embedding = st_model.encode(publication_text, convert_to_tensor=False)\n",
    "\n",
    "    model_names = list(model_embeddings.keys())\n",
    "    embeddings_array = np.array([model_embeddings[model] for model in model_names])\n",
    "\n",
    "    similarities = cosine_similarity([pub_embedding], embeddings_array)[0]\n",
    "\n",
    "    results = {}\n",
    "    for i, model in enumerate(model_names):\n",
    "        sim = similarities[i]\n",
    "        model_threshold = model_thresholds.get(model, threshold)\n",
    "\n",
    "        if sim >= model_threshold:\n",
    "            results[model] = float(sim)\n",
    "\n",
    "    _semantic_match_cache[cache_key] = results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3553c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "_matched_publications_cache = {}\n",
    "_improved_matching_cache = {}\n",
    "\n",
    "# Define model origin years\n",
    "MODEL_ORIGIN_YEARS = {\n",
    "    \"ECCO\": 1998,\n",
    "    \"CARDAMOM\": 2014,\n",
    "    \"CMS-Flux\": 2014,\n",
    "    \"ISSM\": 2003,\n",
    "    \"MOMO-CHEM\": 2014,\n",
    "    \"RAPID\": 2010,\n",
    "}\n",
    "\n",
    "\n",
    "def match_models_improved(\n",
    "    publication: Dict,\n",
    "    curated_mapping: Dict[str, str],\n",
    "    model_keywords: Dict[str, List[str]],\n",
    "    model_embeddings: Dict[str, np.ndarray],\n",
    "    science_classifier,\n",
    "    context_manager=None,\n",
    "    ranker=None,\n",
    "    threshold: float = 0.45,\n",
    "    research_area_affinities=None,\n",
    "    division_affinities=None,\n",
    "    keyword_affinities=None,\n",
    "    model_thresholds=None,\n",
    "    include_classifications: bool = True,\n",
    ") -> Dict:\n",
    "    # Match publication to models\n",
    "    # Try to use cache\n",
    "    pub_key = None\n",
    "    if \"DOI\" in publication and publication[\"DOI\"]:\n",
    "        pub_key = f\"doi:{normalize_doi(publication['DOI'])}\"\n",
    "    elif \"title\" in publication:\n",
    "        if isinstance(publication[\"title\"], list) and publication[\"title\"]:\n",
    "            pub_key = f\"title:{publication['title'][0]}\"\n",
    "        elif isinstance(publication[\"title\"], str):\n",
    "            pub_key = f\"title:{publication['title']}\"\n",
    "\n",
    "    if pub_key and pub_key in _improved_matching_cache:\n",
    "        return _improved_matching_cache[pub_key]\n",
    "\n",
    "    # Load affinities if needed\n",
    "    if research_area_affinities is None:\n",
    "        research_area_affinities = get_research_area_model_affinities()\n",
    "\n",
    "    if division_affinities is None:\n",
    "        division_affinities = get_division_model_affinities()\n",
    "\n",
    "    if keyword_affinities is None:\n",
    "        keyword_affinities = get_science_keyword_model_affinities()\n",
    "\n",
    "    if model_thresholds is None:\n",
    "        model_thresholds = get_model_specific_thresholds()\n",
    "\n",
    "    # Initialize scores\n",
    "    all_models = list(model_embeddings.keys())\n",
    "    confidence_scores = {model: 0.0 for model in all_models}\n",
    "    confidence_sources = {model: [] for model in all_models}\n",
    "\n",
    "    # Step 1: Check curated mapping (known ground truth)\n",
    "    doi = normalize_doi(publication.get(\"DOI\", \"\"))\n",
    "    if doi and doi in curated_mapping:\n",
    "        model = curated_mapping[doi]\n",
    "        confidence_scores[model] = 1.0\n",
    "        confidence_sources[model].append(\"curated_mapping\")\n",
    "\n",
    "    # Step 2: Extract text\n",
    "    title = \"\"\n",
    "    if \"title\" in publication:\n",
    "        if isinstance(publication[\"title\"], list) and publication[\"title\"]:\n",
    "            title = publication[\"title\"][0]\n",
    "        else:\n",
    "            title = publication.get(\"title\", \"\")\n",
    "\n",
    "    abstract = publication.get(\"abstract\", \"\")\n",
    "    publication_text = f\"{title} {abstract}\"\n",
    "\n",
    "    # Early exit if curated match and minimal text\n",
    "    model = None\n",
    "    if doi and doi in curated_mapping:\n",
    "        model = curated_mapping[doi]\n",
    "\n",
    "    if model and (len(publication_text) < 10):\n",
    "        matched_models = [\n",
    "            model\n",
    "            for model, confidence in confidence_scores.items()\n",
    "            if confidence >= threshold\n",
    "        ]\n",
    "\n",
    "        result = {\n",
    "            \"matched_models\": matched_models,\n",
    "            \"confidence_scores\": confidence_scores,\n",
    "            \"confidence_sources\": confidence_sources,\n",
    "        }\n",
    "\n",
    "        if pub_key:\n",
    "            _improved_matching_cache[pub_key] = result\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Step 3: Check model-specific keywords (most direct signal)\n",
    "    keywords = publication.get(\"keywords\", [])\n",
    "    text_for_keyword_matching = preprocess_text(publication_text)\n",
    "\n",
    "    keyword_match_counts = {model: 0 for model in all_models}\n",
    "    keyword_direct_matches = {model: [] for model in all_models}\n",
    "\n",
    "    for model, model_kw_list in model_keywords.items():\n",
    "        # Skip if already high confidence\n",
    "        if model in confidence_scores and confidence_scores[model] >= 0.95:\n",
    "            continue\n",
    "\n",
    "        for kw in model_kw_list:\n",
    "            # Check explicit keywords first (more reliable)\n",
    "            keyword_found = False\n",
    "            for pub_kw in keywords:\n",
    "                matched, score = fuzzy_keyword_match(pub_kw, kw)\n",
    "                if matched:\n",
    "                    keyword_match_counts[model] += 1\n",
    "                    # Store the exact keyword match for reference\n",
    "                    keyword_direct_matches[model].append(kw)\n",
    "                    keyword_found = True\n",
    "                    break\n",
    "\n",
    "            # Check text if not found in keywords\n",
    "            if not keyword_found:\n",
    "                match_found, score = fuzzy_keyword_match(text_for_keyword_matching, kw)\n",
    "                if match_found:\n",
    "                    keyword_match_counts[model] += 1\n",
    "                    keyword_direct_matches[model].append(kw)\n",
    "\n",
    "    # Apply direct keyword boost with improved confidence scale\n",
    "    for model, count in keyword_match_counts.items():\n",
    "        if count > 0:\n",
    "            # Apply more nuanced confidence scaling\n",
    "            if count >= 3:  # Multiple strong keyword matches\n",
    "                kw_confidence = 0.92\n",
    "            elif count == 2:  # Two keyword matches\n",
    "                kw_confidence = 0.85\n",
    "            else:  # Single keyword match\n",
    "                kw_confidence = 0.70\n",
    "\n",
    "            # Check for exact model name match which is a stronger signal\n",
    "            if any(kw.lower() == model.lower() for kw in keyword_direct_matches[model]):\n",
    "                kw_confidence = min(0.95, kw_confidence + 0.10)\n",
    "\n",
    "            if kw_confidence > confidence_scores[model]:\n",
    "                confidence_scores[model] = kw_confidence\n",
    "                confidence_sources[model].append(\"keyword_match\")\n",
    "\n",
    "    # Step 4: Semantic matching (useful for content similarity)\n",
    "    try:\n",
    "        if not any(score >= 0.95 for score in confidence_scores.values()):\n",
    "            # Improved default semantic threshold for better precision\n",
    "            semantic_matches = semantic_match(\n",
    "                publication_text,\n",
    "                model_embeddings,\n",
    "                threshold=0.45,  # Slightly higher default threshold\n",
    "                model_thresholds=model_thresholds,\n",
    "            )\n",
    "\n",
    "            for model, similarity in semantic_matches.items():\n",
    "                if similarity > 0.45:\n",
    "                    # Scale similarity to align with confidence scoring\n",
    "                    scaled_confidence = min(0.90, similarity * 0.95)\n",
    "                    confidence_scores[model] = max(\n",
    "                        confidence_scores[model], scaled_confidence\n",
    "                    )\n",
    "                    confidence_sources[model].append(\"semantic_match\")\n",
    "    except Exception as e:\n",
    "        print(f\"Semantic matching failed: {str(e)}\")\n",
    "\n",
    "    # Step 5: Science classification signals (important metadata matching)\n",
    "    if not any(score >= 0.95 for score in confidence_scores.values()):\n",
    "        try:\n",
    "            science_results = science_classifier.classify(publication)\n",
    "\n",
    "            # Step 5a: Apply science keyword affinities\n",
    "            science_keywords = science_results.get(\"science_keywords\", [])\n",
    "\n",
    "            # Track high-scoring keywords to apply boosting later\n",
    "            model_keyword_matches = defaultdict(list)\n",
    "\n",
    "            for kw_entry in science_keywords:\n",
    "                keyword = kw_entry[\"label\"]\n",
    "                keyword_score = kw_entry[\"score\"]\n",
    "\n",
    "                # Only process high confidence keywords\n",
    "                if keyword_score < 0.40:\n",
    "                    continue\n",
    "\n",
    "                # Use data-driven affinities for keywords\n",
    "                if keyword in keyword_affinities:\n",
    "                    for model, affinity in keyword_affinities[keyword].items():\n",
    "                        # Store keyword match for boosting\n",
    "                        model_keyword_matches[model].append((keyword, keyword_score))\n",
    "\n",
    "                        # Calculate confidence based on affinity and keyword score\n",
    "                        confidence = keyword_score * (affinity - 1.0)\n",
    "\n",
    "                        if confidence > 0.1:\n",
    "                            confidence_scores[model] = max(\n",
    "                                confidence_scores[model], confidence\n",
    "                            )\n",
    "                            confidence_sources[model].append(\n",
    "                                f\"science_keyword:{keyword}\"\n",
    "                            )\n",
    "\n",
    "            # Step 5b: Apply research area affinities\n",
    "            research_areas = science_results.get(\"research_areas\", [])\n",
    "            model_area_matches = defaultdict(list)\n",
    "\n",
    "            for area_entry in research_areas:\n",
    "                area = area_entry[\"label\"]\n",
    "                area_score = area_entry[\"score\"]\n",
    "\n",
    "                # Only consider strong research area matches\n",
    "                if area_score < 0.40:\n",
    "                    continue\n",
    "\n",
    "                if area in research_area_affinities:\n",
    "                    for model, affinity in research_area_affinities[area].items():\n",
    "                        # Store area match for boosting\n",
    "                        model_area_matches[model].append((area, area_score))\n",
    "\n",
    "                        # Apply improved confidence calculation\n",
    "                        confidence = area_score * (affinity - 1.0)\n",
    "                        confidence = min(\n",
    "                            confidence, 0.85\n",
    "                        )  # Cap to avoid overconfidence\n",
    "\n",
    "                        if confidence > 0.12:\n",
    "                            confidence_scores[model] = max(\n",
    "                                confidence_scores[model], confidence\n",
    "                            )\n",
    "                            confidence_sources[model].append(f\"research_area:{area}\")\n",
    "\n",
    "            # Step 5c: Apply division affinities\n",
    "            division_entry = science_results.get(\"division\")\n",
    "            if division_entry:\n",
    "                division = division_entry[\"label\"]\n",
    "                division_score = division_entry[\"score\"]\n",
    "\n",
    "                if division_score >= 0.55 and division in division_affinities:\n",
    "                    for model, affinity in division_affinities[division].items():\n",
    "                        confidence = division_score * (affinity - 1.0)\n",
    "\n",
    "                        if confidence > 0.05:\n",
    "                            confidence_scores[model] = max(\n",
    "                                confidence_scores[model], confidence\n",
    "                            )\n",
    "                            confidence_sources[model].append(f\"division:{division}\")\n",
    "\n",
    "            # Apply boosts for multiple science keyword matches\n",
    "            for model, matches in model_keyword_matches.items():\n",
    "                if len(matches) >= 3 and model in confidence_scores:\n",
    "                    # Calculate average score\n",
    "                    avg_score = sum(score for _, score in matches) / len(matches)\n",
    "                    # Apply a boost for multiple keyword matches\n",
    "                    boost = min(0.15, 0.05 * len(matches))\n",
    "                    current_score = confidence_scores[model]\n",
    "                    confidence_scores[model] = min(0.90, current_score + boost)\n",
    "                    if \"multiple_science_keywords\" not in confidence_sources[model]:\n",
    "                        confidence_sources[model].append(\"multiple_science_keywords\")\n",
    "\n",
    "            # Apply boosts for multiple research area matches\n",
    "            for model, matches in model_area_matches.items():\n",
    "                if len(matches) >= 2 and model in confidence_scores:\n",
    "                    # Calculate average score\n",
    "                    avg_score = sum(score for _, score in matches) / len(matches)\n",
    "                    # Apply a boost for multiple area matches\n",
    "                    boost = min(0.12, 0.06 * len(matches))\n",
    "                    current_score = confidence_scores[model]\n",
    "                    confidence_scores[model] = min(0.90, current_score + boost)\n",
    "                    if \"multiple_research_areas\" not in confidence_sources[model]:\n",
    "                        confidence_sources[model].append(\"multiple_research_areas\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Science classification failed: {str(e)}\")\n",
    "\n",
    "    # Step 6: Context validation (compare with known examples)\n",
    "    if context_manager and not any(\n",
    "        score >= 0.95 for score in confidence_scores.values()\n",
    "    ):\n",
    "        try:\n",
    "            context_scores = context_manager.get_context_scores(publication)\n",
    "            for model, score in context_scores.items():\n",
    "                if score > 0.45:  # Slightly higher threshold for quality\n",
    "                    # Apply a stronger weight to context validation\n",
    "                    context_confidence = score * 0.95\n",
    "                    confidence_scores[model] = max(\n",
    "                        confidence_scores[model], context_confidence\n",
    "                    )\n",
    "                    confidence_sources[model].append(\"context_validation\")\n",
    "        except Exception as e:\n",
    "            print(f\"Context validation failed: {str(e)}\")\n",
    "\n",
    "    # Step 7: Relevance ranking (specific query matching)\n",
    "    if ranker and not any(score >= 0.95 for score in confidence_scores.values()):\n",
    "        try:\n",
    "            # Create a more targeted query for ranking by focusing on title and first part of abstract\n",
    "            query = title\n",
    "            if abstract and len(abstract) > 50:\n",
    "                query = f\"{title} {abstract[:400]}\"\n",
    "\n",
    "            # Only rank models that have some confidence already\n",
    "            candidate_models = [\n",
    "                model for model, score in confidence_scores.items() if score > 0.25\n",
    "            ]\n",
    "\n",
    "            if candidate_models:\n",
    "                rank_scores = ranker.batch_rank(query, candidate_models)\n",
    "\n",
    "                for model, score in rank_scores.items():\n",
    "                    if score > 0.35:  # Apply a stronger filter\n",
    "                        # Scale ranker scores to be more in line with other confidence measures\n",
    "                        ranker_confidence = score * 0.95\n",
    "                        confidence_scores[model] = max(\n",
    "                            confidence_scores[model], ranker_confidence\n",
    "                        )\n",
    "                        confidence_sources[model].append(\"relevance_ranker\")\n",
    "        except Exception as e:\n",
    "            print(f\"Relevance ranking failed: {str(e)}\")\n",
    "\n",
    "    # Step 8: Apply hybrid boosts for evidence consensus\n",
    "    candidate_models = [\n",
    "        model for model, score in confidence_scores.items() if score >= 0.25\n",
    "    ]\n",
    "    for model in candidate_models:\n",
    "        sources = confidence_sources[model]\n",
    "\n",
    "        # Significant boost for keyword + semantic matches (two strong signals)\n",
    "        if \"keyword_match\" in sources and \"semantic_match\" in sources:\n",
    "            current_score = confidence_scores[model]\n",
    "            # Higher boost for this strong combination\n",
    "            confidence_scores[model] = min(0.95, current_score * 1.08)\n",
    "            if \"hybrid_kw_semantic\" not in sources:\n",
    "                sources.append(\"hybrid_kw_semantic\")\n",
    "\n",
    "        # Boost for science metadata consensus (strong metadata alignment)\n",
    "        science_sources = [\n",
    "            s\n",
    "            for s in sources\n",
    "            if s.startswith((\"science_keyword:\", \"research_area:\", \"division:\"))\n",
    "        ]\n",
    "        if len(science_sources) >= 3:  # More sources required for stronger consensus\n",
    "            current_score = confidence_scores[model]\n",
    "            confidence_scores[model] = min(0.92, current_score * 1.15)\n",
    "            if \"science_consensus\" not in sources:\n",
    "                sources.append(\"science_consensus\")\n",
    "\n",
    "        # Special boost for context validation + semantic match (high precision combo)\n",
    "        if \"context_validation\" in sources and \"semantic_match\" in sources:\n",
    "            current_score = confidence_scores[model]\n",
    "            confidence_scores[model] = min(0.95, current_score * 1.10)\n",
    "            if \"hybrid_context_semantic\" not in sources:\n",
    "                sources.append(\"hybrid_context_semantic\")\n",
    "\n",
    "        # Strong consensus boost when multiple independent signals agree\n",
    "        independent_signals = len(\n",
    "            set(\n",
    "                [\n",
    "                    \"keyword_match\",\n",
    "                    \"semantic_match\",\n",
    "                    \"context_validation\",\n",
    "                    \"relevance_ranker\",\n",
    "                    \"science_consensus\",\n",
    "                ]\n",
    "            )\n",
    "            & set(sources)\n",
    "        )\n",
    "        if independent_signals >= 3:  # At least 3 independent signals\n",
    "            current_score = confidence_scores[model]\n",
    "            confidence_scores[model] = min(0.97, current_score * 1.12)\n",
    "            if \"strong_signal_consensus\" not in sources:\n",
    "                sources.append(\"strong_signal_consensus\")\n",
    "\n",
    "    # Step 9: Apply second-order filtering (if a model scores extremely high, increase its threshold)\n",
    "    # This helps avoid false positives from borderline matches when we're very confident about another model\n",
    "    high_confidence_models = [\n",
    "        model for model, score in confidence_scores.items() if score >= 0.92\n",
    "    ]\n",
    "    if high_confidence_models:\n",
    "        # Increase threshold for other models\n",
    "        for model in all_models:\n",
    "            if model not in high_confidence_models and confidence_scores[model] < 0.75:\n",
    "                # Suppress low to medium confidence matches when we have very high confidence elsewhere\n",
    "                confidence_scores[model] = confidence_scores[model] * 0.85\n",
    "\n",
    "    # Step 10: Apply date verification - exclude models that were published after the publication date\n",
    "    # Extract publication date and year\n",
    "    pub_date = extract_publication_date(publication)\n",
    "    pub_year = None\n",
    "    if pub_date:\n",
    "        try:\n",
    "            pub_year = int(pub_date.split(\"-\")[0])\n",
    "        except (ValueError, IndexError):\n",
    "            pass\n",
    "\n",
    "    # If we have a publication year, filter out models that didn't exist yet\n",
    "    if pub_year:\n",
    "        for model in all_models:\n",
    "            if model in MODEL_ORIGIN_YEARS:\n",
    "                model_origin_year = MODEL_ORIGIN_YEARS[model]\n",
    "                if pub_year < model_origin_year:\n",
    "                    # Publication predates the model, so it cannot be about this model\n",
    "                    confidence_scores[model] = 0.0\n",
    "                    confidence_sources[model] = [\"excluded_by_date_verification\"]\n",
    "\n",
    "    # Filter by thresholds\n",
    "    matched_models = []\n",
    "    for model, confidence in confidence_scores.items():\n",
    "        model_threshold = model_thresholds.get(model, threshold)\n",
    "        if confidence >= model_threshold:\n",
    "            matched_models.append(model)\n",
    "\n",
    "    # Add classifications\n",
    "    science_results = None\n",
    "    if include_classifications:\n",
    "        try:\n",
    "            # Use existing science results if available\n",
    "            if not science_results:\n",
    "                science_results = science_classifier.classify(publication)\n",
    "\n",
    "            result_with_classifications = {\n",
    "                \"matched_models\": matched_models,\n",
    "                \"confidence_scores\": confidence_scores,\n",
    "                \"confidence_sources\": confidence_sources,\n",
    "                # Include all classification results\n",
    "                \"classifications\": {\n",
    "                    \"research_areas\": science_results.get(\"research_areas\", []),\n",
    "                    \"science_keywords\": science_results.get(\"science_keywords\", []),\n",
    "                    \"division\": science_results.get(\"division\"),\n",
    "                },\n",
    "            }\n",
    "\n",
    "            # Add context terms if available\n",
    "            if context_manager:\n",
    "                try:\n",
    "                    prepared_text = science_classifier.get_instance()._prepare_text(\n",
    "                        publication\n",
    "                    )\n",
    "                    pub_profile = context_manager._get_pub_profile(prepared_text)\n",
    "                    result_with_classifications[\"context_terms\"] = list(\n",
    "                        pub_profile[\"terms\"]\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting context terms: {str(e)}\")\n",
    "\n",
    "            # Add relevance scores if available\n",
    "            if ranker and publication_text:\n",
    "                try:\n",
    "                    query = publication_text[:500]\n",
    "                    relevance_scores = ranker.batch_rank(query, all_models)\n",
    "                    result_with_classifications[\"relevance_scores\"] = relevance_scores\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting relevance scores: {str(e)}\")\n",
    "\n",
    "            # Extract publication date\n",
    "            pub_date = extract_publication_date(publication)\n",
    "            if pub_date:\n",
    "                result_with_classifications[\"pubdate\"] = pub_date\n",
    "\n",
    "            # Cache result\n",
    "            if pub_key:\n",
    "                _improved_matching_cache[pub_key] = result_with_classifications\n",
    "\n",
    "            return result_with_classifications\n",
    "        except Exception as e:\n",
    "            print(f\"Error including classifications: {str(e)}\")\n",
    "\n",
    "    # Basic result without classifications\n",
    "    result = {\n",
    "        \"matched_models\": matched_models,\n",
    "        \"confidence_scores\": confidence_scores,\n",
    "        \"confidence_sources\": confidence_sources,\n",
    "    }\n",
    "\n",
    "    # Extract publication date\n",
    "    pub_date = extract_publication_date(publication)\n",
    "    if pub_date:\n",
    "        result[\"pubdate\"] = pub_date\n",
    "\n",
    "    # Cache result\n",
    "    if pub_key:\n",
    "        _improved_matching_cache[pub_key] = result\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_publication_batch(\n",
    "    publications: List[Dict],\n",
    "    curated_mapping: Dict[str, str],\n",
    "    model_keywords: Dict[str, List[str]],\n",
    "    model_embeddings: Dict[str, np.ndarray],\n",
    "    science_classifier,\n",
    "    context_manager,\n",
    "    ranker=None,\n",
    "    batch_size: int = 100,\n",
    "    include_classifications: bool = True,\n",
    ") -> List[Dict]:\n",
    "    # Process publications in batches\n",
    "    results = []\n",
    "    total_pubs = len(publications)\n",
    "    num_batches = (total_pubs + batch_size - 1) // batch_size\n",
    "\n",
    "    # Load affinities\n",
    "    research_area_affinities = get_research_area_model_affinities()\n",
    "    division_affinities = get_division_model_affinities()\n",
    "    keyword_affinities = get_science_keyword_model_affinities()\n",
    "    model_thresholds = get_model_specific_thresholds()\n",
    "\n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_pubs)\n",
    "\n",
    "        print(\n",
    "            f\"Processing batch {batch_idx+1}/{num_batches} (publications {start_idx+1}-{end_idx}/{total_pubs})...\"\n",
    "        )\n",
    "\n",
    "        batch_pubs = publications[start_idx:end_idx]\n",
    "        batch_results = []\n",
    "\n",
    "        # Process each publication\n",
    "        for i, pub in enumerate(batch_pubs):\n",
    "            pub_idx = start_idx + i + 1\n",
    "            if pub_idx % 20 == 0:\n",
    "                print(f\"  Processing publication {pub_idx}/{total_pubs}...\")\n",
    "\n",
    "            # Create cache\n",
    "            pub_key = None\n",
    "            if \"DOI\" in pub and pub[\"DOI\"]:\n",
    "                pub_key = f\"doi:{normalize_doi(pub['DOI'])}\"\n",
    "            elif \"title\" in pub:\n",
    "                if isinstance(pub[\"title\"], list) and pub[\"title\"]:\n",
    "                    pub_key = f\"title:{pub['title'][0]}\"\n",
    "                elif isinstance(pub[\"title\"], str):\n",
    "                    pub_key = f\"title:{pub['title']}\"\n",
    "\n",
    "            # Check cache\n",
    "            if pub_key and pub_key in _matched_publications_cache:\n",
    "                cached_result = _matched_publications_cache[pub_key]\n",
    "                pub_copy = pub.copy()\n",
    "                pub_copy.update(cached_result)\n",
    "                batch_results.append(pub_copy)\n",
    "                continue\n",
    "\n",
    "            # Match models\n",
    "            try:\n",
    "                match_result = match_models_improved(\n",
    "                    pub,\n",
    "                    curated_mapping,\n",
    "                    model_keywords,\n",
    "                    model_embeddings,\n",
    "                    science_classifier,\n",
    "                    context_manager,\n",
    "                    ranker,\n",
    "                    research_area_affinities=research_area_affinities,\n",
    "                    division_affinities=division_affinities,\n",
    "                    keyword_affinities=keyword_affinities,\n",
    "                    model_thresholds=model_thresholds,\n",
    "                    include_classifications=include_classifications,\n",
    "                )\n",
    "\n",
    "                # Add results to publication\n",
    "                pub_copy = pub.copy()\n",
    "                # Add all fields from match_result\n",
    "                for key, value in match_result.items():\n",
    "                    pub_copy[key] = value\n",
    "\n",
    "                # Extract characteristics if needed\n",
    "                if ranker is not None:\n",
    "                    pub_copy[\"pub_characteristics\"] = (\n",
    "                        extract_publication_characteristics(pub)\n",
    "                    )\n",
    "\n",
    "                # Add to results\n",
    "                batch_results.append(pub_copy)\n",
    "\n",
    "                # Cache results\n",
    "                if pub_key:\n",
    "                    cache_value = {\n",
    "                        \"matched_models\": match_result[\"matched_models\"],\n",
    "                        \"confidence_scores\": match_result[\"confidence_scores\"],\n",
    "                        \"confidence_sources\": match_result[\"confidence_sources\"],\n",
    "                    }\n",
    "                    if \"pub_characteristics\" in pub_copy:\n",
    "                        cache_value[\"pub_characteristics\"] = pub_copy[\n",
    "                            \"pub_characteristics\"\n",
    "                        ]\n",
    "                    _matched_publications_cache[pub_key] = cache_value\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing publication {pub_idx}: {str(e)}\")\n",
    "                # Add publication without matches\n",
    "                pub_copy = pub.copy()\n",
    "                pub_copy[\"matched_models\"] = []\n",
    "                pub_copy[\"confidence_scores\"] = {}\n",
    "                pub_copy[\"confidence_sources\"] = {}\n",
    "                batch_results.append(pub_copy)\n",
    "\n",
    "        # Add batch results to overall results\n",
    "        results.extend(batch_results)\n",
    "\n",
    "        # Periodically clean caches\n",
    "        if batch_idx % 5 == 4:\n",
    "            _fuzzy_match_cache.clear()\n",
    "            _semantic_match_cache.clear()\n",
    "\n",
    "            # Clean up memory\n",
    "            optimize_memory()\n",
    "\n",
    "        print(f\"Completed batch {batch_idx+1}/{num_batches}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "87e693ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics(\n",
    "    results: List[Dict], output_path_base: str = \"./metrics_visualization\"\n",
    "):\n",
    "    # Create comprehensive visualizations\n",
    "    # results: List of prediction results with true models and predicted models\n",
    "    # output_path_base: Base path for saving visualizations\n",
    "\n",
    "    # Extract publications with ground truth\n",
    "    publications_with_truth = [\n",
    "        result\n",
    "        for result in results\n",
    "        if \"models\" in result and result[\"models\"] and \"matched_models\" in result\n",
    "    ]\n",
    "\n",
    "    if not publications_with_truth:\n",
    "        print(\"No publications with ground truth for evaluation\")\n",
    "        return\n",
    "\n",
    "    # Get all unique models\n",
    "    all_models = set()\n",
    "    for pub in publications_with_truth:\n",
    "        all_models.update(pub.get(\"models\", []))\n",
    "        all_models.update(pub.get(\"matched_models\", []))\n",
    "    all_models = sorted(list(all_models))\n",
    "\n",
    "    # Dictionary to store all metrics\n",
    "    complete_metrics = {\n",
    "        \"model_performance\": {},\n",
    "        \"source_analysis\": {},\n",
    "        \"classification_accuracy\": {},\n",
    "        \"confidence_analysis\": {},\n",
    "        \"temporal_analysis\": {},\n",
    "        \"threshold_analysis\": {},  # New for threshold analysis\n",
    "        \"confusion_matrix\": {},  # New for confusion matrix\n",
    "        \"mcc_analysis\": {},      # New for Matthews Correlation Coefficient\n",
    "    }\n",
    "\n",
    "    # 1. Model Performance Metrics\n",
    "    model_metrics = {}\n",
    "    metrics = {\"precision\": [], \"recall\": [], \"f1\": [], \"mcc\": []}  # Added MCC\n",
    "\n",
    "    for model in all_models:\n",
    "        # For each model, collect binary predictions\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for pub in publications_with_truth:\n",
    "            y_true.append(1 if model in pub.get(\"models\", []) else 0)\n",
    "            y_pred.append(1 if model in pub.get(\"matched_models\", []) else 0)\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        mcc = matthews_corrcoef(y_true, y_pred)  # Calculate MCC\n",
    "\n",
    "        model_metrics[model] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"mcc\": mcc,  # Add MCC to metrics\n",
    "            \"true_positives\": sum(\n",
    "                1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1\n",
    "            ),\n",
    "            \"false_positives\": sum(\n",
    "                1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1\n",
    "            ),\n",
    "            \"false_negatives\": sum(\n",
    "                1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0\n",
    "            ),\n",
    "            \"true_negatives\": sum(\n",
    "                1 for t, p in zip(y_true, y_pred) if t == 0 and p == 0\n",
    "            ),  # Added for MCC\n",
    "            \"match_count\": sum(y_pred),\n",
    "        }\n",
    "\n",
    "        metrics[\"precision\"].append(precision)\n",
    "        metrics[\"recall\"].append(recall)\n",
    "        metrics[\"f1\"].append(f1)\n",
    "        metrics[\"mcc\"].append(mcc)  # Add MCC to metrics list\n",
    "\n",
    "    # Calculate micro-average metrics\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    for pub in publications_with_truth:\n",
    "        true_labels = [\n",
    "            1 if model in pub.get(\"models\", []) else 0 for model in all_models\n",
    "        ]\n",
    "        pred_labels = [\n",
    "            1 if model in pub.get(\"matched_models\", []) else 0 for model in all_models\n",
    "        ]\n",
    "\n",
    "        all_y_true.extend(true_labels)\n",
    "        all_y_pred.extend(pred_labels)\n",
    "\n",
    "    micro_precision = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    micro_recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    micro_f1 = f1_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    micro_mcc = matthews_corrcoef(all_y_true, all_y_pred)  # Calculate overall MCC\n",
    "\n",
    "    complete_metrics[\"model_performance\"] = {\n",
    "        \"per_model\": model_metrics,\n",
    "        \"micro_average\": {\n",
    "            \"precision\": micro_precision,\n",
    "            \"recall\": micro_recall,\n",
    "            \"f1\": micro_f1,\n",
    "            \"mcc\": micro_mcc,  # Add MCC to micro_average\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Create model performance visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Plot per-model metrics\n",
    "    x = np.arange(len(all_models))\n",
    "    width = 0.2  # Adjusted to fit 4 bars (added MCC)\n",
    "\n",
    "    # Skip if no models\n",
    "    if len(all_models) > 0:\n",
    "        bars1 = ax1.bar(x - 1.5*width, metrics[\"precision\"], width, label=\"Precision\")\n",
    "        bars2 = ax1.bar(x - 0.5*width, metrics[\"recall\"], width, label=\"Recall\")\n",
    "        bars3 = ax1.bar(x + 0.5*width, metrics[\"f1\"], width, label=\"F1 Score\")\n",
    "        bars4 = ax1.bar(x + 1.5*width, metrics[\"mcc\"], width, label=\"MCC\")  # Added MCC bars\n",
    "\n",
    "        ax1.set_xlabel(\"Models\")\n",
    "        ax1.set_ylabel(\"Score\")\n",
    "        ax1.set_title(\"Precision, Recall, F1 Score and MCC by Model\")\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(all_models, rotation=45, ha=\"right\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        # Add values on bars\n",
    "        def add_labels(bars):\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax1.text(\n",
    "                    bar.get_x() + bar.get_width() / 2.0,\n",
    "                    height + 0.01,\n",
    "                    f\"{height:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                )\n",
    "\n",
    "        add_labels(bars1)\n",
    "        add_labels(bars2)\n",
    "        add_labels(bars3)\n",
    "        add_labels(bars4)  # Add labels to MCC bars\n",
    "\n",
    "    # Plot micro-average metrics\n",
    "    micro_metrics = {\n",
    "        \"Precision\": micro_precision,\n",
    "        \"Recall\": micro_recall,\n",
    "        \"F1 Score\": micro_f1,\n",
    "        \"MCC\": micro_mcc,  # Added MCC\n",
    "    }\n",
    "\n",
    "    x2 = np.arange(len(micro_metrics))\n",
    "    bars = ax2.bar(x2, micro_metrics.values(), width=0.4)\n",
    "\n",
    "    ax2.set_xlabel(\"Metrics\")\n",
    "    ax2.set_ylabel(\"Score\")\n",
    "    ax2.set_title(\"Overall Micro-Average Metrics\")\n",
    "    ax2.set_xticks(x2)\n",
    "    ax2.set_xticklabels(micro_metrics.keys())\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add values on top of the bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_model_performance.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Create a dedicated visualization for Matthews Correlation Coefficient\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot MCC by model\n",
    "    mcc_values = [model_metrics[model][\"mcc\"] for model in all_models]\n",
    "    bars = ax1.bar(all_models, mcc_values)\n",
    "    \n",
    "    ax1.set_xlabel(\"Models\")\n",
    "    ax1.set_ylabel(\"Matthews Correlation Coefficient\")\n",
    "    ax1.set_title(\"Matthews Correlation Coefficient by Model\")\n",
    "    ax1.set_xticklabels(all_models, rotation=45, ha=\"right\")\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    ax1.axhline(y=micro_mcc, color='r', linestyle='--', label=f'Overall MCC: {micro_mcc:.3f}')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01 if height >= 0 else height - 0.08,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\" if height >= 0 else \"top\",\n",
    "            color=\"black\"\n",
    "        )\n",
    "    \n",
    "    # Plot MCC vs F1 to show their relationship\n",
    "    f1_values = [model_metrics[model][\"f1\"] for model in all_models]\n",
    "    \n",
    "    ax2.scatter(f1_values, mcc_values)\n",
    "    \n",
    "    # Add model labels to points\n",
    "    for i, model in enumerate(all_models):\n",
    "        ax2.annotate(\n",
    "            model, \n",
    "            (f1_values[i], mcc_values[i]),\n",
    "            xytext=(5, 5),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    # Add a diagonal line for reference\n",
    "    min_val = min(min(f1_values), min(mcc_values))\n",
    "    max_val = max(max(f1_values), max(mcc_values))\n",
    "    ax2.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.3)\n",
    "    \n",
    "    ax2.set_xlabel(\"F1 Score\")\n",
    "    ax2.set_ylabel(\"Matthews Correlation Coefficient\")\n",
    "    ax2.set_title(\"MCC vs F1 Score\")\n",
    "    ax2.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_mcc_analysis.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Confidence Analysis\n",
    "\n",
    "    # Extract confidence scores\n",
    "    confidence_data = {}\n",
    "\n",
    "    for model in all_models:\n",
    "        confidence_scores = []\n",
    "        correct_predictions = []\n",
    "\n",
    "        for pub in publications_with_truth:\n",
    "            is_true_match = model in pub.get(\"models\", [])\n",
    "            is_predicted = model in pub.get(\"matched_models\", [])\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "\n",
    "            confidence_scores.append(confidence)\n",
    "            correct_predictions.append(1 if is_true_match == is_predicted else 0)\n",
    "\n",
    "        confidence_data[model] = {\n",
    "            \"scores\": confidence_scores,\n",
    "            \"correct_predictions\": correct_predictions,\n",
    "            \"mean_confidence\": np.mean(confidence_scores) if confidence_scores else 0,\n",
    "            \"median_confidence\": np.median(confidence_scores)\n",
    "            if confidence_scores\n",
    "            else 0,\n",
    "        }\n",
    "\n",
    "    complete_metrics[\"confidence_analysis\"] = confidence_data\n",
    "\n",
    "    # Create confidence visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot mean confidence by model\n",
    "    mean_confidence = [\n",
    "        confidence_data[model][\"mean_confidence\"] for model in all_models\n",
    "    ]\n",
    "\n",
    "    if all_models:\n",
    "        ax1.bar(all_models, mean_confidence)\n",
    "        ax1.set_xlabel(\"Models\")\n",
    "        ax1.set_ylabel(\"Mean Confidence Score\")\n",
    "        ax1.set_title(\"Mean Confidence Score by Model\")\n",
    "        ax1.set_xticklabels(all_models, rotation=45, ha=\"right\")\n",
    "        ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Plot confidence distribution (boxplot)\n",
    "    if all_models:\n",
    "        confidence_values = [confidence_data[model][\"scores\"] for model in all_models]\n",
    "        ax2.boxplot(confidence_values, labels=all_models)\n",
    "        ax2.set_xlabel(\"Models\")\n",
    "        ax2.set_ylabel(\"Confidence Score Distribution\")\n",
    "        ax2.set_title(\"Confidence Score Distribution by Model\")\n",
    "        ax2.set_xticklabels(all_models, rotation=45, ha=\"right\")\n",
    "        ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_confidence_analysis.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Source Analysis\n",
    "\n",
    "    # Analyze the sources used for matches\n",
    "    source_counts = {}\n",
    "    source_accuracy = {}\n",
    "\n",
    "    for pub in publications_with_truth:\n",
    "        for model in all_models:\n",
    "            sources = pub.get(\"confidence_sources\", {}).get(model, [])\n",
    "            is_true_match = model in pub.get(\"models\", [])\n",
    "            is_predicted = model in pub.get(\"matched_models\", [])\n",
    "\n",
    "            for source in sources:\n",
    "                if source not in source_counts:\n",
    "                    source_counts[source] = 0\n",
    "                    source_accuracy[source] = {\"correct\": 0, \"total\": 0}\n",
    "\n",
    "                source_counts[source] += 1\n",
    "\n",
    "                # Track accuracy\n",
    "                source_accuracy[source][\"total\"] += 1\n",
    "                if is_true_match == is_predicted:\n",
    "                    source_accuracy[source][\"correct\"] += 1\n",
    "\n",
    "    # Calculate accuracy rates\n",
    "    for source in source_accuracy:\n",
    "        if source_accuracy[source][\"total\"] > 0:\n",
    "            source_accuracy[source][\"accuracy\"] = (\n",
    "                source_accuracy[source][\"correct\"] / source_accuracy[source][\"total\"]\n",
    "            )\n",
    "        else:\n",
    "            source_accuracy[source][\"accuracy\"] = 0\n",
    "\n",
    "    complete_metrics[\"source_analysis\"] = {\n",
    "        \"counts\": source_counts,\n",
    "        \"accuracy\": source_accuracy,\n",
    "    }\n",
    "\n",
    "    # Create source analysis visualization\n",
    "    if source_counts:\n",
    "        # Prepare data\n",
    "        sources = list(source_counts.keys())\n",
    "        counts = [source_counts[s] for s in sources]\n",
    "        accuracies = [source_accuracy[s][\"accuracy\"] for s in sources]\n",
    "\n",
    "        # Sort by count\n",
    "        sorted_data = sorted(\n",
    "            zip(sources, counts, accuracies), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        sources = [x[0] for x in sorted_data]\n",
    "        counts = [x[1] for x in sorted_data]\n",
    "        accuracies = [x[2] for x in sorted_data]\n",
    "\n",
    "        # Limit to top 15 sources for better visualization\n",
    "        if len(sources) > 15:\n",
    "            sources = sources[:15]\n",
    "            counts = counts[:15]\n",
    "            accuracies = accuracies[:15]\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "        # Plot counts\n",
    "        ax1.bar(sources, counts)\n",
    "        ax1.set_xlabel(\"Confidence Sources\")\n",
    "        ax1.set_ylabel(\"Count\")\n",
    "        ax1.set_title(\"Usage Count by Confidence Source\")\n",
    "        ax1.set_xticklabels(sources, rotation=45, ha=\"right\")\n",
    "        ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        # Plot accuracy\n",
    "        ax2.bar(sources, accuracies)\n",
    "        ax2.set_xlabel(\"Confidence Sources\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.set_title(\"Accuracy by Confidence Source\")\n",
    "        ax2.set_xticklabels(sources, rotation=45, ha=\"right\")\n",
    "        ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_path_base}_source_analysis.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 4. Temporal Analysis\n",
    "\n",
    "    # Analyze performance over time by publication date\n",
    "    date_metrics = {}\n",
    "\n",
    "    for pub in publications_with_truth:\n",
    "        pub_date = pub.get(\"pubdate\")\n",
    "        if not pub_date or len(pub_date) < 7:  # Ensure proper format\n",
    "            continue\n",
    "\n",
    "        year_month = pub_date  # Already in yyyy-mm format\n",
    "\n",
    "        if year_month not in date_metrics:\n",
    "            date_metrics[year_month] = {\n",
    "                \"total\": 0,\n",
    "                \"correct\": 0,\n",
    "                \"precision\": [],\n",
    "                \"recall\": [],\n",
    "                \"f1\": [],\n",
    "                \"mcc\": [],  # Added MCC\n",
    "            }\n",
    "\n",
    "        # Count publications\n",
    "        date_metrics[year_month][\"total\"] += 1\n",
    "\n",
    "        # Calculate accuracy for this publication\n",
    "        true_models = set(pub.get(\"models\", []))\n",
    "        pred_models = set(pub.get(\"matched_models\", []))\n",
    "\n",
    "        # True positives\n",
    "        tp = len(true_models.intersection(pred_models))\n",
    "        # False positives\n",
    "        fp = len(pred_models - true_models)\n",
    "        # False negatives\n",
    "        fn = len(true_models - pred_models)\n",
    "        # True negatives (for MCC) - need to consider all models not in either set\n",
    "        potential_models = set(all_models) - true_models - pred_models\n",
    "        tn = len(potential_models)\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (\n",
    "            2 * precision * recall / (precision + recall)\n",
    "            if (precision + recall) > 0\n",
    "            else 0\n",
    "        )\n",
    "        \n",
    "        # Calculate MCC for this publication\n",
    "        # First create binary arrays for all models\n",
    "        pub_y_true = [1 if m in true_models else 0 for m in all_models]\n",
    "        pub_y_pred = [1 if m in pred_models else 0 for m in all_models]\n",
    "        \n",
    "        # Calculate MCC if we have valid predictions (at least one true or one pred)\n",
    "        if sum(pub_y_true) > 0 or sum(pub_y_pred) > 0:\n",
    "            mcc = matthews_corrcoef(pub_y_true, pub_y_pred)\n",
    "        else:\n",
    "            mcc = 0\n",
    "\n",
    "        date_metrics[year_month][\"precision\"].append(precision)\n",
    "        date_metrics[year_month][\"recall\"].append(recall)\n",
    "        date_metrics[year_month][\"f1\"].append(f1)\n",
    "        date_metrics[year_month][\"mcc\"].append(mcc)  # Add MCC\n",
    "\n",
    "        # Mark as correct if all matches are correct\n",
    "        if true_models == pred_models:\n",
    "            date_metrics[year_month][\"correct\"] += 1\n",
    "\n",
    "    # Calculate average metrics by date\n",
    "    for date in date_metrics:\n",
    "        if date_metrics[date][\"total\"] > 0:\n",
    "            date_metrics[date][\"accuracy\"] = (\n",
    "                date_metrics[date][\"correct\"] / date_metrics[date][\"total\"]\n",
    "            )\n",
    "            date_metrics[date][\"avg_precision\"] = np.mean(\n",
    "                date_metrics[date][\"precision\"]\n",
    "            )\n",
    "            date_metrics[date][\"avg_recall\"] = np.mean(date_metrics[date][\"recall\"])\n",
    "            date_metrics[date][\"avg_f1\"] = np.mean(date_metrics[date][\"f1\"])\n",
    "            date_metrics[date][\"avg_mcc\"] = np.mean(date_metrics[date][\"mcc\"])  # Add avg MCC\n",
    "\n",
    "    complete_metrics[\"temporal_analysis\"] = date_metrics\n",
    "\n",
    "    # Create temporal analysis visualization with MCC\n",
    "    if date_metrics:\n",
    "        # Sort dates chronologically\n",
    "        sorted_dates = sorted(date_metrics.keys())\n",
    "\n",
    "        if len(sorted_dates) > 1:  # Only plot if we have multiple dates\n",
    "            accuracies = [date_metrics[d][\"accuracy\"] for d in sorted_dates]\n",
    "            precisions = [date_metrics[d][\"avg_precision\"] for d in sorted_dates]\n",
    "            recalls = [date_metrics[d][\"avg_recall\"] for d in sorted_dates]\n",
    "            f1_scores = [date_metrics[d][\"avg_f1\"] for d in sorted_dates]\n",
    "            mcc_scores = [date_metrics[d][\"avg_mcc\"] for d in sorted_dates]  # Added MCC\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "            ax.plot(sorted_dates, accuracies, \"o-\", label=\"Accuracy\")\n",
    "            ax.plot(sorted_dates, precisions, \"s-\", label=\"Precision\") \n",
    "            ax.plot(sorted_dates, recalls, \"^-\", label=\"Recall\")\n",
    "            ax.plot(sorted_dates, f1_scores, \"D-\", label=\"F1 Score\")\n",
    "            ax.plot(sorted_dates, mcc_scores, \"X-\", label=\"MCC\")  # Added MCC plot\n",
    "\n",
    "            ax.set_xlabel(\"Publication Date\")\n",
    "            ax.set_ylabel(\"Score\")\n",
    "            ax.set_title(\"Performance Metrics by Publication Date\")\n",
    "            ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "            ax.legend()\n",
    "\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_path_base}_temporal_analysis.png\", dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "    # 5. Classification Accuracy Analysis\n",
    "\n",
    "    # Analyze how classification results correlate with model matches\n",
    "    research_area_stats = {}\n",
    "    science_keyword_stats = {}\n",
    "    division_stats = {}\n",
    "\n",
    "    # Extract data\n",
    "    for pub in publications_with_truth:\n",
    "        # Check if we have classification data\n",
    "        classifications = pub.get(\"classifications\", {})\n",
    "        if not classifications:\n",
    "            continue\n",
    "\n",
    "        # Get match accuracy for this publication\n",
    "        true_models = set(pub.get(\"models\", []))\n",
    "        pred_models = set(pub.get(\"matched_models\", []))\n",
    "        is_correct = true_models == pred_models\n",
    "\n",
    "        # Process research areas\n",
    "        for area in classifications.get(\"research_areas\", []):\n",
    "            area_name = area.get(\"label\", \"\")\n",
    "            area_score = area.get(\"score\", 0)\n",
    "\n",
    "            if area_name and area_score > 0.3:  # Only consider significant areas\n",
    "                if area_name not in research_area_stats:\n",
    "                    research_area_stats[area_name] = {\"correct\": 0, \"total\": 0}\n",
    "\n",
    "                research_area_stats[area_name][\"total\"] += 1\n",
    "                if is_correct:\n",
    "                    research_area_stats[area_name][\"correct\"] += 1\n",
    "\n",
    "        # Process science keywords\n",
    "        for keyword in classifications.get(\"science_keywords\", []):\n",
    "            kw_name = keyword.get(\"label\", \"\")\n",
    "            kw_score = keyword.get(\"score\", 0)\n",
    "\n",
    "            if kw_name and kw_score > 0.3:  # Only consider significant keywords\n",
    "                if kw_name not in science_keyword_stats:\n",
    "                    science_keyword_stats[kw_name] = {\"correct\": 0, \"total\": 0}\n",
    "\n",
    "                science_keyword_stats[kw_name][\"total\"] += 1\n",
    "                if is_correct:\n",
    "                    science_keyword_stats[kw_name][\"correct\"] += 1\n",
    "\n",
    "        # Process division\n",
    "        division = classifications.get(\"division\", {})\n",
    "        if division:\n",
    "            div_name = division.get(\"label\", \"\")\n",
    "            div_score = division.get(\"score\", 0)\n",
    "\n",
    "            if div_name and div_score > 0.5:  # Only consider significant divisions\n",
    "                if div_name not in division_stats:\n",
    "                    division_stats[div_name] = {\"correct\": 0, \"total\": 0}\n",
    "\n",
    "                division_stats[div_name][\"total\"] += 1\n",
    "                if is_correct:\n",
    "                    division_stats[div_name][\"correct\"] += 1\n",
    "\n",
    "    # Calculate accuracy rates\n",
    "    for area in research_area_stats:\n",
    "        if research_area_stats[area][\"total\"] > 0:\n",
    "            research_area_stats[area][\"accuracy\"] = (\n",
    "                research_area_stats[area][\"correct\"]\n",
    "                / research_area_stats[area][\"total\"]\n",
    "            )\n",
    "\n",
    "    for kw in science_keyword_stats:\n",
    "        if science_keyword_stats[kw][\"total\"] > 0:\n",
    "            science_keyword_stats[kw][\"accuracy\"] = (\n",
    "                science_keyword_stats[kw][\"correct\"]\n",
    "                / science_keyword_stats[kw][\"total\"]\n",
    "            )\n",
    "\n",
    "    for div in division_stats:\n",
    "        if division_stats[div][\"total\"] > 0:\n",
    "            division_stats[div][\"accuracy\"] = (\n",
    "                division_stats[div][\"correct\"] / division_stats[div][\"total\"]\n",
    "            )\n",
    "\n",
    "    complete_metrics[\"classification_accuracy\"] = {\n",
    "        \"research_areas\": research_area_stats,\n",
    "        \"science_keywords\": science_keyword_stats,\n",
    "        \"divisions\": division_stats,\n",
    "    }\n",
    "\n",
    "    # Create classification accuracy visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "\n",
    "    # Filter out classifications with too few samples\n",
    "    def filter_and_sort(stats_dict, min_samples=5):\n",
    "        filtered = {k: v for k, v in stats_dict.items() if v[\"total\"] >= min_samples}\n",
    "        return sorted(filtered.items(), key=lambda x: x[1][\"accuracy\"], reverse=True)\n",
    "\n",
    "    # Plot research area accuracy\n",
    "    sorted_areas = filter_and_sort(research_area_stats)\n",
    "    if sorted_areas:\n",
    "        area_names = [a[0] for a in sorted_areas]\n",
    "        area_accuracies = [a[1][\"accuracy\"] for a in sorted_areas]\n",
    "        area_counts = [a[1][\"total\"] for a in sorted_areas]\n",
    "\n",
    "        # Limit to top 15 for better visualization\n",
    "        if len(area_names) > 15:\n",
    "            area_names = area_names[:15]\n",
    "            area_accuracies = area_accuracies[:15]\n",
    "            area_counts = area_counts[:15]\n",
    "\n",
    "        bars = ax1.barh(area_names, area_accuracies)\n",
    "        ax1.set_xlabel(\"Accuracy\")\n",
    "        ax1.set_ylabel(\"Research Area\")\n",
    "        ax1.set_title(\"Match Accuracy by Research Area\")\n",
    "        ax1.set_xlim(0, 1)\n",
    "\n",
    "        # Add count labels to bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax1.text(\n",
    "                width + 0.01,\n",
    "                bar.get_y() + bar.get_height() / 2,\n",
    "                f\"n={area_counts[i]}\",\n",
    "                ha=\"left\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "    else:\n",
    "        ax1.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Insufficient research area data\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax1.transAxes,\n",
    "        )\n",
    "\n",
    "    # Plot science keyword accuracy\n",
    "    sorted_keywords = filter_and_sort(science_keyword_stats)\n",
    "    if sorted_keywords:\n",
    "        kw_names = [k[0] for k in sorted_keywords]\n",
    "        kw_accuracies = [k[1][\"accuracy\"] for k in sorted_keywords]\n",
    "        kw_counts = [k[1][\"total\"] for k in sorted_keywords]\n",
    "\n",
    "        # Limit to top 15 for better visualization\n",
    "        if len(kw_names) > 15:\n",
    "            kw_names = kw_names[:15]\n",
    "            kw_accuracies = kw_accuracies[:15]\n",
    "            kw_counts = kw_counts[:15]\n",
    "\n",
    "        bars = ax2.barh(kw_names, kw_accuracies)\n",
    "        ax2.set_xlabel(\"Accuracy\")\n",
    "        ax2.set_ylabel(\"Science Keyword\")\n",
    "        ax2.set_title(\"Match Accuracy by Science Keyword\")\n",
    "        ax2.set_xlim(0, 1)\n",
    "\n",
    "        # Add count labels to bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax2.text(\n",
    "                width + 0.01,\n",
    "                bar.get_y() + bar.get_height() / 2,\n",
    "                f\"n={kw_counts[i]}\",\n",
    "                ha=\"left\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "    else:\n",
    "        ax2.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Insufficient science keyword data\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax2.transAxes,\n",
    "        )\n",
    "\n",
    "    # Plot division accuracy\n",
    "    sorted_divisions = filter_and_sort(division_stats)\n",
    "    if sorted_divisions:\n",
    "        div_names = [d[0] for d in sorted_divisions]\n",
    "        div_accuracies = [d[1][\"accuracy\"] for d in sorted_divisions]\n",
    "        div_counts = [d[1][\"total\"] for d in sorted_divisions]\n",
    "\n",
    "        bars = ax3.barh(div_names, div_accuracies)\n",
    "        ax3.set_xlabel(\"Accuracy\")\n",
    "        ax3.set_ylabel(\"Division\")\n",
    "        ax3.set_title(\"Match Accuracy by Division\")\n",
    "        ax3.set_xlim(0, 1)\n",
    "\n",
    "        # Add count labels to bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax3.text(\n",
    "                width + 0.01,\n",
    "                bar.get_y() + bar.get_height() / 2,\n",
    "                f\"n={div_counts[i]}\",\n",
    "                ha=\"left\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "    else:\n",
    "        ax3.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Insufficient division data\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax3.transAxes,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_classification_accuracy.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 6. Threshold Analysis (New)\n",
    "\n",
    "    # For each model, analyze different confidence thresholds\n",
    "    threshold_analysis = {}\n",
    "\n",
    "    # Generate possible threshold values to test\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "\n",
    "    for model in all_models:\n",
    "        threshold_metrics = {\n",
    "            t: {\"precision\": 0, \"recall\": 0, \"f1\": 0, \"mcc\": 0, \"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}  # Added MCC\n",
    "            for t in thresholds\n",
    "        }\n",
    "\n",
    "        # Extract confidence scores and true labels for this model\n",
    "        confidence_scores = []\n",
    "        true_labels = []\n",
    "\n",
    "        for pub in publications_with_truth:\n",
    "            score = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            is_true_match = 1 if model in pub.get(\"models\", []) else 0\n",
    "\n",
    "            confidence_scores.append(score)\n",
    "            true_labels.append(is_true_match)\n",
    "\n",
    "        # Calculate metrics at each threshold\n",
    "        for threshold in thresholds:\n",
    "            # Generate predictions using this threshold\n",
    "            predicted_labels = [\n",
    "                1 if score >= threshold else 0 for score in confidence_scores\n",
    "            ]\n",
    "\n",
    "            # Calculate performance metrics\n",
    "            tp = sum(\n",
    "                1 for t, p in zip(true_labels, predicted_labels) if t == 1 and p == 1\n",
    "            )\n",
    "            fp = sum(\n",
    "                1 for t, p in zip(true_labels, predicted_labels) if t == 0 and p == 1\n",
    "            )\n",
    "            fn = sum(\n",
    "                1 for t, p in zip(true_labels, predicted_labels) if t == 1 and p == 0\n",
    "            )\n",
    "            tn = sum(\n",
    "                1 for t, p in zip(true_labels, predicted_labels) if t == 0 and p == 0\n",
    "            )  # Added for MCC\n",
    "\n",
    "            # Precision, recall, and F1\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = (\n",
    "                2 * precision * recall / (precision + recall)\n",
    "                if (precision + recall) > 0\n",
    "                else 0\n",
    "            )\n",
    "            \n",
    "            # Calculate MCC\n",
    "            mcc = matthews_corrcoef(true_labels, predicted_labels)  # Added MCC calculation\n",
    "\n",
    "            threshold_metrics[threshold] = {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"mcc\": mcc,  # Added MCC\n",
    "                \"tp\": tp,\n",
    "                \"fp\": fp,\n",
    "                \"fn\": fn,\n",
    "                \"tn\": tn,  # Added for MCC\n",
    "            }\n",
    "\n",
    "        # Find optimal threshold based on F1 score\n",
    "        f1_scores = [(t, metrics[\"f1\"]) for t, metrics in threshold_metrics.items()]\n",
    "        optimal_threshold_f1 = max(f1_scores, key=lambda x: x[1])[0] if f1_scores else 0.5\n",
    "        \n",
    "        # Find optimal threshold based on MCC\n",
    "        mcc_scores = [(t, metrics[\"mcc\"]) for t, metrics in threshold_metrics.items()]\n",
    "        optimal_threshold_mcc = max(mcc_scores, key=lambda x: x[1])[0] if mcc_scores else 0.5\n",
    "\n",
    "        threshold_analysis[model] = {\n",
    "            \"metrics\": threshold_metrics,\n",
    "            \"optimal_threshold_f1\": optimal_threshold_f1,\n",
    "            \"optimal_f1\": threshold_metrics[optimal_threshold_f1][\"f1\"],\n",
    "            \"optimal_threshold_mcc\": optimal_threshold_mcc,  # Added optimal MCC threshold\n",
    "            \"optimal_mcc\": threshold_metrics[optimal_threshold_mcc][\"mcc\"],  # Added optimal MCC value\n",
    "            \"current_threshold\": MODEL_THRESHOLDS.get(model, 0.4),\n",
    "        }\n",
    "\n",
    "    # Calculate overall threshold analysis\n",
    "    overall_threshold_metrics = {\n",
    "        t: {\"precision\": 0, \"recall\": 0, \"f1\": 0, \"mcc\": 0} for t in thresholds  # Added MCC\n",
    "    }\n",
    "\n",
    "    # Get all confidence scores and labels across all models\n",
    "    all_confidence_scores = []\n",
    "    all_true_labels = []\n",
    "\n",
    "    for pub in publications_with_truth:\n",
    "        for model in all_models:\n",
    "            score = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            is_true_match = 1 if model in pub.get(\"models\", []) else 0\n",
    "\n",
    "            all_confidence_scores.append(score)\n",
    "            all_true_labels.append(is_true_match)\n",
    "\n",
    "    # Calculate metrics for each threshold\n",
    "    for threshold in thresholds:\n",
    "        predicted_labels = [\n",
    "            1 if score >= threshold else 0 for score in all_confidence_scores\n",
    "        ]\n",
    "\n",
    "        precision = precision_score(all_true_labels, predicted_labels, zero_division=0)\n",
    "        recall = recall_score(all_true_labels, predicted_labels, zero_division=0)\n",
    "        f1 = f1_score(all_true_labels, predicted_labels, zero_division=0)\n",
    "        mcc = matthews_corrcoef(all_true_labels, predicted_labels)  # Added MCC\n",
    "\n",
    "        overall_threshold_metrics[threshold] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"mcc\": mcc,  # Added MCC\n",
    "        }\n",
    "\n",
    "    # Find optimal overall thresholds (F1 and MCC)\n",
    "    overall_f1_scores = [\n",
    "        (t, metrics[\"f1\"]) for t, metrics in overall_threshold_metrics.items()\n",
    "    ]\n",
    "    overall_optimal_threshold_f1 = (\n",
    "        max(overall_f1_scores, key=lambda x: x[1])[0] if overall_f1_scores else 0.5\n",
    "    )\n",
    "    \n",
    "    overall_mcc_scores = [\n",
    "        (t, metrics[\"mcc\"]) for t, metrics in overall_threshold_metrics.items()\n",
    "    ]\n",
    "    overall_optimal_threshold_mcc = (\n",
    "        max(overall_mcc_scores, key=lambda x: x[1])[0] if overall_mcc_scores else 0.5\n",
    "    )\n",
    "\n",
    "    threshold_analysis[\"overall\"] = {\n",
    "        \"metrics\": overall_threshold_metrics,\n",
    "        \"optimal_threshold_f1\": overall_optimal_threshold_f1,\n",
    "        \"optimal_f1\": overall_threshold_metrics[overall_optimal_threshold_f1][\"f1\"],\n",
    "        \"optimal_threshold_mcc\": overall_optimal_threshold_mcc,  # Added optimal MCC threshold\n",
    "        \"optimal_mcc\": overall_threshold_metrics[overall_optimal_threshold_mcc][\"mcc\"],  # Added optimal MCC value\n",
    "        \"current_threshold\": 0.4,  # Default overall threshold\n",
    "    }\n",
    "\n",
    "    complete_metrics[\"threshold_analysis\"] = threshold_analysis\n",
    "\n",
    "    # Create threshold analysis visualizations\n",
    "    # 1. Model-specific threshold analysis with MCC\n",
    "    for model in all_models:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        model_thresholds = sorted(list(threshold_analysis[model][\"metrics\"].keys()))\n",
    "        precision_values = [\n",
    "            threshold_analysis[model][\"metrics\"][t][\"precision\"]\n",
    "            for t in model_thresholds\n",
    "        ]\n",
    "        recall_values = [\n",
    "            threshold_analysis[model][\"metrics\"][t][\"recall\"] for t in model_thresholds\n",
    "        ]\n",
    "        f1_values = [\n",
    "            threshold_analysis[model][\"metrics\"][t][\"f1\"] for t in model_thresholds\n",
    "        ]\n",
    "        mcc_values = [\n",
    "            threshold_analysis[model][\"metrics\"][t][\"mcc\"] for t in model_thresholds\n",
    "        ]  # Added MCC values\n",
    "\n",
    "        ax.plot(model_thresholds, precision_values, \"b-\", label=\"Precision\")\n",
    "        ax.plot(model_thresholds, recall_values, \"g-\", label=\"Recall\")\n",
    "        ax.plot(model_thresholds, f1_values, \"r-\", label=\"F1 Score\")\n",
    "        ax.plot(model_thresholds, mcc_values, \"m-\", label=\"MCC\")  # Added MCC plot\n",
    "\n",
    "        # Mark current and optimal thresholds\n",
    "        current_threshold = threshold_analysis[model][\"current_threshold\"]\n",
    "        optimal_threshold_f1 = threshold_analysis[model][\"optimal_threshold_f1\"]\n",
    "        optimal_threshold_mcc = threshold_analysis[model][\"optimal_threshold_mcc\"]  # Added MCC threshold\n",
    "\n",
    "        ax.axvline(\n",
    "            x=current_threshold,\n",
    "            color=\"gray\",\n",
    "            linestyle=\"--\",\n",
    "            label=f\"Current Threshold ({current_threshold:.2f})\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=optimal_threshold_f1,\n",
    "            color=\"red\",\n",
    "            linestyle=\"-.\",\n",
    "            label=f\"Optimal F1 Threshold ({optimal_threshold_f1:.2f})\",\n",
    "        )\n",
    "        ax.axvline(\n",
    "            x=optimal_threshold_mcc,\n",
    "            color=\"magenta\",\n",
    "            linestyle=\"-.\",\n",
    "            label=f\"Optimal MCC Threshold ({optimal_threshold_mcc:.2f})\",\n",
    "        )  # Added MCC threshold line\n",
    "\n",
    "        ax.set_xlabel(\"Confidence Threshold\")\n",
    "        ax.set_ylabel(\"Score\")\n",
    "        ax.set_title(f\"Threshold Analysis for {model}\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_path_base}_threshold_{model}.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 2. Overall threshold analysis with MCC\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    overall_thresholds = sorted(list(threshold_analysis[\"overall\"][\"metrics\"].keys()))\n",
    "    precision_values = [\n",
    "        threshold_analysis[\"overall\"][\"metrics\"][t][\"precision\"]\n",
    "        for t in overall_thresholds\n",
    "    ]\n",
    "    recall_values = [\n",
    "        threshold_analysis[\"overall\"][\"metrics\"][t][\"recall\"]\n",
    "        for t in overall_thresholds\n",
    "    ]\n",
    "    f1_values = [\n",
    "        threshold_analysis[\"overall\"][\"metrics\"][t][\"f1\"] for t in overall_thresholds\n",
    "    ]\n",
    "    mcc_values = [\n",
    "        threshold_analysis[\"overall\"][\"metrics\"][t][\"mcc\"] for t in overall_thresholds\n",
    "    ]  # Added MCC values\n",
    "\n",
    "    ax.plot(overall_thresholds, precision_values, \"b-\", label=\"Precision\")\n",
    "    ax.plot(overall_thresholds, recall_values, \"g-\", label=\"Recall\")\n",
    "    ax.plot(overall_thresholds, f1_values, \"r-\", label=\"F1 Score\")\n",
    "    ax.plot(overall_thresholds, mcc_values, \"m-\", label=\"MCC\")  # Added MCC plot\n",
    "\n",
    "    # Mark current and optimal thresholds\n",
    "    current_threshold = threshold_analysis[\"overall\"][\"current_threshold\"]\n",
    "    optimal_threshold_f1 = threshold_analysis[\"overall\"][\"optimal_threshold_f1\"]\n",
    "    optimal_threshold_mcc = threshold_analysis[\"overall\"][\"optimal_threshold_mcc\"]  # Added MCC threshold\n",
    "\n",
    "    ax.axvline(\n",
    "        x=current_threshold,\n",
    "        color=\"gray\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Current Threshold ({current_threshold:.2f})\",\n",
    "    )\n",
    "    ax.axvline(\n",
    "        x=optimal_threshold_f1,\n",
    "        color=\"red\",\n",
    "        linestyle=\"-.\",\n",
    "        label=f\"Optimal F1 Threshold ({optimal_threshold_f1:.2f})\",\n",
    "    )\n",
    "    ax.axvline(\n",
    "        x=optimal_threshold_mcc,\n",
    "        color=\"magenta\",\n",
    "        linestyle=\"-.\",\n",
    "        label=f\"Optimal MCC Threshold ({optimal_threshold_mcc:.2f})\",\n",
    "    )  # Added MCC threshold line\n",
    "\n",
    "    ax.set_xlabel(\"Confidence Threshold\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Overall Threshold Analysis\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_threshold_overall.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Comparative threshold analysis with MCC\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    # Create a bar chart comparing current vs. optimal thresholds (F1)\n",
    "    model_names = all_models\n",
    "    current_thresholds = [\n",
    "        threshold_analysis[model][\"current_threshold\"] for model in model_names\n",
    "    ]\n",
    "    optimal_thresholds_f1 = [\n",
    "        threshold_analysis[model][\"optimal_threshold_f1\"] for model in model_names\n",
    "    ]\n",
    "    optimal_thresholds_mcc = [\n",
    "        threshold_analysis[model][\"optimal_threshold_mcc\"] for model in model_names\n",
    "    ]  # Added MCC thresholds\n",
    "\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.25  # Adjusted for 3 sets of bars\n",
    "\n",
    "    bars1 = ax1.bar(x - width, current_thresholds, width, label=\"Current Threshold\")\n",
    "    bars2 = ax1.bar(x, optimal_thresholds_f1, width, label=\"Optimal F1 Threshold\")\n",
    "    bars3 = ax1.bar(x + width, optimal_thresholds_mcc, width, label=\"Optimal MCC Threshold\")  # Added MCC bars\n",
    "\n",
    "    ax1.set_xlabel(\"Model\")\n",
    "    ax1.set_ylabel(\"Threshold Value\")\n",
    "    ax1.set_title(\"Current vs. Optimal Thresholds by Model\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Create a bar chart comparing F1 vs MCC scores at their optimal thresholds\n",
    "    optimal_f1_scores = [threshold_analysis[model][\"optimal_f1\"] for model in model_names]\n",
    "    optimal_mcc_scores = [threshold_analysis[model][\"optimal_mcc\"] for model in model_names]\n",
    "\n",
    "    bars4 = ax2.bar(x - width/2, optimal_f1_scores, width, label=\"Optimal F1 Score\")\n",
    "    bars5 = ax2.bar(x + width/2, optimal_mcc_scores, width, label=\"Optimal MCC Score\")\n",
    "\n",
    "    ax2.set_xlabel(\"Model\")\n",
    "    ax2.set_ylabel(\"Score\")\n",
    "    ax2.set_title(\"Optimal F1 vs MCC Scores by Model\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_threshold_comparison.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 7. Confusion Matrix Analysis\n",
    "\n",
    "    # Create a confusion matrix for all models\n",
    "    # Initialize the confusion matrix with zeros\n",
    "    num_models = len(all_models)\n",
    "    confusion_matrix = np.zeros((num_models, num_models), dtype=int)\n",
    "\n",
    "    # Fill the confusion matrix\n",
    "    for pub in publications_with_truth:\n",
    "        true_models = pub.get(\"models\", [])\n",
    "        pred_models = pub.get(\"matched_models\", [])\n",
    "\n",
    "        for i, true_model in enumerate(all_models):\n",
    "            for j, pred_model in enumerate(all_models):\n",
    "                # Check if the true model is present in the ground truth\n",
    "                # and the predicted model is in the predictions\n",
    "                if true_model in true_models and pred_model in pred_models:\n",
    "                    confusion_matrix[i, j] += 1\n",
    "\n",
    "    # Store the confusion matrix in the metrics dictionary\n",
    "    complete_metrics[\"confusion_matrix\"] = {\n",
    "        \"matrix\": confusion_matrix.tolist(),\n",
    "        \"model_names\": all_models,\n",
    "    }\n",
    "\n",
    "    # Visualize the confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    im = ax.imshow(confusion_matrix, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    # Set ticks and labels\n",
    "    ax.set_xticks(np.arange(num_models))\n",
    "    ax.set_yticks(np.arange(num_models))\n",
    "    ax.set_xticklabels(all_models, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(all_models)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    for i in range(num_models):\n",
    "        for j in range(num_models):\n",
    "            text_color = (\n",
    "                \"white\"\n",
    "                if confusion_matrix[i, j] > confusion_matrix.max() / 2\n",
    "                else \"black\"\n",
    "            )\n",
    "            ax.text(\n",
    "                j, i, confusion_matrix[i, j], ha=\"center\", va=\"center\", color=text_color\n",
    "            )\n",
    "\n",
    "    ax.set_xlabel(\"Predicted Model\")\n",
    "    ax.set_ylabel(\"True Model\")\n",
    "    ax.set_title(\"Model Confusion Matrix\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 8. TP/TN/FP/FN Analysis for each model\n",
    "\n",
    "    # Calculate these metrics for each model\n",
    "    confusion_stats = {}\n",
    "\n",
    "    for i, model in enumerate(all_models):\n",
    "        # True positives, false positives, etc.\n",
    "        tp = model_metrics[model][\"true_positives\"]\n",
    "        fp = model_metrics[model][\"false_positives\"]\n",
    "        fn = model_metrics[model][\"false_negatives\"]\n",
    "        tn = model_metrics[model][\"true_negatives\"]  # Added for MCC\n",
    "\n",
    "        confusion_stats[model] = {\"tp\": tp, \"fp\": fp, \"tn\": tn, \"fn\": fn}\n",
    "\n",
    "    complete_metrics[\"confusion_stats\"] = confusion_stats\n",
    "\n",
    "    # Visualize TP/TN/FP/FN for each model\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Prepare data for grouped bar chart\n",
    "    bar_width = 0.2\n",
    "    index = np.arange(len(all_models))\n",
    "\n",
    "    # Extract metrics for each type\n",
    "    tp_values = [confusion_stats[model][\"tp\"] for model in all_models]\n",
    "    tn_values = [confusion_stats[model][\"tn\"] for model in all_models]\n",
    "    fp_values = [confusion_stats[model][\"fp\"] for model in all_models]\n",
    "    fn_values = [confusion_stats[model][\"fn\"] for model in all_models]\n",
    "\n",
    "    # Create bars\n",
    "    bars1 = ax.bar(\n",
    "        index - 1.5 * bar_width,\n",
    "        tp_values,\n",
    "        bar_width,\n",
    "        label=\"True Positives\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "    bars2 = ax.bar(\n",
    "        index - 0.5 * bar_width,\n",
    "        tn_values,\n",
    "        bar_width,\n",
    "        label=\"True Negatives\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    bars3 = ax.bar(\n",
    "        index + 0.5 * bar_width,\n",
    "        fp_values,\n",
    "        bar_width,\n",
    "        label=\"False Positives\",\n",
    "        color=\"red\",\n",
    "    )\n",
    "    bars4 = ax.bar(\n",
    "        index + 1.5 * bar_width,\n",
    "        fn_values,\n",
    "        bar_width,\n",
    "        label=\"False Negatives\",\n",
    "        color=\"orange\",\n",
    "    )\n",
    "\n",
    "    # Add some text for labels, title and custom axis\n",
    "    ax.set_xlabel(\"Model\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Confusion Matrix Statistics by Model\")\n",
    "    ax.set_xticks(index)\n",
    "    ax.set_xticklabels(all_models, rotation=45, ha=\"right\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_confusion_stats.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Create a summary visualization including MCC\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Top left: Model F1 scores\n",
    "    if all_models:\n",
    "        f1_values = [model_metrics[model][\"f1\"] for model in all_models]\n",
    "        mcc_values = [model_metrics[model][\"mcc\"] for model in all_models]  # Added MCC values\n",
    "        \n",
    "        # Create double bar chart\n",
    "        x = np.arange(len(all_models))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = axs[0, 0].bar(x - width/2, f1_values, width, label=\"F1 Score\")\n",
    "        bars2 = axs[0, 0].bar(x + width/2, mcc_values, width, label=\"MCC\")\n",
    "        \n",
    "        axs[0, 0].set_title(\"F1 Score and MCC by Model\")\n",
    "        axs[0, 0].set_xlabel(\"Model\")\n",
    "        axs[0, 0].set_ylabel(\"Score\")\n",
    "        axs[0, 0].set_xticks(x)\n",
    "        axs[0, 0].set_xticklabels(all_models, rotation=45, ha=\"right\")\n",
    "        axs[0, 0].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "        axs[0, 0].legend()\n",
    "\n",
    "        # Add labels\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            axs[0, 0].text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height + 0.01,\n",
    "                f\"{height:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "            \n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            axs[0, 0].text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height + 0.01,\n",
    "                f\"{height:.2f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "    else:\n",
    "        axs[0, 0].text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"No model data available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=axs[0, 0].transAxes,\n",
    "        )\n",
    "\n",
    "    # Top right: Sources accuracy (top 5)\n",
    "    if source_accuracy:\n",
    "        # Get top 5 most used sources\n",
    "        top_sources = sorted(source_counts.items(), key=lambda x: x[1], reverse=True)[\n",
    "            :5\n",
    "        ]\n",
    "        top_source_names = [s[0] for s in top_sources]\n",
    "\n",
    "        source_accuracies = [source_accuracy[s][\"accuracy\"] for s in top_source_names]\n",
    "        source_counts_plot = [source_counts[s] for s in top_source_names]\n",
    "\n",
    "        bars = axs[0, 1].bar(top_source_names, source_accuracies)\n",
    "        axs[0, 1].set_title(\"Accuracy by Top 5 Confidence Sources\")\n",
    "        axs[0, 1].set_xlabel(\"Source\")\n",
    "        axs[0, 1].set_ylabel(\"Accuracy\")\n",
    "        axs[0, 1].set_xticklabels(top_source_names, rotation=45, ha=\"right\")\n",
    "        axs[0, 1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        # Add count labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            axs[0, 1].text(\n",
    "                bar.get_x() + bar.get_width() / 2.0,\n",
    "                height + 0.01,\n",
    "                f\"n={source_counts_plot[i]}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "            )\n",
    "    else:\n",
    "        axs[0, 1].text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"No source data available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=axs[0, 1].transAxes,\n",
    "        )\n",
    "\n",
    "    # Bottom left: Classification accuracy\n",
    "    if research_area_stats or science_keyword_stats or division_stats:\n",
    "        # Create a summary of classification performance\n",
    "        classification_summary = {}\n",
    "\n",
    "        # Average accuracy by type\n",
    "        if research_area_stats:\n",
    "            values = [\n",
    "                s[\"accuracy\"] for s in research_area_stats.values() if s[\"total\"] >= 5\n",
    "            ]\n",
    "            if values:\n",
    "                classification_summary[\"Research Areas\"] = np.mean(values)\n",
    "\n",
    "        if science_keyword_stats:\n",
    "            values = [\n",
    "                s[\"accuracy\"] for s in science_keyword_stats.values() if s[\"total\"] >= 5\n",
    "            ]\n",
    "            if values:\n",
    "                classification_summary[\"Science Keywords\"] = np.mean(values)\n",
    "\n",
    "        if division_stats:\n",
    "            values = [s[\"accuracy\"] for s in division_stats.values() if s[\"total\"] >= 5]\n",
    "            if values:\n",
    "                classification_summary[\"Divisions\"] = np.mean(values)\n",
    "\n",
    "        if classification_summary:\n",
    "            names = list(classification_summary.keys())\n",
    "            values = list(classification_summary.values())\n",
    "\n",
    "            bars = axs[1, 0].bar(names, values)\n",
    "            axs[1, 0].set_title(\"Average Accuracy by Classification Type\")\n",
    "            axs[1, 0].set_xlabel(\"Classification Type\")\n",
    "            axs[1, 0].set_ylabel(\"Average Accuracy\")\n",
    "            axs[1, 0].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "            # Add labels\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                axs[1, 0].text(\n",
    "                    bar.get_x() + bar.get_width() / 2.0,\n",
    "                    height + 0.01,\n",
    "                    f\"{height:.2f}\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                )\n",
    "        else:\n",
    "            axs[1, 0].text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                \"Insufficient classification data\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=axs[1, 0].transAxes,\n",
    "            )\n",
    "    else:\n",
    "        axs[1, 0].text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"No classification data available\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=axs[1, 0].transAxes,\n",
    "        )\n",
    "\n",
    "    # Bottom right: Micro-average performance metrics including MCC\n",
    "    micro_metrics = {\n",
    "        \"Precision\": micro_precision,\n",
    "        \"Recall\": micro_recall,\n",
    "        \"F1 Score\": micro_f1,\n",
    "        \"MCC\": micro_mcc,  # Added MCC\n",
    "    }\n",
    "\n",
    "    bars = axs[1, 1].bar(micro_metrics.keys(), micro_metrics.values())\n",
    "    axs[1, 1].set_title(\"Overall Performance Metrics\")\n",
    "    axs[1, 1].set_xlabel(\"Metric\")\n",
    "    axs[1, 1].set_ylabel(\"Score\")\n",
    "    axs[1, 1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axs[1, 1].text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_summary.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Save all metrics to JSON\n",
    "    with open(f\"{output_path_base}_complete.json\", \"w\") as f:\n",
    "        # Convert NumPy values to Python types for JSON serialization\n",
    "        def convert_numpy(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_numpy(i) for i in obj]\n",
    "            else:\n",
    "                return obj\n",
    "\n",
    "        json.dump(convert_numpy(complete_metrics), f, indent=2)\n",
    "\n",
    "    print(\n",
    "        f\"Comprehensive metrics visualizations saved with base path: {output_path_base}\"\n",
    "    )\n",
    "\n",
    "    # Return all metrics for further analysis\n",
    "    return complete_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4343c60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Starting optimized science publication classifier...\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Initialize classifier\n",
    "    print(\"Initializing science classifier...\")\n",
    "    with torch.inference_mode():\n",
    "        science_classifier = ScienceClassifier.get_instance()\n",
    "    print(\"Science classifier initialized successfully.\")\n",
    "\n",
    "    # Load configuration files\n",
    "    print(\"Loading configuration files...\")\n",
    "    curated_mapping = load_curated_models('./curated_publications.json')\n",
    "    model_keywords = load_model_keywords('./model_keywords.json')\n",
    "    model_descriptions = load_model_descriptions('./model_descriptions.json')\n",
    "\n",
    "    # Initialize ranker\n",
    "    print(\"Initializing relevance ranker...\")\n",
    "    with torch.inference_mode():\n",
    "        ranker = RelevanceRanker(model_descriptions)\n",
    "\n",
    "    # Initialize model embeddings\n",
    "    print(\"Initializing model embeddings...\")\n",
    "    with torch.inference_mode():\n",
    "        model_embeddings = initialize_model_embeddings(model_descriptions)\n",
    "\n",
    "    # Clean memory\n",
    "    optimize_memory()\n",
    "\n",
    "    # Load test data\n",
    "    print(\"Loading test publications...\")\n",
    "    try:\n",
    "        with open('./all_2025-2-19.json') as f:\n",
    "            test_data = json.load(f)\n",
    "        print(f\"Loaded {len(test_data)} test publications.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test data: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Initialize context manager\n",
    "    print(\"Building context validation profiles...\")\n",
    "    with open('./curated_publications.json') as f:\n",
    "        full_curated = json.load(f)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        context_manager = ModelContextManager(full_curated)\n",
    "\n",
    "    # Derive data-driven affinities from the curated dataset\n",
    "    print(\"Deriving data-driven affinities from curated dataset...\")\n",
    "    derive_data_driven_affinities('./curated_publications.json')\n",
    "\n",
    "    # Process publications\n",
    "    print(\"\\nProcessing publications...\")\n",
    "    results = process_publication_batch(\n",
    "        test_data,\n",
    "        curated_mapping,\n",
    "        model_keywords,\n",
    "        model_embeddings,\n",
    "        science_classifier,\n",
    "        context_manager,\n",
    "        ranker=ranker\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    print(\"Saving results to results.json...\")\n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    # Generate visualizations with threshold analysis\n",
    "    print(\"\\nGenerating visualizations with threshold analysis...\")\n",
    "    metrics = visualize_metrics(results)\n",
    "\n",
    "    # Show TF-IDF model-specific terms\n",
    "    print(\"\\nModel-specific terminology based on TF-IDF analysis:\")\n",
    "    model_specific_terms = context_manager.get_model_specific_terms()\n",
    "    for model, terms in model_specific_terms.items():\n",
    "        print(f\"\\n{model} distinctive terms:\")\n",
    "        print(\", \".join(terms[:10]))  # Show top 10 terms\n",
    "\n",
    "    # Find optimal thresholds\n",
    "    print(\"\\nFinding optimal thresholds...\")\n",
    "    try:\n",
    "        optimal_thresholds = find_optimal_thresholds(results)\n",
    "\n",
    "        print(\"\\nOptimal model-specific thresholds:\")\n",
    "        for model, data in optimal_thresholds['per_model'].items():\n",
    "            current = MODEL_THRESHOLDS.get(model, 0.4)\n",
    "            # Use threshold_f1 instead of threshold\n",
    "            print(f\"{model}: {data['threshold_f1']:.2f} (current: {current:.2f}, F1: {data['f1']:.2f})\")\n",
    "\n",
    "        print(f\"\\nOptimal overall threshold: {optimal_thresholds['overall']['threshold_f1']:.2f}\")\n",
    "        print(f\"Overall F1 score with optimal thresholds: {optimal_thresholds['overall']['f1']:.3f}\")\n",
    "\n",
    "        # Compare performance with current vs. optimal thresholds\n",
    "        print(\"\\nComparing performance with current vs. optimal thresholds:\")\n",
    "        current_performance = analyze_threshold_performance(results)\n",
    "        optimal_performance = analyze_threshold_performance(\n",
    "            results,\n",
    "            model_thresholds=optimal_thresholds['model_thresholds_f1'],\n",
    "            overall_threshold=optimal_thresholds['overall']['threshold_f1']\n",
    "        )\n",
    "\n",
    "        print(f\"Current F1: {current_performance['overall']['f1']:.3f}, \" +\n",
    "            f\"Optimal F1: {optimal_performance['overall']['f1']:.3f}, \" +\n",
    "            f\"Improvement: {(optimal_performance['overall']['f1'] - current_performance['overall']['f1']) * 100:.1f}%\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during threshold optimization: {str(e)}\")\n",
    "\n",
    "    # Report completion\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nProcessing completed in {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"Average time per publication: {total_time/len(test_data):.4f}s\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Peak GPU memory usage: {torch.cuda.max_memory_allocated(0) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee00cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa7cc14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model generalization to unseen data\n",
    "\n",
    "def evaluate_generalization(\n",
    "    results: List[Dict], test_split_ratio: float = 0.3, random_seed: int = 42\n",
    ") -> Dict:\n",
    "    # Evaluate how well the classifier generalizes to unseen data by:\n",
    "    # 1. Splitting the dataset into training and testing sets\n",
    "    # 2. Comparing performance metrics between the sets\n",
    "    # 3. Analyzing the model's confidence distribution on both sets\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Extract publications with ground truth\n",
    "    publications_with_truth = [\n",
    "        result\n",
    "        for result in results\n",
    "        if \"models\" in result and result[\"models\"] and \"confidence_scores\" in result\n",
    "    ]\n",
    "\n",
    "    if not publications_with_truth:\n",
    "        return {\"error\": \"No publications with ground truth for evaluation\"}\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(publications_with_truth)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    split_idx = int(len(publications_with_truth) * (1 - test_split_ratio))\n",
    "    train_data = publications_with_truth[:split_idx]\n",
    "    test_data = publications_with_truth[split_idx:]\n",
    "\n",
    "    print(\n",
    "        f\"Split data into {len(train_data)} training and {len(test_data)} testing publications\"\n",
    "    )\n",
    "\n",
    "    # Get all unique models\n",
    "    all_models = set()\n",
    "    for pub in publications_with_truth:\n",
    "        all_models.update(pub.get(\"models\", []))\n",
    "    all_models = sorted(list(all_models))\n",
    "\n",
    "    # Calculate metrics for each set\n",
    "    generalization_metrics = {\n",
    "        \"training\": calculate_set_metrics(train_data, all_models),\n",
    "        \"testing\": calculate_set_metrics(test_data, all_models),\n",
    "        \"difference\": {},\n",
    "    }\n",
    "\n",
    "    # Calculate differences between training and testing metrics\n",
    "    for model in all_models:\n",
    "        if (\n",
    "            model in generalization_metrics[\"training\"][\"per_model\"]\n",
    "            and model in generalization_metrics[\"testing\"][\"per_model\"]\n",
    "        ):\n",
    "            train_metrics = generalization_metrics[\"training\"][\"per_model\"][model]\n",
    "            test_metrics = generalization_metrics[\"testing\"][\"per_model\"][model]\n",
    "\n",
    "            generalization_metrics[\"difference\"][model] = {\n",
    "                \"precision_diff\": test_metrics[\"precision\"]\n",
    "                - train_metrics[\"precision\"],\n",
    "                \"recall_diff\": test_metrics[\"recall\"] - train_metrics[\"recall\"],\n",
    "                \"f1_diff\": test_metrics[\"f1\"] - train_metrics[\"f1\"],\n",
    "            }\n",
    "\n",
    "    # Calculate overall difference\n",
    "    train_overall = generalization_metrics[\"training\"][\"overall\"]\n",
    "    test_overall = generalization_metrics[\"testing\"][\"overall\"]\n",
    "\n",
    "    generalization_metrics[\"difference\"][\"overall\"] = {\n",
    "        \"precision_diff\": test_overall[\"precision\"] - train_overall[\"precision\"],\n",
    "        \"recall_diff\": test_overall[\"recall\"] - train_overall[\"recall\"],\n",
    "        \"f1_diff\": test_overall[\"f1\"] - train_overall[\"f1\"],\n",
    "    }\n",
    "\n",
    "    # Analyze confidence score distribution for correct and incorrect predictions\n",
    "    generalization_metrics[\"confidence_analysis\"] = analyze_confidence_distribution(\n",
    "        train_data, test_data, all_models\n",
    "    )\n",
    "\n",
    "    # Calculate generalization gap metrics\n",
    "    generalization_gap = abs(train_overall[\"f1\"] - test_overall[\"f1\"])\n",
    "    generalization_metrics[\"generalization_gap\"] = {\n",
    "        \"f1_gap\": generalization_gap,\n",
    "        \"relative_gap\": generalization_gap / train_overall[\"f1\"]\n",
    "        if train_overall[\"f1\"] > 0\n",
    "        else float(\"inf\"),\n",
    "        \"gap_assessment\": assess_generalization_gap(generalization_gap),\n",
    "    }\n",
    "\n",
    "    # Generate visualizations\n",
    "    visualize_generalization_metrics(generalization_metrics, all_models)\n",
    "\n",
    "    return generalization_metrics\n",
    "\n",
    "\n",
    "def calculate_set_metrics(data: List[Dict], all_models: List[str]) -> Dict:\n",
    "    # Calculate performance metrics for a specific dataset\n",
    "    model_metrics = {}\n",
    "\n",
    "    # Calculate per-model metrics\n",
    "    for model in all_models:\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for pub in data:\n",
    "            is_true_match = 1 if model in pub.get(\"models\", []) else 0\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            model_threshold = MODEL_THRESHOLDS.get(model, 0.4)\n",
    "            is_predicted = 1 if confidence >= model_threshold else 0\n",
    "\n",
    "            y_true.append(is_true_match)\n",
    "            y_pred.append(is_predicted)\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        model_metrics[model] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"true_positives\": sum(\n",
    "                1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1\n",
    "            ),\n",
    "            \"false_positives\": sum(\n",
    "                1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1\n",
    "            ),\n",
    "            \"false_negatives\": sum(\n",
    "                1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    for pub in data:\n",
    "        for model in all_models:\n",
    "            is_true_match = 1 if model in pub.get(\"models\", []) else 0\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            model_threshold = MODEL_THRESHOLDS.get(model, 0.4)\n",
    "            is_predicted = 1 if confidence >= model_threshold else 0\n",
    "\n",
    "            all_y_true.append(is_true_match)\n",
    "            all_y_pred.append(is_predicted)\n",
    "\n",
    "    micro_precision = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    micro_recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    micro_f1 = f1_score(all_y_true, all_y_pred, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"per_model\": model_metrics,\n",
    "        \"overall\": {\n",
    "            \"precision\": micro_precision,\n",
    "            \"recall\": micro_recall,\n",
    "            \"f1\": micro_f1,\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_confidence_distribution(\n",
    "    train_data: List[Dict], test_data: List[Dict], all_models: List[str]\n",
    ") -> Dict:\n",
    "    # Analyze confidence score distributions for correct and incorrect predictions\n",
    "    result = {\"train\": {}, \"test\": {}}\n",
    "\n",
    "    for dataset_name, dataset in [(\"train\", train_data), (\"test\", test_data)]:\n",
    "        # Overall confidence distribution\n",
    "        all_confidences = {\"correct\": [], \"incorrect\": []}\n",
    "\n",
    "        # Per-model confidence distributions\n",
    "        model_confidences = {\n",
    "            model: {\"correct\": [], \"incorrect\": []} for model in all_models\n",
    "        }\n",
    "\n",
    "        for pub in dataset:\n",
    "            true_models = set(pub.get(\"models\", []))\n",
    "\n",
    "            for model in all_models:\n",
    "                confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "                model_threshold = MODEL_THRESHOLDS.get(model, 0.4)\n",
    "                is_predicted = confidence >= model_threshold\n",
    "                is_true_match = model in true_models\n",
    "\n",
    "                # Determine if prediction is correct\n",
    "                is_correct = (is_predicted and is_true_match) or (\n",
    "                    not is_predicted and not is_true_match\n",
    "                )\n",
    "\n",
    "                # Store confidence score\n",
    "                category = \"correct\" if is_correct else \"incorrect\"\n",
    "                all_confidences[category].append(confidence)\n",
    "                model_confidences[model][category].append(confidence)\n",
    "\n",
    "        # Calculate statistics for all models\n",
    "        result[dataset_name][\"all_models\"] = {\n",
    "            \"correct\": {\n",
    "                \"count\": len(all_confidences[\"correct\"]),\n",
    "                \"mean\": np.mean(all_confidences[\"correct\"])\n",
    "                if all_confidences[\"correct\"]\n",
    "                else 0,\n",
    "                \"median\": np.median(all_confidences[\"correct\"])\n",
    "                if all_confidences[\"correct\"]\n",
    "                else 0,\n",
    "                \"std\": np.std(all_confidences[\"correct\"])\n",
    "                if all_confidences[\"correct\"]\n",
    "                else 0,\n",
    "            },\n",
    "            \"incorrect\": {\n",
    "                \"count\": len(all_confidences[\"incorrect\"]),\n",
    "                \"mean\": np.mean(all_confidences[\"incorrect\"])\n",
    "                if all_confidences[\"incorrect\"]\n",
    "                else 0,\n",
    "                \"median\": np.median(all_confidences[\"incorrect\"])\n",
    "                if all_confidences[\"incorrect\"]\n",
    "                else 0,\n",
    "                \"std\": np.std(all_confidences[\"incorrect\"])\n",
    "                if all_confidences[\"incorrect\"]\n",
    "                else 0,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Calculate statistics for each model\n",
    "        result[dataset_name][\"per_model\"] = {}\n",
    "        for model in all_models:\n",
    "            result[dataset_name][\"per_model\"][model] = {\n",
    "                \"correct\": {\n",
    "                    \"count\": len(model_confidences[model][\"correct\"]),\n",
    "                    \"mean\": np.mean(model_confidences[model][\"correct\"])\n",
    "                    if model_confidences[model][\"correct\"]\n",
    "                    else 0,\n",
    "                    \"median\": np.median(model_confidences[model][\"correct\"])\n",
    "                    if model_confidences[model][\"correct\"]\n",
    "                    else 0,\n",
    "                    \"std\": np.std(model_confidences[model][\"correct\"])\n",
    "                    if model_confidences[model][\"correct\"]\n",
    "                    else 0,\n",
    "                },\n",
    "                \"incorrect\": {\n",
    "                    \"count\": len(model_confidences[model][\"incorrect\"]),\n",
    "                    \"mean\": np.mean(model_confidences[model][\"incorrect\"])\n",
    "                    if model_confidences[model][\"incorrect\"]\n",
    "                    else 0,\n",
    "                    \"median\": np.median(model_confidences[model][\"incorrect\"])\n",
    "                    if model_confidences[model][\"incorrect\"]\n",
    "                    else 0,\n",
    "                    \"std\": np.std(model_confidences[model][\"incorrect\"])\n",
    "                    if model_confidences[model][\"incorrect\"]\n",
    "                    else 0,\n",
    "                },\n",
    "            }\n",
    "\n",
    "    # Calculate differences between train and test sets\n",
    "    result[\"diff\"] = {\n",
    "        \"all_models\": {\n",
    "            \"correct_mean_diff\": result[\"test\"][\"all_models\"][\"correct\"][\"mean\"]\n",
    "            - result[\"train\"][\"all_models\"][\"correct\"][\"mean\"],\n",
    "            \"incorrect_mean_diff\": result[\"test\"][\"all_models\"][\"incorrect\"][\"mean\"]\n",
    "            - result[\"train\"][\"all_models\"][\"incorrect\"][\"mean\"],\n",
    "            \"correct_median_diff\": result[\"test\"][\"all_models\"][\"correct\"][\"median\"]\n",
    "            - result[\"train\"][\"all_models\"][\"correct\"][\"median\"],\n",
    "            \"incorrect_median_diff\": result[\"test\"][\"all_models\"][\"incorrect\"][\"median\"]\n",
    "            - result[\"train\"][\"all_models\"][\"incorrect\"][\"median\"],\n",
    "        },\n",
    "        \"per_model\": {},\n",
    "    }\n",
    "\n",
    "    for model in all_models:\n",
    "        result[\"diff\"][\"per_model\"][model] = {\n",
    "            \"correct_mean_diff\": result[\"test\"][\"per_model\"][model][\"correct\"][\"mean\"]\n",
    "            - result[\"train\"][\"per_model\"][model][\"correct\"][\"mean\"],\n",
    "            \"incorrect_mean_diff\": result[\"test\"][\"per_model\"][model][\"incorrect\"][\n",
    "                \"mean\"\n",
    "            ]\n",
    "            - result[\"train\"][\"per_model\"][model][\"incorrect\"][\"mean\"],\n",
    "        }\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def assess_generalization_gap(gap: float) -> str:\n",
    "    # Assess the generalization gap qualitatively\n",
    "    if gap < 0.03:\n",
    "        return \"Excellent generalization - model performs consistently on unseen data\"\n",
    "    elif gap < 0.05:\n",
    "        return \"Good generalization - minimal performance degradation on unseen data\"\n",
    "    elif gap < 0.1:\n",
    "        return \"Fair generalization - noticeable performance drop on unseen data\"\n",
    "    elif gap < 0.2:\n",
    "        return \"Poor generalization - significant performance drop on unseen data\"\n",
    "    else:\n",
    "        return \"Very poor generalization - model is likely overfitting\"\n",
    "\n",
    "\n",
    "def visualize_generalization_metrics(metrics: Dict, all_models: List[str]):\n",
    "    # Generate visualizations for generalization metrics\n",
    "    # 1. Train vs Test overall performance\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot overall metrics comparison\n",
    "    metrics_names = [\"Precision\", \"Recall\", \"F1 Score\"]\n",
    "    train_values = [\n",
    "        metrics[\"training\"][\"overall\"][\"precision\"],\n",
    "        metrics[\"training\"][\"overall\"][\"recall\"],\n",
    "        metrics[\"training\"][\"overall\"][\"f1\"],\n",
    "    ]\n",
    "    test_values = [\n",
    "        metrics[\"testing\"][\"overall\"][\"precision\"],\n",
    "        metrics[\"testing\"][\"overall\"][\"recall\"],\n",
    "        metrics[\"testing\"][\"overall\"][\"f1\"],\n",
    "    ]\n",
    "\n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.35\n",
    "\n",
    "    ax1.bar(x - width / 2, train_values, width, label=\"Training\")\n",
    "    ax1.bar(x + width / 2, test_values, width, label=\"Testing\")\n",
    "\n",
    "    ax1.set_xlabel(\"Metrics\")\n",
    "    ax1.set_ylabel(\"Score\")\n",
    "    ax1.set_title(\"Overall Metrics: Training vs Testing\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(metrics_names)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(train_values):\n",
    "        ax1.text(i - width / 2, v + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "    for i, v in enumerate(test_values):\n",
    "        ax1.text(i + width / 2, v + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Plot F1 score comparison by model\n",
    "    train_f1 = [metrics[\"training\"][\"per_model\"][model][\"f1\"] for model in all_models]\n",
    "    test_f1 = [metrics[\"testing\"][\"per_model\"][model][\"f1\"] for model in all_models]\n",
    "\n",
    "    x = np.arange(len(all_models))\n",
    "\n",
    "    ax2.bar(x - width / 2, train_f1, width, label=\"Training\")\n",
    "    ax2.bar(x + width / 2, test_f1, width, label=\"Testing\")\n",
    "\n",
    "    ax2.set_xlabel(\"Models\")\n",
    "    ax2.set_ylabel(\"F1 Score\")\n",
    "    ax2.set_title(\"F1 Score by Model: Training vs Testing\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(all_models, rotation=45, ha=\"right\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./metrics_visualization/generalization_overall.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Generalization gap analysis\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot F1 difference by model\n",
    "    f1_diffs = [metrics[\"difference\"][model][\"f1_diff\"] for model in all_models]\n",
    "\n",
    "    colors = [\"green\" if diff >= 0 else \"red\" for diff in f1_diffs]\n",
    "    ax1.bar(all_models, f1_diffs, color=colors)\n",
    "\n",
    "    ax1.set_xlabel(\"Models\")\n",
    "    ax1.set_ylabel(\"F1 Score Difference (Test - Train)\")\n",
    "    ax1.set_title(\"F1 Score Difference by Model\")\n",
    "    ax1.set_xticklabels(all_models, rotation=45, ha=\"right\")\n",
    "    ax1.axhline(y=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(f1_diffs):\n",
    "        va = \"bottom\" if v >= 0 else \"top\"\n",
    "        ax1.text(i, v + 0.01 if v >= 0 else v - 0.01, f\"{v:.3f}\", ha=\"center\", va=va)\n",
    "\n",
    "    # Plot confidence analysis\n",
    "    train_correct = metrics[\"confidence_analysis\"][\"train\"][\"all_models\"][\"correct\"][\n",
    "        \"mean\"\n",
    "    ]\n",
    "    train_incorrect = metrics[\"confidence_analysis\"][\"train\"][\"all_models\"][\n",
    "        \"incorrect\"\n",
    "    ][\"mean\"]\n",
    "    test_correct = metrics[\"confidence_analysis\"][\"test\"][\"all_models\"][\"correct\"][\n",
    "        \"mean\"\n",
    "    ]\n",
    "    test_incorrect = metrics[\"confidence_analysis\"][\"test\"][\"all_models\"][\"incorrect\"][\n",
    "        \"mean\"\n",
    "    ]\n",
    "\n",
    "    confidence_data = [train_correct, train_incorrect, test_correct, test_incorrect]\n",
    "    labels = [\"Train Correct\", \"Train Incorrect\", \"Test Correct\", \"Test Incorrect\"]\n",
    "\n",
    "    ax2.bar(labels, confidence_data)\n",
    "\n",
    "    ax2.set_xlabel(\"Prediction Category\")\n",
    "    ax2.set_ylabel(\"Mean Confidence Score\")\n",
    "    ax2.set_title(\"Confidence Score Analysis\")\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(confidence_data):\n",
    "        ax2.text(i, v + 0.01, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./metrics_visualization/generalization_gap.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Calibration analysis (confidence vs accuracy)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Create confidence bins\n",
    "    bins = np.linspace(0, 1, 11)  # 10 bins: 0-0.1, 0.1-0.2, etc.\n",
    "\n",
    "    # Process training data\n",
    "    train_bin_accuracies, train_bin_counts = calculate_calibration(\n",
    "        metrics[\"training\"], all_models, bins\n",
    "    )\n",
    "\n",
    "    # Process testing data\n",
    "    test_bin_accuracies, test_bin_counts = calculate_calibration(\n",
    "        metrics[\"testing\"], all_models, bins\n",
    "    )\n",
    "\n",
    "    # Plot calibration curves\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\n",
    "    ax1.plot(bin_centers, train_bin_accuracies, \"o-\", label=\"Training\")\n",
    "    ax1.plot(bin_centers, test_bin_accuracies, \"s-\", label=\"Testing\")\n",
    "\n",
    "    ax1.set_xlabel(\"Mean Confidence\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_title(\"Calibration Curve (Confidence vs Accuracy)\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Plot counts per bin\n",
    "    width = 0.35\n",
    "    x = np.arange(len(bin_centers))\n",
    "\n",
    "    ax2.bar(x - width / 2, train_bin_counts, width, label=\"Training\")\n",
    "    ax2.bar(x + width / 2, test_bin_counts, width, label=\"Testing\")\n",
    "\n",
    "    ax2.set_xlabel(\"Confidence Bin\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.set_title(\"Samples per Confidence Bin\")\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([f\"{b:.1f}-{b+0.1:.1f}\" for b in bin_centers], rotation=45)\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./metrics_visualization/generalization_calibration.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def calculate_calibration(\n",
    "    data: Dict, all_models: List[str], bins: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Calculate calibration data for confidence vs accuracy\n",
    "    bin_accuracies = np.zeros(len(bins) - 1)\n",
    "    bin_counts = np.zeros(len(bins) - 1)\n",
    "\n",
    "    # Collect all confidence scores and corresponding correctness\n",
    "    confidences = []\n",
    "    correctness = []\n",
    "\n",
    "    for model_data in data[\"per_model\"].values():\n",
    "        tp = model_data[\"true_positives\"]\n",
    "        fp = model_data[\"false_positives\"]\n",
    "        fn = model_data[\"false_negatives\"]\n",
    "\n",
    "        # Skip models with insufficient data\n",
    "        if tp + fp + fn < 5:\n",
    "            continue\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "\n",
    "        # Use model mean confidence as a proxy\n",
    "        # In a real implementation, we'd use individual prediction confidences\n",
    "        mean_confidence = 0.5  # Placeholder\n",
    "\n",
    "        confidences.append(mean_confidence)\n",
    "        correctness.append(accuracy)\n",
    "\n",
    "    # Bin the data\n",
    "    for i in range(len(bins) - 1):\n",
    "        bin_mask = (confidences >= bins[i]) & (confidences < bins[i + 1])\n",
    "        if np.sum(bin_mask) > 0:\n",
    "            bin_accuracies[i] = np.mean(np.array(correctness)[bin_mask])\n",
    "            bin_counts[i] = np.sum(bin_mask)\n",
    "        else:\n",
    "            bin_accuracies[i] = 0\n",
    "            bin_counts[i] = 0\n",
    "\n",
    "    return bin_accuracies, bin_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ea7662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run cross-validation evaluation for more robust generalization assessment\n",
    "def evaluate_with_cross_validation(\n",
    "    results: List[Dict], n_folds: int = 15, random_seed: int = 42\n",
    ") -> Dict:\n",
    "    # Evaluate model generalization using k-fold cross-validation\n",
    "\n",
    "    # results: List of prediction results with true models and confidence scores\n",
    "    # n_folds: Number of folds for cross-validation\n",
    "    # random_seed: Seed for reproducibility\n",
    "\n",
    "    # Set random seed\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Extract publications with ground truth\n",
    "    publications_with_truth = [\n",
    "        result\n",
    "        for result in results\n",
    "        if \"models\" in result and result[\"models\"] and \"confidence_scores\" in result\n",
    "    ]\n",
    "\n",
    "    if not publications_with_truth:\n",
    "        return {\"error\": \"No publications with ground truth for evaluation\"}\n",
    "\n",
    "    # Get all unique models\n",
    "    all_models = set()\n",
    "    for pub in publications_with_truth:\n",
    "        all_models.update(pub.get(\"models\", []))\n",
    "    all_models = sorted(list(all_models))\n",
    "\n",
    "    # Shuffle the data\n",
    "    np.random.shuffle(publications_with_truth)\n",
    "\n",
    "    # Prepare folds\n",
    "    fold_size = len(publications_with_truth) // n_folds\n",
    "    folds = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        start_idx = i * fold_size\n",
    "        end_idx = (\n",
    "            start_idx + fold_size if i < n_folds - 1 else len(publications_with_truth)\n",
    "        )\n",
    "        folds.append(publications_with_truth[start_idx:end_idx])\n",
    "\n",
    "    print(f\"Created {n_folds} folds with ~{fold_size} publications each\")\n",
    "\n",
    "    # Cross-validation results\n",
    "    fold_metrics = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        print(f\"Processing fold {i+1}/{n_folds}...\")\n",
    "\n",
    "        # Create test and train sets\n",
    "        test_fold = folds[i]\n",
    "        train_folds = [fold for j, fold in enumerate(folds) if j != i]\n",
    "        train_data = [item for fold in train_folds for item in fold]  # Flatten\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_metrics = calculate_set_metrics(train_data, all_models)\n",
    "        test_metrics = calculate_set_metrics(test_fold, all_models)\n",
    "\n",
    "        # Calculate differences\n",
    "        differences = {}\n",
    "        for model in all_models:\n",
    "            if (\n",
    "                model in train_metrics[\"per_model\"]\n",
    "                and model in test_metrics[\"per_model\"]\n",
    "            ):\n",
    "                train_model = train_metrics[\"per_model\"][model]\n",
    "                test_model = test_metrics[\"per_model\"][model]\n",
    "\n",
    "                differences[model] = {\n",
    "                    \"precision_diff\": test_model[\"precision\"]\n",
    "                    - train_model[\"precision\"],\n",
    "                    \"recall_diff\": test_model[\"recall\"] - train_model[\"recall\"],\n",
    "                    \"f1_diff\": test_model[\"f1\"] - train_model[\"f1\"],\n",
    "                }\n",
    "\n",
    "        # Overall difference\n",
    "        train_overall = train_metrics[\"overall\"]\n",
    "        test_overall = test_metrics[\"overall\"]\n",
    "\n",
    "        differences[\"overall\"] = {\n",
    "            \"precision_diff\": test_overall[\"precision\"] - train_overall[\"precision\"],\n",
    "            \"recall_diff\": test_overall[\"recall\"] - train_overall[\"recall\"],\n",
    "            \"f1_diff\": test_overall[\"f1\"] - train_overall[\"f1\"],\n",
    "        }\n",
    "\n",
    "        # Add to fold metrics\n",
    "        fold_metrics.append(\n",
    "            {\n",
    "                \"train\": train_metrics,\n",
    "                \"test\": test_metrics,\n",
    "                \"diff\": differences,\n",
    "                \"fold_idx\": i,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Aggregate results across folds\n",
    "    cv_results = {\n",
    "        \"per_fold\": fold_metrics,\n",
    "        \"aggregated\": {\n",
    "            \"train\": {\n",
    "                \"overall\": {\n",
    "                    \"precision\": np.mean(\n",
    "                        [fold[\"train\"][\"overall\"][\"precision\"] for fold in fold_metrics]\n",
    "                    ),\n",
    "                    \"recall\": np.mean(\n",
    "                        [fold[\"train\"][\"overall\"][\"recall\"] for fold in fold_metrics]\n",
    "                    ),\n",
    "                    \"f1\": np.mean(\n",
    "                        [fold[\"train\"][\"overall\"][\"f1\"] for fold in fold_metrics]\n",
    "                    ),\n",
    "                },\n",
    "                \"per_model\": {},\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"overall\": {\n",
    "                    \"precision\": np.mean(\n",
    "                        [fold[\"test\"][\"overall\"][\"precision\"] for fold in fold_metrics]\n",
    "                    ),\n",
    "                    \"recall\": np.mean(\n",
    "                        [fold[\"test\"][\"overall\"][\"recall\"] for fold in fold_metrics]\n",
    "                    ),\n",
    "                    \"f1\": np.mean(\n",
    "                        [fold[\"test\"][\"overall\"][\"f1\"] for fold in fold_metrics]\n",
    "                    ),\n",
    "                },\n",
    "                \"per_model\": {},\n",
    "            },\n",
    "            \"diff\": {\n",
    "                \"overall\": {\n",
    "                    \"precision_diff\": np.mean(\n",
    "                        [\n",
    "                            fold[\"diff\"][\"overall\"][\"precision_diff\"]\n",
    "                            for fold in fold_metrics\n",
    "                        ]\n",
    "                    ),\n",
    "                    \"recall_diff\": np.mean(\n",
    "                        [\n",
    "                            fold[\"diff\"][\"overall\"][\"recall_diff\"]\n",
    "                            for fold in fold_metrics\n",
    "                        ]\n",
    "                    ),\n",
    "                    \"f1_diff\": np.mean(\n",
    "                        [fold[\"diff\"][\"overall\"][\"f1_diff\"] for fold in fold_metrics]\n",
    "                    ),\n",
    "                },\n",
    "                \"per_model\": {},\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Aggregate per-model metrics\n",
    "    for model in all_models:\n",
    "        # Check if model exists in all folds\n",
    "        if all(model in fold[\"train\"][\"per_model\"] for fold in fold_metrics) and all(\n",
    "            model in fold[\"test\"][\"per_model\"] for fold in fold_metrics\n",
    "        ):\n",
    "            cv_results[\"aggregated\"][\"train\"][\"per_model\"][model] = {\n",
    "                \"precision\": np.mean(\n",
    "                    [\n",
    "                        fold[\"train\"][\"per_model\"][model][\"precision\"]\n",
    "                        for fold in fold_metrics\n",
    "                    ]\n",
    "                ),\n",
    "                \"recall\": np.mean(\n",
    "                    [\n",
    "                        fold[\"train\"][\"per_model\"][model][\"recall\"]\n",
    "                        for fold in fold_metrics\n",
    "                    ]\n",
    "                ),\n",
    "                \"f1\": np.mean(\n",
    "                    [fold[\"train\"][\"per_model\"][model][\"f1\"] for fold in fold_metrics]\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            cv_results[\"aggregated\"][\"test\"][\"per_model\"][model] = {\n",
    "                \"precision\": np.mean(\n",
    "                    [\n",
    "                        fold[\"test\"][\"per_model\"][model][\"precision\"]\n",
    "                        for fold in fold_metrics\n",
    "                    ]\n",
    "                ),\n",
    "                \"recall\": np.mean(\n",
    "                    [\n",
    "                        fold[\"test\"][\"per_model\"][model][\"recall\"]\n",
    "                        for fold in fold_metrics\n",
    "                    ]\n",
    "                ),\n",
    "                \"f1\": np.mean(\n",
    "                    [fold[\"test\"][\"per_model\"][model][\"f1\"] for fold in fold_metrics]\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            cv_results[\"aggregated\"][\"diff\"][\"per_model\"][model] = {\n",
    "                \"precision_diff\": np.mean(\n",
    "                    [fold[\"diff\"][model][\"precision_diff\"] for fold in fold_metrics]\n",
    "                ),\n",
    "                \"recall_diff\": np.mean(\n",
    "                    [fold[\"diff\"][model][\"recall_diff\"] for fold in fold_metrics]\n",
    "                ),\n",
    "                \"f1_diff\": np.mean(\n",
    "                    [fold[\"diff\"][model][\"f1_diff\"] for fold in fold_metrics]\n",
    "                ),\n",
    "            }\n",
    "\n",
    "    # Calculate generalization gap\n",
    "    generalization_gap = abs(\n",
    "        cv_results[\"aggregated\"][\"train\"][\"overall\"][\"f1\"]\n",
    "        - cv_results[\"aggregated\"][\"test\"][\"overall\"][\"f1\"]\n",
    "    )\n",
    "\n",
    "    cv_results[\"generalization_gap\"] = {\n",
    "        \"f1_gap\": generalization_gap,\n",
    "        \"relative_gap\": generalization_gap\n",
    "        / cv_results[\"aggregated\"][\"train\"][\"overall\"][\"f1\"]\n",
    "        if cv_results[\"aggregated\"][\"train\"][\"overall\"][\"f1\"] > 0\n",
    "        else float(\"inf\"),\n",
    "        \"gap_assessment\": assess_generalization_gap(generalization_gap),\n",
    "    }\n",
    "\n",
    "    # Visualize cross-validation results\n",
    "    visualize_cross_validation_results(cv_results, all_models, n_folds)\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "\n",
    "def visualize_cross_validation_results(\n",
    "    cv_results: Dict, all_models: List[str], n_folds: int\n",
    "):\n",
    "    \"\"\"Generate visualizations for cross-validation results\"\"\"\n",
    "    # 1. Fold comparison for overall F1 scores\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Extract data\n",
    "    fold_indices = list(range(1, n_folds + 1))\n",
    "    train_f1_by_fold = [\n",
    "        fold[\"train\"][\"overall\"][\"f1\"] for fold in cv_results[\"per_fold\"]\n",
    "    ]\n",
    "    test_f1_by_fold = [fold[\"test\"][\"overall\"][\"f1\"] for fold in cv_results[\"per_fold\"]]\n",
    "    f1_diff_by_fold = [\n",
    "        fold[\"diff\"][\"overall\"][\"f1_diff\"] for fold in cv_results[\"per_fold\"]\n",
    "    ]\n",
    "\n",
    "    # Plot F1 scores across folds\n",
    "    width = 0.35\n",
    "    ax1.bar(\n",
    "        [i - width / 2 for i in fold_indices], train_f1_by_fold, width, label=\"Train F1\"\n",
    "    )\n",
    "    ax1.bar(\n",
    "        [i + width / 2 for i in fold_indices], test_f1_by_fold, width, label=\"Test F1\"\n",
    "    )\n",
    "\n",
    "    ax1.set_xlabel(\"Fold\")\n",
    "    ax1.set_ylabel(\"F1 Score\")\n",
    "    ax1.set_title(\"F1 Score by Fold\")\n",
    "    ax1.set_xticks(fold_indices)\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add mean line\n",
    "    ax1.axhline(\n",
    "        y=np.mean(train_f1_by_fold),\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f\"Mean Train F1: {np.mean(train_f1_by_fold):.3f}\",\n",
    "    )\n",
    "    ax1.axhline(\n",
    "        y=np.mean(test_f1_by_fold),\n",
    "        color=\"orange\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f\"Mean Test F1: {np.mean(test_f1_by_fold):.3f}\",\n",
    "    )\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot F1 differences across folds\n",
    "    colors = [\"green\" if diff >= 0 else \"red\" for diff in f1_diff_by_fold]\n",
    "    ax2.bar(fold_indices, f1_diff_by_fold, color=colors)\n",
    "\n",
    "    ax2.set_xlabel(\"Fold\")\n",
    "    ax2.set_ylabel(\"F1 Difference (Test - Train)\")\n",
    "    ax2.set_title(\"F1 Score Difference by Fold\")\n",
    "    ax2.set_xticks(fold_indices)\n",
    "    ax2.axhline(y=0, color=\"black\", linestyle=\"-\", alpha=0.3)\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add mean line\n",
    "    mean_diff = np.mean(f1_diff_by_fold)\n",
    "    ax2.axhline(\n",
    "        y=mean_diff,\n",
    "        color=\"purple\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f\"Mean Diff: {mean_diff:.3f}\",\n",
    "    )\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./metrics_visualization/cv_folds.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Per-model generalization performance\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Extract data for models with results across all folds\n",
    "    models_with_data = [\n",
    "        model\n",
    "        for model in all_models\n",
    "        if model in cv_results[\"aggregated\"][\"train\"][\"per_model\"]\n",
    "        and model in cv_results[\"aggregated\"][\"test\"][\"per_model\"]\n",
    "    ]\n",
    "\n",
    "    if models_with_data:\n",
    "        train_f1 = [\n",
    "            cv_results[\"aggregated\"][\"train\"][\"per_model\"][model][\"f1\"]\n",
    "            for model in models_with_data\n",
    "        ]\n",
    "        test_f1 = [\n",
    "            cv_results[\"aggregated\"][\"test\"][\"per_model\"][model][\"f1\"]\n",
    "            for model in models_with_data\n",
    "        ]\n",
    "        f1_diffs = [test - train for train, test in zip(train_f1, test_f1)]\n",
    "\n",
    "        # Sort by generalization gap (absolute difference)\n",
    "        sorted_data = sorted(\n",
    "            zip(models_with_data, train_f1, test_f1, f1_diffs),\n",
    "            key=lambda x: abs(x[3]),\n",
    "            reverse=True,\n",
    "        )\n",
    "\n",
    "        models_sorted = [x[0] for x in sorted_data]\n",
    "        train_f1_sorted = [x[1] for x in sorted_data]\n",
    "        test_f1_sorted = [x[2] for x in sorted_data]\n",
    "\n",
    "        # Plot train vs test F1 for each model\n",
    "        x = np.arange(len(models_sorted))\n",
    "        width = 0.35\n",
    "\n",
    "        ax.bar(x - width / 2, train_f1_sorted, width, label=\"Train F1\")\n",
    "        ax.bar(x + width / 2, test_f1_sorted, width, label=\"Test F1\")\n",
    "\n",
    "        # Add connecting lines to visualize gap\n",
    "        for i, (train, test) in enumerate(zip(train_f1_sorted, test_f1_sorted)):\n",
    "            ax.plot([i - width / 2, i + width / 2], [train, test], \"k-\", alpha=0.3)\n",
    "\n",
    "        ax.set_xlabel(\"Model\")\n",
    "        ax.set_ylabel(\"F1 Score\")\n",
    "        ax.set_title(\"Cross-Validation: Train vs Test F1 by Model\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models_sorted, rotation=45, ha=\"right\")\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        # Add overall average F1 lines\n",
    "        ax.axhline(\n",
    "            y=cv_results[\"aggregated\"][\"train\"][\"overall\"][\"f1\"],\n",
    "            color=\"blue\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.5,\n",
    "            label=f\"Avg Train F1: {cv_results['aggregated']['train']['overall']['f1']:.3f}\",\n",
    "        )\n",
    "        ax.axhline(\n",
    "            y=cv_results[\"aggregated\"][\"test\"][\"overall\"][\"f1\"],\n",
    "            color=\"orange\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.5,\n",
    "            label=f\"Avg Test F1: {cv_results['aggregated']['test']['overall']['f1']:.3f}\",\n",
    "        )\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Insufficient data for model comparison\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./metrics_visualization/cv_models.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Metrics variance across folds\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Calculate variances for overall metrics\n",
    "    metrics_names = [\"Precision\", \"Recall\", \"F1 Score\"]\n",
    "    train_variances = [\n",
    "        np.var(\n",
    "            [fold[\"train\"][\"overall\"][\"precision\"] for fold in cv_results[\"per_fold\"]]\n",
    "        ),\n",
    "        np.var([fold[\"train\"][\"overall\"][\"recall\"] for fold in cv_results[\"per_fold\"]]),\n",
    "        np.var([fold[\"train\"][\"overall\"][\"f1\"] for fold in cv_results[\"per_fold\"]]),\n",
    "    ]\n",
    "    test_variances = [\n",
    "        np.var(\n",
    "            [fold[\"test\"][\"overall\"][\"precision\"] for fold in cv_results[\"per_fold\"]]\n",
    "        ),\n",
    "        np.var([fold[\"test\"][\"overall\"][\"recall\"] for fold in cv_results[\"per_fold\"]]),\n",
    "        np.var([fold[\"test\"][\"overall\"][\"f1\"] for fold in cv_results[\"per_fold\"]]),\n",
    "    ]\n",
    "\n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.35\n",
    "\n",
    "    ax.bar(x - width / 2, train_variances, width, label=\"Train Variance\")\n",
    "    ax.bar(x + width / 2, test_variances, width, label=\"Test Variance\")\n",
    "\n",
    "    ax.set_xlabel(\"Metric\")\n",
    "    ax.set_ylabel(\"Variance\")\n",
    "    ax.set_title(\"Metric Variance Across Folds\")\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_names)\n",
    "    ax.legend()\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(train_variances):\n",
    "        ax.text(i - width / 2, v + 0.001, f\"{v:.4f}\", ha=\"center\", va=\"bottom\")\n",
    "    for i, v in enumerate(test_variances):\n",
    "        ax.text(i + width / 2, v + 0.001, f\"{v:.4f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./metrics_visualization/cv_variance.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24f44736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional evaluation function to analyze classifier robustness\n",
    "def analyze_classifier_robustness(results: List[Dict]) -> Dict:\n",
    "    # Analyze the robustness of the classifier by studying:\n",
    "    # 1. Performance on boundary cases (near threshold)\n",
    "    # 2. Contribution of different evidence sources\n",
    "    # 3. Consistency of classifications across models\n",
    "\n",
    "    # Extract publications with ground truth\n",
    "    publications_with_truth = [\n",
    "        result\n",
    "        for result in results\n",
    "        if \"models\" in result and result[\"models\"] and \"confidence_scores\" in result\n",
    "    ]\n",
    "\n",
    "    if not publications_with_truth:\n",
    "        return {\"error\": \"No publications with ground truth for evaluation\"}\n",
    "\n",
    "    # Get all unique models\n",
    "    all_models = set()\n",
    "    for pub in publications_with_truth:\n",
    "        all_models.update(pub.get(\"models\", []))\n",
    "        all_models.update(pub.get(\"matched_models\", []))\n",
    "    all_models = sorted(list(all_models))\n",
    "\n",
    "    # Initialize robustness metrics\n",
    "    robustness_metrics = {\n",
    "        \"boundary_cases\": analyze_boundary_cases(publications_with_truth, all_models),\n",
    "        \"evidence_sources\": analyze_evidence_sources(\n",
    "            publications_with_truth, all_models\n",
    "        ),\n",
    "        \"classification_consistency\": analyze_classification_consistency(\n",
    "            publications_with_truth, all_models\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # Visualize robustness metrics\n",
    "    visualize_robustness_metrics(robustness_metrics)\n",
    "\n",
    "    return robustness_metrics\n",
    "\n",
    "\n",
    "def analyze_boundary_cases(publications: List[Dict], all_models: List[str]) -> Dict:\n",
    "    # Analyze performance on boundary cases (confidence scores near thresholds)\n",
    "    # Define boundary ranges (near thresholds)\n",
    "    boundary_width = 0.1  # Define boundary as +/- 0.1 from threshold\n",
    "\n",
    "    boundary_metrics = {\n",
    "        model: {\n",
    "            \"boundary_count\": 0,\n",
    "            \"boundary_correct\": 0,\n",
    "            \"non_boundary_count\": 0,\n",
    "            \"non_boundary_correct\": 0,\n",
    "        }\n",
    "        for model in all_models\n",
    "    }\n",
    "\n",
    "    # Collect boundary cases and their performance\n",
    "    for pub in publications:\n",
    "        true_models = set(pub.get(\"models\", []))\n",
    "\n",
    "        for model in all_models:\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            threshold = MODEL_THRESHOLDS.get(model, 0.4)\n",
    "\n",
    "            # Is this a boundary case?\n",
    "            is_boundary = abs(confidence - threshold) < boundary_width\n",
    "            is_true_match = model in true_models\n",
    "            is_predicted = confidence >= threshold\n",
    "            is_correct = (is_predicted and is_true_match) or (\n",
    "                not is_predicted and not is_true_match\n",
    "            )\n",
    "\n",
    "            if is_boundary:\n",
    "                boundary_metrics[model][\"boundary_count\"] += 1\n",
    "                if is_correct:\n",
    "                    boundary_metrics[model][\"boundary_correct\"] += 1\n",
    "            else:\n",
    "                boundary_metrics[model][\"non_boundary_count\"] += 1\n",
    "                if is_correct:\n",
    "                    boundary_metrics[model][\"non_boundary_correct\"] += 1\n",
    "\n",
    "    # Calculate boundary vs non-boundary accuracy\n",
    "    for model in all_models:\n",
    "        # Boundary accuracy\n",
    "        boundary_count = boundary_metrics[model][\"boundary_count\"]\n",
    "        boundary_correct = boundary_metrics[model][\"boundary_correct\"]\n",
    "        boundary_metrics[model][\"boundary_accuracy\"] = (\n",
    "            boundary_correct / boundary_count if boundary_count > 0 else 0\n",
    "        )\n",
    "\n",
    "        # Non-boundary accuracy\n",
    "        non_boundary_count = boundary_metrics[model][\"non_boundary_count\"]\n",
    "        non_boundary_correct = boundary_metrics[model][\"non_boundary_correct\"]\n",
    "        boundary_metrics[model][\"non_boundary_accuracy\"] = (\n",
    "            non_boundary_correct / non_boundary_count if non_boundary_count > 0 else 0\n",
    "        )\n",
    "\n",
    "        # Calculate robustness score (higher is better)\n",
    "        boundary_metrics[model][\"robustness_score\"] = boundary_metrics[model][\n",
    "            \"boundary_accuracy\"\n",
    "        ] / max(0.001, 1 - boundary_metrics[model][\"non_boundary_accuracy\"])\n",
    "\n",
    "    # Overall metrics\n",
    "    total_boundary_correct = sum(\n",
    "        metrics[\"boundary_correct\"] for metrics in boundary_metrics.values()\n",
    "    )\n",
    "    total_boundary_count = sum(\n",
    "        metrics[\"boundary_count\"] for metrics in boundary_metrics.values()\n",
    "    )\n",
    "    total_non_boundary_correct = sum(\n",
    "        metrics[\"non_boundary_correct\"] for metrics in boundary_metrics.values()\n",
    "    )\n",
    "    total_non_boundary_count = sum(\n",
    "        metrics[\"non_boundary_count\"] for metrics in boundary_metrics.values()\n",
    "    )\n",
    "\n",
    "    overall_boundary_accuracy = (\n",
    "        total_boundary_correct / total_boundary_count if total_boundary_count > 0 else 0\n",
    "    )\n",
    "    overall_non_boundary_accuracy = (\n",
    "        total_non_boundary_correct / total_non_boundary_count\n",
    "        if total_non_boundary_count > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"per_model\": boundary_metrics,\n",
    "        \"overall\": {\n",
    "            \"boundary_accuracy\": overall_boundary_accuracy,\n",
    "            \"non_boundary_accuracy\": overall_non_boundary_accuracy,\n",
    "            \"total_boundary_count\": total_boundary_count,\n",
    "            \"total_non_boundary_count\": total_non_boundary_count,\n",
    "            \"robustness_score\": overall_boundary_accuracy\n",
    "            / max(0.001, 1 - overall_non_boundary_accuracy),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_evidence_sources(publications: List[Dict], all_models: List[str]) -> Dict:\n",
    "    # Analyze contribution of different evidence sources to prediction accuracy\n",
    "    # Initialize evidence source metrics\n",
    "    source_metrics = {}\n",
    "\n",
    "    # Collect all unique sources\n",
    "    all_sources = set()\n",
    "    for pub in publications:\n",
    "        for model in all_models:\n",
    "            sources = pub.get(\"confidence_sources\", {}).get(model, [])\n",
    "            all_sources.update(sources)\n",
    "\n",
    "    # Initialize metrics for each source\n",
    "    for source in all_sources:\n",
    "        source_metrics[source] = {\n",
    "            \"count\": 0,\n",
    "            \"correct\": 0,\n",
    "            \"incorrect\": 0,\n",
    "            \"true_positives\": 0,\n",
    "            \"false_positives\": 0,\n",
    "            \"per_model\": {model: {\"count\": 0, \"correct\": 0} for model in all_models},\n",
    "        }\n",
    "\n",
    "    # Collect metrics by source\n",
    "    for pub in publications:\n",
    "        true_models = set(pub.get(\"models\", []))\n",
    "\n",
    "        for model in all_models:\n",
    "            sources = pub.get(\"confidence_sources\", {}).get(model, [])\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            threshold = MODEL_THRESHOLDS.get(model, 0.4)\n",
    "\n",
    "            is_predicted = confidence >= threshold\n",
    "            is_true_match = model in true_models\n",
    "            is_correct = (is_predicted and is_true_match) or (\n",
    "                not is_predicted and not is_true_match\n",
    "            )\n",
    "\n",
    "            for source in sources:\n",
    "                # Update overall source metrics\n",
    "                source_metrics[source][\"count\"] += 1\n",
    "                if is_correct:\n",
    "                    source_metrics[source][\"correct\"] += 1\n",
    "                else:\n",
    "                    source_metrics[source][\"incorrect\"] += 1\n",
    "\n",
    "                # Track true/false positives\n",
    "                if is_predicted and is_true_match:\n",
    "                    source_metrics[source][\"true_positives\"] += 1\n",
    "                elif is_predicted and not is_true_match:\n",
    "                    source_metrics[source][\"false_positives\"] += 1\n",
    "\n",
    "                # Update per-model metrics\n",
    "                source_metrics[source][\"per_model\"][model][\"count\"] += 1\n",
    "                if is_correct:\n",
    "                    source_metrics[source][\"per_model\"][model][\"correct\"] += 1\n",
    "\n",
    "    # Calculate accuracies and contribution scores\n",
    "    for source in source_metrics:\n",
    "        # Skip sources with too few samples\n",
    "        if source_metrics[source][\"count\"] < 5:\n",
    "            continue\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = source_metrics[source][\"correct\"] / source_metrics[source][\"count\"]\n",
    "        source_metrics[source][\"accuracy\"] = accuracy\n",
    "\n",
    "        # Calculate precision\n",
    "        tp = source_metrics[source][\"true_positives\"]\n",
    "        fp = source_metrics[source][\"false_positives\"]\n",
    "        source_metrics[source][\"precision\"] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "\n",
    "        # Calculate per-model accuracies\n",
    "        for model in all_models:\n",
    "            model_count = source_metrics[source][\"per_model\"][model][\"count\"]\n",
    "            if model_count > 0:\n",
    "                model_correct = source_metrics[source][\"per_model\"][model][\"correct\"]\n",
    "                source_metrics[source][\"per_model\"][model][\"accuracy\"] = (\n",
    "                    model_correct / model_count\n",
    "                )\n",
    "\n",
    "    return {\n",
    "        \"per_source\": source_metrics,\n",
    "        \"overall\": {\n",
    "            \"unique_sources\": len(source_metrics),\n",
    "            \"top_sources\": sorted(\n",
    "                [\n",
    "                    (s, m[\"accuracy\"], m[\"count\"])\n",
    "                    for s, m in source_metrics.items()\n",
    "                    if m[\"count\"] >= 5\n",
    "                ],\n",
    "                key=lambda x: x[1]\n",
    "                * math.log(x[2]),  # Sort by accuracy weighted by log of count\n",
    "                reverse=True,\n",
    "            )[:5],\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_classification_consistency(\n",
    "    publications: List[Dict], all_models: List[str]\n",
    ") -> Dict:\n",
    "    # Analyze consistency of classifications across different models\n",
    "    # Initialize consistency metrics\n",
    "    consistency_metrics = {\n",
    "        \"publication_consistency\": [],\n",
    "        \"model_consistency\": {\n",
    "            model: {\"consistency_score\": 0, \"samples\": 0} for model in all_models\n",
    "        },\n",
    "        \"confidence_vs_accuracy\": {\"bins\": [], \"accuracy\": []},\n",
    "    }\n",
    "\n",
    "    # Create confidence bins\n",
    "    bins = np.linspace(0, 1, 11)  # 10 bins\n",
    "    bin_counts = np.zeros(10)\n",
    "    bin_correct = np.zeros(10)\n",
    "\n",
    "    # Analyze consistency for each publication\n",
    "    for pub in publications:\n",
    "        true_models = set(pub.get(\"models\", []))\n",
    "        pred_models = set(pub.get(\"matched_models\", []))\n",
    "\n",
    "        # Calculate agreement ratio for this publication\n",
    "        total_models = len(true_models.union(pred_models))\n",
    "        if total_models > 0:\n",
    "            agreement = len(true_models.intersection(pred_models)) / total_models\n",
    "            consistency_metrics[\"publication_consistency\"].append(agreement)\n",
    "\n",
    "        # Analyze per-model consistency\n",
    "        for model in all_models:\n",
    "            confidence = pub.get(\"confidence_scores\", {}).get(model, 0)\n",
    "            is_true_match = model in true_models\n",
    "\n",
    "            # Update confidence bins\n",
    "            bin_idx = min(9, int(confidence * 10))\n",
    "            bin_counts[bin_idx] += 1\n",
    "            if (confidence >= MODEL_THRESHOLDS.get(model, 0.4) and is_true_match) or (\n",
    "                confidence < MODEL_THRESHOLDS.get(model, 0.4) and not is_true_match\n",
    "            ):\n",
    "                bin_correct[bin_idx] += 1\n",
    "\n",
    "            # Skip if not enough data for this model\n",
    "            if not true_models:\n",
    "                continue\n",
    "\n",
    "            # Calculate how well this model's prediction aligns with others\n",
    "            other_true_models = true_models - {model}\n",
    "            if not other_true_models:\n",
    "                continue\n",
    "\n",
    "            # Is this model's prediction consistent with others?\n",
    "            is_predicted = confidence >= MODEL_THRESHOLDS.get(model, 0.4)\n",
    "\n",
    "            if is_true_match:\n",
    "                # For true matches, check if other true models are also predicted\n",
    "                other_pred_models = (\n",
    "                    pred_models - {model} if is_predicted else pred_models\n",
    "                )\n",
    "                overlap = len(other_true_models.intersection(other_pred_models))\n",
    "                consistency = (\n",
    "                    overlap / len(other_true_models) if other_true_models else 0\n",
    "                )\n",
    "\n",
    "                consistency_metrics[\"model_consistency\"][model][\n",
    "                    \"consistency_score\"\n",
    "                ] += consistency\n",
    "                consistency_metrics[\"model_consistency\"][model][\"samples\"] += 1\n",
    "\n",
    "    # Calculate average consistency for each model\n",
    "    for model in all_models:\n",
    "        samples = consistency_metrics[\"model_consistency\"][model][\"samples\"]\n",
    "        if samples > 0:\n",
    "            consistency_metrics[\"model_consistency\"][model][\"consistency_score\"] /= (\n",
    "                samples\n",
    "            )\n",
    "\n",
    "    # Calculate overall consistency\n",
    "    if consistency_metrics[\"publication_consistency\"]:\n",
    "        consistency_metrics[\"overall_consistency\"] = sum(\n",
    "            consistency_metrics[\"publication_consistency\"]\n",
    "        ) / len(consistency_metrics[\"publication_consistency\"])\n",
    "    else:\n",
    "        consistency_metrics[\"overall_consistency\"] = 0\n",
    "\n",
    "    # Prepare confidence vs accuracy data\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_accuracy = np.zeros(10)\n",
    "\n",
    "    for i in range(10):\n",
    "        if bin_counts[i] > 0:\n",
    "            bin_accuracy[i] = bin_correct[i] / bin_counts[i]\n",
    "\n",
    "    consistency_metrics[\"confidence_vs_accuracy\"] = {\n",
    "        \"bins\": bin_centers,\n",
    "        \"accuracy\": bin_accuracy,\n",
    "        \"counts\": bin_counts,\n",
    "    }\n",
    "\n",
    "    return consistency_metrics\n",
    "\n",
    "\n",
    "def visualize_robustness_metrics(metrics: Dict):\n",
    "    # Generate visualizations for robustness metrics\n",
    "    # 1. Boundary vs Non-Boundary Accuracy\n",
    "    boundary_metrics = metrics[\"boundary_cases\"]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Extract data\n",
    "    models = sorted(boundary_metrics[\"per_model\"].keys())\n",
    "    boundary_accuracy = [\n",
    "        boundary_metrics[\"per_model\"][model][\"boundary_accuracy\"] for model in models\n",
    "    ]\n",
    "    non_boundary_accuracy = [\n",
    "        boundary_metrics[\"per_model\"][model][\"non_boundary_accuracy\"]\n",
    "        for model in models\n",
    "    ]\n",
    "    boundary_counts = [\n",
    "        boundary_metrics[\"per_model\"][model][\"boundary_count\"] for model in models\n",
    "    ]\n",
    "\n",
    "    # Plot boundary vs non-boundary accuracy\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax1.bar(x - width / 2, boundary_accuracy, width, label=\"Boundary\")\n",
    "    bars2 = ax1.bar(x + width / 2, non_boundary_accuracy, width, label=\"Non-Boundary\")\n",
    "\n",
    "    ax1.set_xlabel(\"Model\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_title(\"Boundary vs Non-Boundary Accuracy by Model\")\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Plot boundary case counts\n",
    "    ax2.bar(models, boundary_counts)\n",
    "    ax2.set_xlabel(\"Model\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.set_title(\"Boundary Case Count by Model\")\n",
    "    ax2.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    # Add overall accuracy line\n",
    "    ax1.axhline(\n",
    "        y=boundary_metrics[\"overall\"][\"boundary_accuracy\"],\n",
    "        color=\"blue\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f'Overall Boundary: {boundary_metrics[\"overall\"][\"boundary_accuracy\"]:.3f}',\n",
    "    )\n",
    "    ax1.axhline(\n",
    "        y=boundary_metrics[\"overall\"][\"non_boundary_accuracy\"],\n",
    "        color=\"orange\",\n",
    "        linestyle=\"--\",\n",
    "        alpha=0.7,\n",
    "        label=f'Overall Non-Boundary: {boundary_metrics[\"overall\"][\"non_boundary_accuracy\"]:.3f}',\n",
    "    )\n",
    "    ax1.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./metrics_visualization/robustness_boundary.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Evidence Source Analysis\n",
    "    source_metrics = metrics[\"evidence_sources\"][\"per_source\"]\n",
    "\n",
    "    # Filter sources with sufficient samples\n",
    "    sources = [\n",
    "        source\n",
    "        for source, metrics in source_metrics.items()\n",
    "        if metrics.get(\"count\", 0) >= 20\n",
    "    ]  # Min 20 samples\n",
    "\n",
    "    if sources:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        # Extract data\n",
    "        accuracies = [source_metrics[s][\"accuracy\"] for s in sources]\n",
    "        counts = [source_metrics[s][\"count\"] for s in sources]\n",
    "\n",
    "        # Sort by accuracy\n",
    "        sorted_data = sorted(\n",
    "            zip(sources, accuracies, counts), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        sources_sorted = [x[0] for x in sorted_data]\n",
    "        accuracies_sorted = [x[1] for x in sorted_data]\n",
    "        counts_sorted = [x[2] for x in sorted_data]\n",
    "\n",
    "        # Truncate to top 10 for better visualization\n",
    "        if len(sources_sorted) > 10:\n",
    "            sources_sorted = sources_sorted[:10]\n",
    "            accuracies_sorted = accuracies_sorted[:10]\n",
    "            counts_sorted = counts_sorted[:10]\n",
    "\n",
    "        # Plot source accuracies\n",
    "        bars = ax1.barh(sources_sorted, accuracies_sorted)\n",
    "\n",
    "        ax1.set_xlabel(\"Accuracy\")\n",
    "        ax1.set_ylabel(\"Evidence Source\")\n",
    "        ax1.set_title(\"Accuracy by Evidence Source\")\n",
    "        ax1.set_xlim(0, 1)\n",
    "        ax1.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        # Add count labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax1.text(\n",
    "                width + 0.01,\n",
    "                bar.get_y() + bar.get_height() / 2,\n",
    "                f\"n={counts_sorted[i]}\",\n",
    "                va=\"center\",\n",
    "            )\n",
    "\n",
    "        # Plot source counts\n",
    "        ax2.barh(sources_sorted, counts_sorted)\n",
    "\n",
    "        ax2.set_xlabel(\"Count\")\n",
    "        ax2.set_ylabel(\"Evidence Source\")\n",
    "        ax2.set_title(\"Usage Count by Evidence Source\")\n",
    "        ax2.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./metrics_visualization/robustness_sources.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # 3. Classification Consistency Analysis\n",
    "    consistency_metrics = metrics[\"classification_consistency\"]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot confidence vs accuracy relationship\n",
    "    confidence_data = consistency_metrics[\"confidence_vs_accuracy\"]\n",
    "\n",
    "    ax1.plot(confidence_data[\"bins\"], confidence_data[\"accuracy\"], \"o-\")\n",
    "    ax1.plot([0, 1], [0, 1], \"k--\", alpha=0.5, label=\"Perfect Calibration\")\n",
    "\n",
    "    ax1.set_xlabel(\"Confidence Score Bin\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_title(\"Confidence vs Accuracy (Calibration)\")\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.grid(True, linestyle=\"--\", alpha=0.7)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Add sample counts\n",
    "    for i, (bin_center, accuracy, count) in enumerate(\n",
    "        zip(\n",
    "            confidence_data[\"bins\"],\n",
    "            confidence_data[\"accuracy\"],\n",
    "            confidence_data[\"counts\"],\n",
    "        )\n",
    "    ):\n",
    "        if count > 0:\n",
    "            ax1.annotate(\n",
    "                f\"n={int(count)}\",\n",
    "                (bin_center, accuracy),\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(0, 10),\n",
    "                ha=\"center\",\n",
    "            )\n",
    "\n",
    "    # Plot model consistency\n",
    "    models = []\n",
    "    consistency_scores = []\n",
    "\n",
    "    for model, data in consistency_metrics[\"model_consistency\"].items():\n",
    "        if data[\"samples\"] >= 10:  # Min 10 samples\n",
    "            models.append(model)\n",
    "            consistency_scores.append(data[\"consistency_score\"])\n",
    "\n",
    "    if models:\n",
    "        # Sort by consistency score\n",
    "        sorted_data = sorted(\n",
    "            zip(models, consistency_scores), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        models_sorted = [x[0] for x in sorted_data]\n",
    "        scores_sorted = [x[1] for x in sorted_data]\n",
    "\n",
    "        ax2.bar(models_sorted, scores_sorted)\n",
    "\n",
    "        ax2.set_xlabel(\"Model\")\n",
    "        ax2.set_ylabel(\"Consistency Score\")\n",
    "        ax2.set_title(\"Model Prediction Consistency\")\n",
    "        ax2.set_xticklabels(models_sorted, rotation=45, ha=\"right\")\n",
    "        ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "        # Add overall consistency line\n",
    "        ax2.axhline(\n",
    "            y=consistency_metrics[\"overall_consistency\"],\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.7,\n",
    "            label=f'Overall: {consistency_metrics[\"overall_consistency\"]:.3f}',\n",
    "        )\n",
    "        ax2.legend()\n",
    "    else:\n",
    "        ax2.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Insufficient data for consistency analysis\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax2.transAxes,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./metrics_visualization/robustness_consistency.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6be63467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generalization evaluations after main processing is complete\n",
    "def evaluate_generalization_metrics(results_path: str = \"./results.json\"):\n",
    "    print(\"\\nEvaluating model generalization to unseen data...\")\n",
    "\n",
    "    # Load results\n",
    "    try:\n",
    "        with open(results_path, \"r\") as f:\n",
    "            results = json.load(f)\n",
    "        print(f\"Loaded {len(results)} publications from results\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading results: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Run simple train/test split evaluation\n",
    "    print(\"\\n1. Running train/test split evaluation...\")\n",
    "    try:\n",
    "        generalization_metrics = evaluate_generalization(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during train/test evaluation: {str(e)}\")\n",
    "        generalization_metrics = None\n",
    "\n",
    "    # Run cross-validation evaluation\n",
    "    print(\"\\n2. Running cross-validation evaluation...\")\n",
    "    try:\n",
    "        cv_metrics = evaluate_with_cross_validation(results, n_folds=5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cross-validation: {str(e)}\")\n",
    "        cv_metrics = None\n",
    "\n",
    "    # Run robustness analysis\n",
    "    print(\"\\n3. Running robustness analysis...\")\n",
    "    try:\n",
    "        robustness_metrics = analyze_classifier_robustness(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during robustness analysis: {str(e)}\")\n",
    "        robustness_metrics = None\n",
    "\n",
    "    # Summarize the results\n",
    "    print(\"\\n=== Generalization Evaluation Summary ===\")\n",
    "\n",
    "    # Train/test summary\n",
    "    if generalization_metrics:\n",
    "        train_f1 = (\n",
    "            generalization_metrics.get(\"training\", {}).get(\"overall\", {}).get(\"f1\", 0)\n",
    "        )\n",
    "        test_f1 = (\n",
    "            generalization_metrics.get(\"testing\", {}).get(\"overall\", {}).get(\"f1\", 0)\n",
    "        )\n",
    "        gap = generalization_metrics.get(\"generalization_gap\", {}).get(\"f1_gap\", 0)\n",
    "        assessment = generalization_metrics.get(\"generalization_gap\", {}).get(\n",
    "            \"gap_assessment\", \"N/A\"\n",
    "        )\n",
    "\n",
    "        print(f\"Train/Test Split Results:\")\n",
    "        print(f\"  - Training F1: {train_f1:.3f}\")\n",
    "        print(f\"  - Testing F1: {test_f1:.3f}\")\n",
    "        print(f\"  - Generalization Gap: {gap:.3f}\")\n",
    "        print(f\"  - Assessment: {assessment}\")\n",
    "    else:\n",
    "        print(\"Train/Test Split Results: Failed to generate metrics\")\n",
    "\n",
    "    # Cross-validation summary\n",
    "    if cv_metrics:\n",
    "        cv_train_f1 = (\n",
    "            cv_metrics.get(\"aggregated\", {})\n",
    "            .get(\"train\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"f1\", 0)\n",
    "        )\n",
    "        cv_test_f1 = (\n",
    "            cv_metrics.get(\"aggregated\", {})\n",
    "            .get(\"test\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"f1\", 0)\n",
    "        )\n",
    "        cv_gap = cv_metrics.get(\"generalization_gap\", {}).get(\"f1_gap\", 0)\n",
    "        cv_assessment = cv_metrics.get(\"generalization_gap\", {}).get(\n",
    "            \"gap_assessment\", \"N/A\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\nCross-Validation Results:\")\n",
    "        print(f\"  - Average Training F1: {cv_train_f1:.3f}\")\n",
    "        print(f\"  - Average Testing F1: {cv_test_f1:.3f}\")\n",
    "        print(f\"  - Generalization Gap: {cv_gap:.3f}\")\n",
    "        print(f\"  - Assessment: {cv_assessment}\")\n",
    "    else:\n",
    "        print(\"\\nCross-Validation Results: Failed to generate metrics\")\n",
    "\n",
    "    # Robustness summary\n",
    "    if robustness_metrics:\n",
    "        boundary_acc = (\n",
    "            robustness_metrics.get(\"boundary_cases\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"boundary_accuracy\", 0)\n",
    "        )\n",
    "        non_boundary_acc = (\n",
    "            robustness_metrics.get(\"boundary_cases\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"non_boundary_accuracy\", 0)\n",
    "        )\n",
    "        robustness_score = (\n",
    "            robustness_metrics.get(\"boundary_cases\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"robustness_score\", 0)\n",
    "        )\n",
    "\n",
    "        print(f\"\\nRobustness Analysis:\")\n",
    "        print(f\"  - Boundary Case Accuracy: {boundary_acc:.3f}\")\n",
    "        print(f\"  - Non-Boundary Accuracy: {non_boundary_acc:.3f}\")\n",
    "        print(f\"  - Robustness Score: {robustness_score:.3f}\")\n",
    "\n",
    "        # Top evidence sources\n",
    "        if \"top_sources\" in robustness_metrics.get(\"evidence_sources\", {}).get(\n",
    "            \"overall\", {}\n",
    "        ):\n",
    "            top_sources = robustness_metrics[\"evidence_sources\"][\"overall\"][\n",
    "                \"top_sources\"\n",
    "            ]\n",
    "            if top_sources:\n",
    "                print(\n",
    "                    f\"  - Top Evidence Source: {top_sources[0][0]} (Accuracy: {top_sources[0][1]:.3f}, Count: {top_sources[0][2]})\"\n",
    "                )\n",
    "\n",
    "        # Consistency\n",
    "        if \"overall_consistency\" in robustness_metrics.get(\n",
    "            \"classification_consistency\", {}\n",
    "        ):\n",
    "            consistency = robustness_metrics[\"classification_consistency\"][\n",
    "                \"overall_consistency\"\n",
    "            ]\n",
    "            print(f\"  - Classification Consistency: {consistency:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nRobustness Analysis: Failed to generate metrics\")\n",
    "\n",
    "    print(\"\\nGeneralization evaluation complete. Visualizations saved to:\")\n",
    "    print(\"  - /metrics_visualization/generalization_*.png\")\n",
    "\n",
    "    # Save full generalization metrics to JSON (with safe handling of numpy values)\n",
    "    try:\n",
    "        generalization_data = {\n",
    "            \"simple_split\": generalization_metrics if generalization_metrics else {},\n",
    "            \"cross_validation\": cv_metrics if cv_metrics else {},\n",
    "            \"robustness\": robustness_metrics if robustness_metrics else {},\n",
    "        }\n",
    "\n",
    "        # Convert numpy types to Python native types\n",
    "        def numpy_to_python(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: numpy_to_python(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [numpy_to_python(i) for i in obj]\n",
    "            return obj\n",
    "\n",
    "        with open(\"./generalization_metrics.json\", \"w\") as f:\n",
    "            json.dump(numpy_to_python(generalization_data), f, indent=2)\n",
    "        print(\"Full generalization metrics saved to generalization_metrics.json\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving generalization metrics: {str(e)}\")\n",
    "\n",
    "    return generalization_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b3a2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modify the main function to include generalization evaluation\n",
    "# def main_with_generalization():\n",
    "#     \"\"\"Extended main function that includes generalization evaluation\"\"\"\n",
    "#     # Run standard pipeline\n",
    "#     main()\n",
    "\n",
    "#     # Run generalization evaluation\n",
    "#     print(\"\\n======================================\")\n",
    "#     print(\"Running generalization evaluation...\")\n",
    "#     print(\"======================================\")\n",
    "#     evaluate_generalization_metrics()\n",
    "\n",
    "# Modify the main function to include generalization evaluation\n",
    "def main_with_generalization():\n",
    "    # Extended main function that includes generalization evaluation\n",
    "    try:\n",
    "        # First let's load and preprocess the test data\n",
    "        print(\"Loading test data...\")\n",
    "        try:\n",
    "            with open(\"./labeled_test_data_plusmomo.json\") as f:\n",
    "                test_data = json.load(f)\n",
    "\n",
    "            print(f\"Loaded {len(test_data)} publications from test data\")\n",
    "\n",
    "            # Load curated publications to use as ground truth\n",
    "            print(\"Loading curated publications for ground truth...\")\n",
    "            with open(\"./curated_publications.json\") as f:\n",
    "                curated_publications = json.load(f)\n",
    "            # with open('./labeled_test_data.json') as f:\n",
    "            #     curated_publications = json.load(f)\n",
    "\n",
    "            print(f\"Loaded {len(curated_publications)} curated publications.\")\n",
    "\n",
    "            # Create a mapping from DOIs to models for quick lookup\n",
    "            doi_to_model_map = {}\n",
    "            for pub in curated_publications:\n",
    "                if \"doi\" in pub and pub[\"doi\"] and \"model\" in pub and pub[\"model\"]:\n",
    "                    normalized_doi = normalize_doi(pub[\"doi\"])\n",
    "                    if normalized_doi:\n",
    "                        doi_to_model_map[normalized_doi] = pub[\"model\"]\n",
    "\n",
    "            print(f\"Created mapping with {len(doi_to_model_map)} DOI to model pairs.\")\n",
    "\n",
    "            # Add 'models' field to test data where possible using the DOI mapping\n",
    "            models_added = 0\n",
    "            for pub in test_data:\n",
    "                if \"DOI\" in pub and pub[\"DOI\"]:\n",
    "                    normalized_doi = normalize_doi(pub[\"DOI\"])\n",
    "                    if normalized_doi in doi_to_model_map:\n",
    "                        pub[\"models\"] = [doi_to_model_map[normalized_doi]]\n",
    "                        models_added += 1\n",
    "\n",
    "            print(\n",
    "                f\"Added ground truth 'models' to {models_added} publications using curated mapping.\"\n",
    "            )\n",
    "            print(f\"This will allow evaluation for these publications.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing test data: {str(e)}\")\n",
    "            print(\"Falling back to default processing...\")\n",
    "\n",
    "            # Run standard pipeline if there's an issue with the test data\n",
    "            main()\n",
    "            return\n",
    "\n",
    "        # Continue with processing using the enhanced test data\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(\"Starting optimized science publication classifier...\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(\n",
    "                f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\"\n",
    "            )\n",
    "\n",
    "        # Initialize classifier\n",
    "        print(\"Initializing science classifier...\")\n",
    "        with torch.inference_mode():\n",
    "            science_classifier = ScienceClassifier.get_instance()\n",
    "        print(\"Science classifier initialized successfully.\")\n",
    "\n",
    "        # Load configuration files\n",
    "        print(\"Loading configuration files...\")\n",
    "        curated_mapping = load_curated_models(\"./curated_publications.json\")\n",
    "        # curated_mapping = load_curated_models('./labeled_test_data.json')\n",
    "        model_keywords = load_model_keywords(\"./model_keywords.json\")\n",
    "        model_descriptions = load_model_descriptions(\"./model_descriptions.json\")\n",
    "\n",
    "        # Initialize ranker\n",
    "        print(\"Initializing relevance ranker...\")\n",
    "        with torch.inference_mode():\n",
    "            ranker = RelevanceRanker(model_descriptions)\n",
    "\n",
    "        # Initialize model embeddings\n",
    "        print(\"Initializing model embeddings...\")\n",
    "        with torch.inference_mode():\n",
    "            model_embeddings = initialize_model_embeddings(model_descriptions)\n",
    "\n",
    "        # Clean memory\n",
    "        optimize_memory()\n",
    "\n",
    "        # Initialize context manager\n",
    "        print(\"Building context validation profiles...\")\n",
    "        with open(\"./curated_publications.json\") as f:\n",
    "            full_curated = json.load(f)\n",
    "        # with open('./labeled_test_data.json') as f:\n",
    "        #     full_curated = json.load(f)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            context_manager = ModelContextManager(full_curated)\n",
    "\n",
    "        # Derive data-driven affinities from the curated dataset\n",
    "        print(\"Deriving data-driven affinities from curated dataset...\")\n",
    "        derive_data_driven_affinities(\"./curated_publications.json\")\n",
    "        # derive_data_driven_affinities('./labeled_test_data.json')\n",
    "\n",
    "        # Process publications\n",
    "        print(\"\\nProcessing publications...\")\n",
    "        results = process_publication_batch(\n",
    "            test_data,\n",
    "            curated_mapping,\n",
    "            model_keywords,\n",
    "            model_embeddings,\n",
    "            science_classifier,\n",
    "            context_manager,\n",
    "            ranker=ranker,\n",
    "        )\n",
    "\n",
    "        # Save results\n",
    "        print(\"Saving results...\")\n",
    "        with open(\"./labeled_test_data_plusmomo_results.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        # Generate visualizations with threshold analysis\n",
    "        print(\"\\nGenerating visualizations with threshold analysis...\")\n",
    "        metrics = visualize_metrics(results)\n",
    "\n",
    "        # Show TF-IDF model-specific terms\n",
    "        print(\"\\nModel-specific terminology based on TF-IDF analysis:\")\n",
    "        model_specific_terms = context_manager.get_model_specific_terms()\n",
    "        for model, terms in model_specific_terms.items():\n",
    "            print(f\"\\n{model} distinctive terms:\")\n",
    "            print(\", \".join(terms[:10]))  # Show top 10 terms\n",
    "\n",
    "        # Find optimal thresholds\n",
    "        print(\"\\nFinding optimal thresholds...\")\n",
    "        try:\n",
    "            optimal_thresholds = find_optimal_thresholds(results)\n",
    "\n",
    "            # Add error check for per_model key\n",
    "            if \"per_model\" not in optimal_thresholds:\n",
    "                print(\n",
    "                    \"Warning: 'per_model' key missing in optimal_thresholds, using empty dict\"\n",
    "                )\n",
    "                optimal_thresholds[\"per_model\"] = {}\n",
    "\n",
    "            print(\"\\nOptimal model-specific thresholds:\")\n",
    "            for model, data in optimal_thresholds[\"per_model\"].items():\n",
    "                current = MODEL_THRESHOLDS.get(model, 0.4)\n",
    "                # Use threshold_f1 instead of threshold\n",
    "                print(\n",
    "                    f\"{model}: {data['threshold_f1']:.2f} (current: {current:.2f}, F1: {data['f1']:.2f})\"\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                f\"\\nOptimal overall threshold: {optimal_thresholds['overall']['threshold_f1']:.2f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"Overall F1 score with optimal thresholds: {optimal_thresholds['overall']['f1']:.3f}\"\n",
    "            )\n",
    "\n",
    "            # Compare performance with current vs. optimal thresholds\n",
    "            print(\"\\nComparing performance with current vs. optimal thresholds:\")\n",
    "            current_performance = analyze_threshold_performance(results)\n",
    "            optimal_performance = analyze_threshold_performance(\n",
    "                results,\n",
    "                model_thresholds=optimal_thresholds.get(\"model_thresholds_f1\", {}),\n",
    "                overall_threshold=optimal_thresholds.get(\"overall\", {}).get(\n",
    "                    \"threshold_f1\", 0.4\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            current_f1 = current_performance.get(\"overall\", {}).get(\"f1\", 0)\n",
    "            optimal_f1 = optimal_performance.get(\"overall\", {}).get(\"f1\", 0)\n",
    "            improvement = (optimal_f1 - current_f1) * 100\n",
    "\n",
    "            print(\n",
    "                f\"Current F1: {current_f1:.3f}, \"\n",
    "                + f\"Optimal F1: {optimal_f1:.3f}, \"\n",
    "                + f\"Improvement: {improvement:.1f}%\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during threshold optimization: {str(e)}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Run generalization evaluation\n",
    "        print(\"\\n======================================\")\n",
    "        print(\"Running generalization evaluation...\")\n",
    "        print(\"======================================\")\n",
    "        try:\n",
    "            # Use results from this run directly instead of loading from file\n",
    "            generalization_metrics = evaluate_generalization_metrics_inline(results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generalization evaluation: {str(e)}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Report completion\n",
    "        total_time = time.time() - start_time\n",
    "        print(\n",
    "            f\"\\nProcessing completed in {total_time:.2f}s ({total_time/60:.2f} minutes)\"\n",
    "        )\n",
    "        print(f\"Average time per publication: {total_time/len(test_data):.4f}s\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(\n",
    "                f\"Peak GPU memory usage: {torch.cuda.max_memory_allocated(0) / 1024**2:.2f} MB\"\n",
    "            )\n",
    "\n",
    "        # Report additional information\n",
    "        print(\n",
    "            f\"\\nNote: Ground truth 'models' were added to {models_added}/{len(test_data)} publications.\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Evaluation metrics are based on these {models_added} publications with ground truth.\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in main_with_generalization: {str(e)}\")\n",
    "        import traceback\n",
    "\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "# Special version of evaluate_generalization_metrics that works directly with results in memory\n",
    "def evaluate_generalization_metrics_inline(results):\n",
    "    print(\"\\nEvaluating model generalization on in-memory results...\")\n",
    "    print(f\"Using {len(results)} publications for evaluation\")\n",
    "\n",
    "    # Run simple train/test split evaluation\n",
    "    print(\"\\n1. Running train/test split evaluation...\")\n",
    "    try:\n",
    "        generalization_metrics = evaluate_generalization(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during train/test evaluation: {str(e)}\")\n",
    "        generalization_metrics = None\n",
    "\n",
    "    # Run cross-validation evaluation\n",
    "    print(\"\\n2. Running cross-validation evaluation...\")\n",
    "    try:\n",
    "        cv_metrics = evaluate_with_cross_validation(results, n_folds=5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cross-validation: {str(e)}\")\n",
    "        cv_metrics = None\n",
    "\n",
    "    # Run robustness analysis\n",
    "    print(\"\\n3. Running robustness analysis...\")\n",
    "    try:\n",
    "        robustness_metrics = analyze_classifier_robustness(results)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during robustness analysis: {str(e)}\")\n",
    "        robustness_metrics = None\n",
    "\n",
    "    # Summarize the results\n",
    "    print(\"\\n=== Generalization Evaluation Summary ===\")\n",
    "\n",
    "    # Train/test summary\n",
    "    if generalization_metrics:\n",
    "        train_f1 = (\n",
    "            generalization_metrics.get(\"training\", {}).get(\"overall\", {}).get(\"f1\", 0)\n",
    "        )\n",
    "        test_f1 = (\n",
    "            generalization_metrics.get(\"testing\", {}).get(\"overall\", {}).get(\"f1\", 0)\n",
    "        )\n",
    "        gap = generalization_metrics.get(\"generalization_gap\", {}).get(\"f1_gap\", 0)\n",
    "        assessment = generalization_metrics.get(\"generalization_gap\", {}).get(\n",
    "            \"gap_assessment\", \"N/A\"\n",
    "        )\n",
    "\n",
    "        print(f\"Train/Test Split Results:\")\n",
    "        print(f\"  - Training F1: {train_f1:.3f}\")\n",
    "        print(f\"  - Testing F1: {test_f1:.3f}\")\n",
    "        print(f\"  - Generalization Gap: {gap:.3f}\")\n",
    "        print(f\"  - Assessment: {assessment}\")\n",
    "    else:\n",
    "        print(\"Train/Test Split Results: Failed to generate metrics\")\n",
    "\n",
    "    # Cross-validation summary\n",
    "    if cv_metrics:\n",
    "        cv_train_f1 = (\n",
    "            cv_metrics.get(\"aggregated\", {})\n",
    "            .get(\"train\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"f1\", 0)\n",
    "        )\n",
    "        cv_test_f1 = (\n",
    "            cv_metrics.get(\"aggregated\", {})\n",
    "            .get(\"test\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"f1\", 0)\n",
    "        )\n",
    "        cv_gap = cv_metrics.get(\"generalization_gap\", {}).get(\"f1_gap\", 0)\n",
    "        cv_assessment = cv_metrics.get(\"generalization_gap\", {}).get(\n",
    "            \"gap_assessment\", \"N/A\"\n",
    "        )\n",
    "\n",
    "        print(f\"\\nCross-Validation Results:\")\n",
    "        print(f\"  - Average Training F1: {cv_train_f1:.3f}\")\n",
    "        print(f\"  - Average Testing F1: {cv_test_f1:.3f}\")\n",
    "        print(f\"  - Generalization Gap: {cv_gap:.3f}\")\n",
    "        print(f\"  - Assessment: {cv_assessment}\")\n",
    "    else:\n",
    "        print(\"\\nCross-Validation Results: Failed to generate metrics\")\n",
    "\n",
    "    # Robustness summary\n",
    "    if robustness_metrics:\n",
    "        boundary_acc = (\n",
    "            robustness_metrics.get(\"boundary_cases\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"boundary_accuracy\", 0)\n",
    "        )\n",
    "        non_boundary_acc = (\n",
    "            robustness_metrics.get(\"boundary_cases\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"non_boundary_accuracy\", 0)\n",
    "        )\n",
    "        robustness_score = (\n",
    "            robustness_metrics.get(\"boundary_cases\", {})\n",
    "            .get(\"overall\", {})\n",
    "            .get(\"robustness_score\", 0)\n",
    "        )\n",
    "\n",
    "        print(f\"\\nRobustness Analysis:\")\n",
    "        print(f\"  - Boundary Case Accuracy: {boundary_acc:.3f}\")\n",
    "        print(f\"  - Non-Boundary Accuracy: {non_boundary_acc:.3f}\")\n",
    "        print(f\"  - Robustness Score: {robustness_score:.3f}\")\n",
    "\n",
    "        # Top evidence sources\n",
    "        if \"top_sources\" in robustness_metrics.get(\"evidence_sources\", {}).get(\n",
    "            \"overall\", {}\n",
    "        ):\n",
    "            top_sources = robustness_metrics[\"evidence_sources\"][\"overall\"][\n",
    "                \"top_sources\"\n",
    "            ]\n",
    "            if top_sources:\n",
    "                print(\n",
    "                    f\"  - Top Evidence Source: {top_sources[0][0]} (Accuracy: {top_sources[0][1]:.3f}, Count: {top_sources[0][2]})\"\n",
    "                )\n",
    "\n",
    "        # Consistency\n",
    "        if \"overall_consistency\" in robustness_metrics.get(\n",
    "            \"classification_consistency\", {}\n",
    "        ):\n",
    "            consistency = robustness_metrics[\"classification_consistency\"][\n",
    "                \"overall_consistency\"\n",
    "            ]\n",
    "            print(f\"  - Classification Consistency: {consistency:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nRobustness Analysis: Failed to generate metrics\")\n",
    "\n",
    "    print(\"\\nGeneralization evaluation complete. Visualizations saved to:\")\n",
    "    print(\"  - metrics_visualization_generalization_*.png\")\n",
    "\n",
    "    # Save full generalization metrics to JSON (with safe handling of numpy values)\n",
    "    try:\n",
    "        generalization_data = {\n",
    "            \"simple_split\": generalization_metrics if generalization_metrics else {},\n",
    "            \"cross_validation\": cv_metrics if cv_metrics else {},\n",
    "            \"robustness\": robustness_metrics if robustness_metrics else {},\n",
    "        }\n",
    "\n",
    "        # Convert numpy types to Python native types\n",
    "        def numpy_to_python(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: numpy_to_python(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [numpy_to_python(i) for i in obj]\n",
    "            return obj\n",
    "\n",
    "        with open(\"./generalization_metrics_labeled_test_data_plusmomo.json\", \"w\") as f:\n",
    "            json.dump(numpy_to_python(generalization_data), f, indent=2)\n",
    "        print(\"Full generalization metrics saved to file\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving generalization metrics: {str(e)}\")\n",
    "\n",
    "    return generalization_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a3aa25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Loaded 3398 publications from test data\n",
      "Loading curated publications for ground truth...\n",
      "Loaded 2344 curated publications.\n",
      "Created mapping with 2260 DOI to model pairs.\n",
      "Added ground truth 'models' to 1548 publications using curated mapping.\n",
      "This will allow evaluation for these publications.\n",
      "Starting optimized science publication classifier...\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4070 Ti SUPER\n",
      "CUDA memory allocated: 486.60 MB\n",
      "Initializing science classifier...\n",
      "Loading research_area model from arminmehrabian/nasa-impact-nasa-smd-ibm-st-v2-classification-finetuned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded research_area model\n",
      "Loading science_keywords model from nasa-impact/science-keyword-classification...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded science_keywords model\n",
      "Loading division model from nasa-impact/division-classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded division model\n",
      "Science classifier initialized successfully.\n",
      "Loading configuration files...\n",
      "Initializing relevance ranker...\n",
      "Initializing model embeddings...\n",
      "Building context validation profiles...\n",
      "Deriving data-driven affinities from curated dataset...\n",
      "Deriving data-driven affinities from curated dataset...\n",
      "Saved data-driven keyword affinities with 186 entries\n",
      "Derived affinities for 186 keywords, 15 research areas, and 4 divisions\n",
      "\n",
      "Processing publications...\n",
      "Processing batch 1/34 (publications 1-100/3398)...\n",
      "  Processing publication 20/3398...\n",
      "  Processing publication 40/3398...\n",
      "  Processing publication 60/3398...\n",
      "  Processing publication 80/3398...\n",
      "  Processing publication 100/3398...\n",
      "Completed batch 1/34\n",
      "Processing batch 2/34 (publications 101-200/3398)...\n",
      "  Processing publication 120/3398...\n",
      "  Processing publication 140/3398...\n",
      "  Processing publication 160/3398...\n",
      "  Processing publication 180/3398...\n",
      "  Processing publication 200/3398...\n",
      "Completed batch 2/34\n",
      "Processing batch 3/34 (publications 201-300/3398)...\n",
      "  Processing publication 220/3398...\n",
      "  Processing publication 240/3398...\n",
      "  Processing publication 260/3398...\n",
      "  Processing publication 280/3398...\n",
      "  Processing publication 300/3398...\n",
      "Completed batch 3/34\n",
      "Processing batch 4/34 (publications 301-400/3398)...\n",
      "  Processing publication 320/3398...\n",
      "  Processing publication 340/3398...\n",
      "  Processing publication 360/3398...\n",
      "  Processing publication 380/3398...\n",
      "  Processing publication 400/3398...\n",
      "Completed batch 4/34\n",
      "Processing batch 5/34 (publications 401-500/3398)...\n",
      "  Processing publication 420/3398...\n",
      "  Processing publication 440/3398...\n",
      "  Processing publication 460/3398...\n",
      "  Processing publication 480/3398...\n",
      "  Processing publication 500/3398...\n",
      "Completed batch 5/34\n",
      "Processing batch 6/34 (publications 501-600/3398)...\n",
      "  Processing publication 520/3398...\n",
      "  Processing publication 540/3398...\n",
      "  Processing publication 560/3398...\n",
      "  Processing publication 580/3398...\n",
      "  Processing publication 600/3398...\n",
      "Completed batch 6/34\n",
      "Processing batch 7/34 (publications 601-700/3398)...\n",
      "  Processing publication 620/3398...\n",
      "  Processing publication 640/3398...\n",
      "  Processing publication 660/3398...\n",
      "  Processing publication 680/3398...\n",
      "  Processing publication 700/3398...\n",
      "Completed batch 7/34\n",
      "Processing batch 8/34 (publications 701-800/3398)...\n",
      "  Processing publication 720/3398...\n",
      "  Processing publication 740/3398...\n",
      "  Processing publication 760/3398...\n",
      "  Processing publication 780/3398...\n",
      "  Processing publication 800/3398...\n",
      "Completed batch 8/34\n",
      "Processing batch 9/34 (publications 801-900/3398)...\n",
      "  Processing publication 820/3398...\n",
      "  Processing publication 840/3398...\n",
      "  Processing publication 860/3398...\n",
      "  Processing publication 880/3398...\n",
      "  Processing publication 900/3398...\n",
      "Completed batch 9/34\n",
      "Processing batch 10/34 (publications 901-1000/3398)...\n",
      "  Processing publication 920/3398...\n",
      "  Processing publication 940/3398...\n",
      "  Processing publication 960/3398...\n",
      "  Processing publication 980/3398...\n",
      "  Processing publication 1000/3398...\n",
      "Completed batch 10/34\n",
      "Processing batch 11/34 (publications 1001-1100/3398)...\n",
      "  Processing publication 1020/3398...\n",
      "  Processing publication 1040/3398...\n",
      "  Processing publication 1060/3398...\n",
      "  Processing publication 1080/3398...\n",
      "  Processing publication 1100/3398...\n",
      "Completed batch 11/34\n",
      "Processing batch 12/34 (publications 1101-1200/3398)...\n",
      "  Processing publication 1120/3398...\n",
      "  Processing publication 1140/3398...\n",
      "  Processing publication 1160/3398...\n",
      "  Processing publication 1180/3398...\n",
      "  Processing publication 1200/3398...\n",
      "Completed batch 12/34\n",
      "Processing batch 13/34 (publications 1201-1300/3398)...\n",
      "  Processing publication 1220/3398...\n",
      "  Processing publication 1240/3398...\n",
      "  Processing publication 1260/3398...\n",
      "  Processing publication 1280/3398...\n",
      "  Processing publication 1300/3398...\n",
      "Completed batch 13/34\n",
      "Processing batch 14/34 (publications 1301-1400/3398)...\n",
      "  Processing publication 1320/3398...\n",
      "  Processing publication 1340/3398...\n",
      "  Processing publication 1360/3398...\n",
      "  Processing publication 1380/3398...\n",
      "  Processing publication 1400/3398...\n",
      "Completed batch 14/34\n",
      "Processing batch 15/34 (publications 1401-1500/3398)...\n",
      "  Processing publication 1420/3398...\n",
      "  Processing publication 1440/3398...\n",
      "  Processing publication 1460/3398...\n",
      "  Processing publication 1480/3398...\n",
      "  Processing publication 1500/3398...\n",
      "Completed batch 15/34\n",
      "Processing batch 16/34 (publications 1501-1600/3398)...\n",
      "  Processing publication 1520/3398...\n",
      "  Processing publication 1540/3398...\n",
      "  Processing publication 1560/3398...\n",
      "  Processing publication 1580/3398...\n",
      "  Processing publication 1600/3398...\n",
      "Completed batch 16/34\n",
      "Processing batch 17/34 (publications 1601-1700/3398)...\n",
      "  Processing publication 1620/3398...\n",
      "  Processing publication 1640/3398...\n",
      "  Processing publication 1660/3398...\n",
      "  Processing publication 1680/3398...\n",
      "  Processing publication 1700/3398...\n",
      "Completed batch 17/34\n",
      "Processing batch 18/34 (publications 1701-1800/3398)...\n",
      "  Processing publication 1720/3398...\n",
      "  Processing publication 1740/3398...\n",
      "  Processing publication 1760/3398...\n",
      "  Processing publication 1780/3398...\n",
      "  Processing publication 1800/3398...\n",
      "Completed batch 18/34\n",
      "Processing batch 19/34 (publications 1801-1900/3398)...\n",
      "  Processing publication 1820/3398...\n",
      "  Processing publication 1840/3398...\n",
      "  Processing publication 1860/3398...\n",
      "  Processing publication 1880/3398...\n",
      "  Processing publication 1900/3398...\n",
      "Completed batch 19/34\n",
      "Processing batch 20/34 (publications 1901-2000/3398)...\n",
      "  Processing publication 1920/3398...\n",
      "  Processing publication 1940/3398...\n",
      "  Processing publication 1960/3398...\n",
      "  Processing publication 1980/3398...\n",
      "  Processing publication 2000/3398...\n",
      "Completed batch 20/34\n",
      "Processing batch 21/34 (publications 2001-2100/3398)...\n",
      "  Processing publication 2020/3398...\n",
      "  Processing publication 2040/3398...\n",
      "  Processing publication 2060/3398...\n",
      "  Processing publication 2080/3398...\n",
      "  Processing publication 2100/3398...\n",
      "Completed batch 21/34\n",
      "Processing batch 22/34 (publications 2101-2200/3398)...\n",
      "  Processing publication 2120/3398...\n",
      "  Processing publication 2140/3398...\n",
      "  Processing publication 2160/3398...\n",
      "  Processing publication 2180/3398...\n",
      "  Processing publication 2200/3398...\n",
      "Completed batch 22/34\n",
      "Processing batch 23/34 (publications 2201-2300/3398)...\n",
      "  Processing publication 2220/3398...\n",
      "  Processing publication 2240/3398...\n",
      "  Processing publication 2260/3398...\n",
      "  Processing publication 2280/3398...\n",
      "  Processing publication 2300/3398...\n",
      "Completed batch 23/34\n",
      "Processing batch 24/34 (publications 2301-2400/3398)...\n",
      "  Processing publication 2320/3398...\n"
     ]
    }
   ],
   "source": [
    "# Run the extended main function when directly executed\n",
    "if __name__ == \"__main__\":\n",
    "    main_with_generalization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
