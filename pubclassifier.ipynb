{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55c684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import gc\n",
    "from typing import List, Dict, Set, Tuple, Any, Optional, Union\n",
    "from collections import defaultdict, Counter\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from rapidfuzz import fuzz\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import lru_cache\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b31995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPU/CPU\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Initialize SpaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Downloading SpaCy model...\")\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize SentenceTransformer model\n",
    "st_model = SentenceTransformer('nasa-impact/nasa-ibm-st.38m')\n",
    "st_model.eval()\n",
    "st_model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Memory management\n",
    "def optimize_memory():\n",
    "    # Free up memory resources \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88900140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScienceClassifier:\n",
    "    # Science document classifier with model loading and caching\n",
    "    _instance = None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_instance(cls):\n",
    "        # Get or create singleton instance\n",
    "        if cls._instance is None:\n",
    "            cls._instance = cls()\n",
    "        return cls._instance\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Model configurations\n",
    "        self.models = {\n",
    "            'research_area': {\n",
    "                'name': 'arminmehrabian/nasa-impact-nasa-smd-ibm-st-v2-classification-finetuned',\n",
    "                'label_map': {\n",
    "                    0: \"Agriculture\",\n",
    "                    1: \"Air Quality\",\n",
    "                    2: \"Atmospheric/Ocean Indicators\",\n",
    "                    3: \"Cryospheric Indicators\",\n",
    "                    4: \"Droughts\",\n",
    "                    5: \"Earthquakes\",\n",
    "                    6: \"Ecosystems\",\n",
    "                    7: \"Energy Production/Use\",\n",
    "                    8: \"Environmental Impacts\",\n",
    "                    9: \"Floods\",\n",
    "                    10: \"Greenhouse Gases\",\n",
    "                    11: \"Habitat Conversion/Fragmentation\",\n",
    "                    12: \"Heat\",\n",
    "                    13: \"Land Surface/Agriculture Indicators\",\n",
    "                    14: \"Public Health\",\n",
    "                    15: \"Severe Storms\",\n",
    "                    16: \"Sun-Earth Interactions\",\n",
    "                    17: \"Validation\",\n",
    "                    18: \"Volcanic Eruptions\",\n",
    "                    19: \"Water Quality\",\n",
    "                    20: \"Wildfires\"\n",
    "                }\n",
    "            },\n",
    "            'science_keywords': {\n",
    "                'name': 'nasa-impact/science-keyword-classification'\n",
    "            },\n",
    "            'division': {\n",
    "                'name': 'nasa-impact/division-classifier',\n",
    "                'label_map': {\n",
    "                    0: 'Astrophysics',\n",
    "                    1: 'Biological and Physical Sciences',\n",
    "                    2: 'Earth Science',\n",
    "                    3: 'Heliophysics',\n",
    "                    4: 'Planetary Science'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        self.classification_cache = {}\n",
    "        self._load_models()\n",
    "    \n",
    "    def _load_models(self):\n",
    "        # Load classification models\n",
    "        self.classifiers = {}\n",
    "        for task, config in self.models.items():\n",
    "            try:\n",
    "                print(f\"Loading {task} model from {config['name']}...\")\n",
    "                with torch.no_grad():\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(config['name'])\n",
    "                    model = AutoModelForSequenceClassification.from_pretrained(config['name'])\n",
    "                    model.eval()\n",
    "                \n",
    "                self.classifiers[task] = {\n",
    "                    'pipe': pipeline(\n",
    "                        'text-classification',\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        device=device,\n",
    "                        batch_size=32\n",
    "                    ),\n",
    "                    'config': config\n",
    "                }\n",
    "                print(f\"Successfully loaded {task} model\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {task} model: {str(e)}\")\n",
    "    \n",
    "    def _prepare_text(self, publication: Dict) -> str:\n",
    "        # Extract and combine text from publication for classification\n",
    "        # Extract title\n",
    "        title = \"\"\n",
    "        if 'title' in publication:\n",
    "            if isinstance(publication['title'], list) and publication['title']:\n",
    "                title = publication['title'][0]\n",
    "            elif isinstance(publication['title'], str):\n",
    "                title = publication['title']\n",
    "                \n",
    "        abstract = publication.get('abstract', '')\n",
    "        keywords = ' '.join(publication.get('keywords', []))\n",
    "        return ' '.join([title, abstract, keywords])\n",
    "    \n",
    "    def _get_cache_key(self, publication: Dict) -> str:\n",
    "        # Generate caches for publication\n",
    "        if 'DOI' in publication and publication['DOI']:\n",
    "            return f\"doi:{publication['DOI']}\"\n",
    "        \n",
    "        title = \"\"\n",
    "        if 'title' in publication:\n",
    "            if isinstance(publication['title'], list) and publication['title']:\n",
    "                title = publication['title'][0]\n",
    "            elif isinstance(publication['title'], str):\n",
    "                title = publication.get('title', '')\n",
    "        \n",
    "        return f\"title:{title}\"\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def classify(self, publication: Dict) -> Dict:\n",
    "        # Run classification on publications with caching\n",
    "        # Check cache\n",
    "        cache_key = self._get_cache_key(publication)\n",
    "        if cache_key in self.classification_cache:\n",
    "            return self.classification_cache[cache_key]\n",
    "        \n",
    "        text = self._prepare_text(publication)\n",
    "        results = {\n",
    "            'research_areas': [],\n",
    "            'science_keywords': [],\n",
    "            'division': None\n",
    "        }\n",
    "        \n",
    "        # Research Area Classification\n",
    "        if self.classifiers.get('research_area'):\n",
    "            try:\n",
    "                res_area = self.classifiers['research_area']['pipe'](\n",
    "                    text, top_k=3, truncation=True, max_length=512\n",
    "                )\n",
    "                results['research_areas'] = [{\n",
    "                    'label': self.models['research_area']['label_map'][int(pred['label'].replace('LABEL_', ''))],\n",
    "                    'score': float(pred['score'])\n",
    "                } for pred in res_area]\n",
    "            except Exception as e:\n",
    "                print(f\"Research area classification failed: {str(e)}\")\n",
    "        \n",
    "        # Science Keywords Classification\n",
    "        if self.classifiers.get('science_keywords'):\n",
    "            try:\n",
    "                science_keywords = self.classifiers['science_keywords']['pipe'](\n",
    "                    text, truncation=True, max_length=512, top_k=10\n",
    "                )\n",
    "                \n",
    "                for pred in science_keywords:\n",
    "                    if pred['score'] > 0.35 and len(pred['label']) >= 4:\n",
    "                        results['science_keywords'].append({\n",
    "                            'label': pred['label'],\n",
    "                            'score': float(pred['score'])\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Science keyword classification failed: {str(e)}\")\n",
    "        \n",
    "        # Division Classification\n",
    "        if self.classifiers.get('division'):\n",
    "            try:\n",
    "                division_result = self.classifiers['division']['pipe'](\n",
    "                    text, top_k=1, truncation=True, max_length=512\n",
    "                )\n",
    "                \n",
    "                if division_result:\n",
    "                    division = division_result[0]\n",
    "                    if 'score' in division:\n",
    "                        results['division'] = {\n",
    "                            'label': division['label'],\n",
    "                            'score': float(division['score'])\n",
    "                        }\n",
    "            except Exception as e:\n",
    "                print(f\"Division classification failed: {str(e)}\")\n",
    "        \n",
    "        # Cache results\n",
    "        self.classification_cache[cache_key] = results\n",
    "        return results\n",
    "\n",
    "\n",
    "class ModelContextManager:\n",
    "    # Context validation with model profiles\n",
    "    \n",
    "    def __init__(self, curated_publications: List[Dict]):\n",
    "        self.st_model = st_model\n",
    "        self.model_profiles = self._build_model_profiles(curated_publications)\n",
    "        self.profile_cache = {}\n",
    "        self.corpus_term_frequencies = self._build_corpus_term_frequencies(curated_publications)\n",
    "        self.model_tfidf_terms = self._calculate_model_tfidf_terms(curated_publications)\n",
    "        \n",
    "    def _build_model_profiles(self, publications: List[Dict]) -> Dict[str, Dict]:\n",
    "        # Build model context profiles from curated publications\n",
    "        model_texts = defaultdict(list)\n",
    "        model_terms = defaultdict(set)\n",
    "        \n",
    "        # Collect texts and terms\n",
    "        for pub in publications:\n",
    "            model = pub.get('model')\n",
    "            if model:\n",
    "                prepared_text = ScienceClassifier.get_instance()._prepare_text(pub)\n",
    "                model_texts[model].append(prepared_text)\n",
    "                model_terms[model].update(self._extract_key_terms(prepared_text))\n",
    "        \n",
    "        # Create embeddings\n",
    "        final_profiles = {}\n",
    "        for model, texts in model_texts.items():\n",
    "            aggregated_text = ' '.join(texts[:min(100, len(texts))])\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                embedding = self.st_model.encode(aggregated_text, convert_to_tensor=True)\n",
    "                \n",
    "            final_profiles[model] = {\n",
    "                'embedding': embedding.cpu().numpy(),\n",
    "                'terms': set(sorted(model_terms[model], key=lambda x: -len(x))[:25]),\n",
    "                'text_count': len(texts)\n",
    "            }\n",
    "            \n",
    "        optimize_memory()\n",
    "        return final_profiles\n",
    "    \n",
    "    def _build_corpus_term_frequencies(self, publications: List[Dict]) -> Dict[str, int]:\n",
    "        # Build term frequency dictionary for entire corpus\n",
    "        corpus_terms = Counter()\n",
    "        \n",
    "        for pub in publications:\n",
    "            prepared_text = ScienceClassifier.get_instance()._prepare_text(pub)\n",
    "            words = re.findall(r'\\b[a-z]{4,}\\b', prepared_text.lower())\n",
    "            corpus_terms.update(words)\n",
    "            \n",
    "        return corpus_terms\n",
    "    \n",
    "    def _calculate_model_tfidf_terms(self, publications: List[Dict]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        # Calculate TF-IDF terms for each model\n",
    "        model_term_counts = defaultdict(Counter)\n",
    "        model_doc_counts = defaultdict(int)\n",
    "        \n",
    "        # Count terms by model\n",
    "        for pub in publications:\n",
    "            model = pub.get('model')\n",
    "            if model:\n",
    "                model_doc_counts[model] += 1\n",
    "                prepared_text = ScienceClassifier.get_instance()._prepare_text(pub)\n",
    "                words = re.findall(r'\\b[a-z]{4,}\\b', prepared_text.lower())\n",
    "                model_term_counts[model].update(words)\n",
    "        \n",
    "        # Calculate total document count\n",
    "        total_docs = sum(model_doc_counts.values())\n",
    "        \n",
    "        # Calculate TF-IDF for each term in each model\n",
    "        model_tfidf_terms = {}\n",
    "        for model, term_counts in model_term_counts.items():\n",
    "            tfidf_scores = {}\n",
    "            model_doc_count = model_doc_counts[model]\n",
    "            \n",
    "            for term, count in term_counts.items():\n",
    "                # Term frequency in this model\n",
    "                tf = count / sum(term_counts.values())\n",
    "                \n",
    "                # Inverse document frequency (add 1 to avoid division by zero)\n",
    "                term_doc_count = sum(1 for m, tc in model_term_counts.items() if term in tc)\n",
    "                idf = np.log((total_docs + 1) / (term_doc_count + 1))\n",
    "                \n",
    "                # TF-IDF score\n",
    "                tfidf = tf * idf\n",
    "                \n",
    "                # Only keep terms with sufficient frequency and length\n",
    "                if count >= 3 and len(term) >= 4:\n",
    "                    tfidf_scores[term] = tfidf\n",
    "            \n",
    "            # Sort by TF-IDF score and take top terms\n",
    "            sorted_terms = sorted(tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            model_tfidf_terms[model] = sorted_terms[:30]  # Keep top 30 distinctive terms\n",
    "            \n",
    "        return model_tfidf_terms\n",
    "    \n",
    "    def _extract_key_terms(self, text: str) -> Set[str]:\n",
    "        # Extract key terms from text\n",
    "        words = re.findall(r'\\b[a-z]{4,}\\b', text.lower())\n",
    "        word_counts = Counter(words)\n",
    "        return {word for word, count in word_counts.items() if count >= 2}\n",
    "    \n",
    "    @lru_cache(maxsize=5000)\n",
    "    def _get_pub_profile(self, prepared_text: str) -> Dict:\n",
    "        # Get publication profile with caching\n",
    "        if prepared_text in self.profile_cache:\n",
    "            return self.profile_cache[prepared_text]\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            embedding = self.st_model.encode(prepared_text)\n",
    "            \n",
    "        profile = {\n",
    "            'embedding': embedding,\n",
    "            'terms': self._extract_key_terms(prepared_text)\n",
    "        }\n",
    "        \n",
    "        self.profile_cache[prepared_text] = profile\n",
    "        return profile\n",
    "    \n",
    "    def get_model_specific_terms(self) -> Dict[str, List[str]]:\n",
    "        # Return model-specific terminology based on TF-IDF analysis\n",
    "        model_terms = {}\n",
    "        for model, tfidf_terms in self.model_tfidf_terms.items():\n",
    "            model_terms[model] = [term for term, score in tfidf_terms]\n",
    "        return model_terms\n",
    "    \n",
    "    def get_context_scores(self, publication: Dict) -> Dict[str, float]:\n",
    "        # Get context validation scores\n",
    "        prepared_text = ScienceClassifier.get_instance()._prepare_text(publication)\n",
    "        pub_profile = self._get_pub_profile(prepared_text)\n",
    "        \n",
    "        scores = {}\n",
    "        for model, model_profile in self.model_profiles.items():\n",
    "            # Basic term overlap score\n",
    "            pub_terms = pub_profile['terms']\n",
    "            model_terms = model_profile['terms']\n",
    "            \n",
    "            if not pub_terms or not model_terms:\n",
    "                term_overlap = 0.0\n",
    "            else:\n",
    "                intersection = len(pub_terms.intersection(model_terms))\n",
    "                union = len(pub_terms.union(model_terms))\n",
    "                term_overlap = intersection / union if union > 0 else 0.0\n",
    "            \n",
    "            # TF-IDF term match score\n",
    "            model_tfidf_terms = self.model_tfidf_terms.get(model, [])\n",
    "            tfidf_term_set = {term for term, score in model_tfidf_terms}\n",
    "            tfidf_match_count = len(pub_terms.intersection(tfidf_term_set))\n",
    "            tfidf_match_score = min(1.0, tfidf_match_count / 5) if tfidf_term_set else 0.0\n",
    "            \n",
    "            # Semantic similarity\n",
    "            semantic_sim = cosine_similarity(\n",
    "                [pub_profile['embedding']], \n",
    "                [model_profile['embedding']]\n",
    "            )[0][0]\n",
    "            \n",
    "            # Combined score (with TF-IDF term matches)\n",
    "            scores[model] = 0.5 * semantic_sim + 0.3 * term_overlap + 0.2 * tfidf_match_score\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class RelevanceRanker:\n",
    "    # Rank publications by relevance to models\n",
    "    \n",
    "    def __init__(self, model_descriptions: Dict[str, str]):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_descriptions = model_descriptions\n",
    "        self.description_cache = {}\n",
    "        self.result_cache = {}\n",
    "        \n",
    "        # Load model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"nasa-impact/nasa-smd-ibm-ranker\")\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                \"nasa-impact/nasa-smd-ibm-ranker\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            if self.device.type != 'cuda':\n",
    "                self.model = self.model.float()\n",
    "        \n",
    "        self.model.eval()\n",
    "\n",
    "    def _safe_prepare_model_text(self, model_id: str) -> str:\n",
    "        # Prepare model text with caching\n",
    "        if model_id not in self.description_cache:\n",
    "            desc = self.model_descriptions.get(model_id, '')[:380]\n",
    "            self.description_cache[model_id] = re.sub(r'\\s+', ' ', desc)\n",
    "        return self.description_cache[model_id]\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def batch_rank(self, query: str, model_ids: List[str]) -> Dict[str, float]:\n",
    "        # Rank models by relevance to query\n",
    "        if not model_ids:\n",
    "            return {}\n",
    "            \n",
    "        # Check cache\n",
    "        cache_key = f\"{hash(query)}_{hash(tuple(sorted(model_ids)))}\"\n",
    "        if cache_key in self.result_cache:\n",
    "            return self.result_cache[cache_key]\n",
    "            \n",
    "        query = query[:400]  # Truncate query\n",
    "        batch_texts = [self._safe_prepare_model_text(mid) for mid in model_ids]\n",
    "        \n",
    "        try:\n",
    "            # Prepare inputs\n",
    "            inputs = self.tokenizer(\n",
    "                [query] * len(batch_texts),\n",
    "                batch_texts,\n",
    "                padding='longest' if self.device.type == 'cuda' else True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "\n",
    "            outputs = self.model(**inputs)\n",
    "            scores = F.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()\n",
    "            result = dict(zip(model_ids, scores.tolist()))\n",
    "            \n",
    "            # Cache results\n",
    "            self.result_cache[cache_key] = result\n",
    "            return result\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"Ranker fallback to CPU: {str(e)}\")\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.model = self.model.to(self.device).float()\n",
    "            return self.batch_rank(query, model_ids)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ranker failed: {str(e)}\")\n",
    "            return {mid: 0.0 for mid in model_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiled regex patterns for efficiency\n",
    "HTML_TAG_PATTERN = re.compile(r'<[^>]+>')\n",
    "HYPHEN_UNDERSCORE_PATTERN = re.compile(r'[-_]')\n",
    "SPECIAL_CHAR_PATTERN = re.compile(r'[^a-zA-Z0-9]')\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    # lean and normalize text for matching\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = HTML_TAG_PATTERN.sub(' ', text)\n",
    "    text = HYPHEN_UNDERSCORE_PATTERN.sub(' ', text)\n",
    "    return SPECIAL_CHAR_PATTERN.sub(' ', text).lower()\n",
    "\n",
    "# DOI normalization cache\n",
    "_doi_cache = {}\n",
    "def normalize_doi(doi: str) -> str:\n",
    "    # Normalize DOI strings\n",
    "    if not doi:\n",
    "        return \"\"\n",
    "    \n",
    "    if doi in _doi_cache:\n",
    "        return _doi_cache[doi]\n",
    "    \n",
    "    doi = doi.lower().replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    if \"doi.org/\" in doi:\n",
    "        doi = doi.replace(\"doi.org/\", \"\")\n",
    "    doi = doi.replace(\",\", \".\")\n",
    "    result = doi.strip(\"/ \\n\\r\\t\")\n",
    "    \n",
    "    _doi_cache[doi] = result\n",
    "    return result\n",
    "\n",
    "# Fuzzy matching cache\n",
    "_fuzzy_match_cache = {}\n",
    "def fuzzy_keyword_match(text: str, keyword: str, threshold: float = 90.0) -> Tuple[bool, float]:\n",
    "    # Fuzzy keyword matching\n",
    "    cache_key = f\"{hash(text)}_{keyword}_{threshold}\"\n",
    "    if cache_key in _fuzzy_match_cache:\n",
    "        return _fuzzy_match_cache[cache_key]\n",
    "    \n",
    "    # Check for exact match first\n",
    "    keyword_lower = keyword.lower()\n",
    "    if keyword_lower in text.lower():\n",
    "        result = (True, 100.0)\n",
    "        _fuzzy_match_cache[cache_key] = result\n",
    "        return result\n",
    "    \n",
    "    # Use direct fuzzy matching for shorter texts\n",
    "    if len(text) < 1000:\n",
    "        score = fuzz.token_sort_ratio(text.lower(), keyword_lower)\n",
    "        result = (score >= threshold, score)\n",
    "        _fuzzy_match_cache[cache_key] = result\n",
    "        return result\n",
    "    \n",
    "    # For longer texts, use spaCy\n",
    "    doc = nlp(text[:2000])\n",
    "    \n",
    "    # Process relevant parts\n",
    "    chunks = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
    "    entities = [ent.text.lower() for ent in doc.ents]\n",
    "    tokens = [token.text.lower() for token in doc \n",
    "              if token.is_alpha and not token.is_stop and len(token.text) > 3]\n",
    "    \n",
    "    all_spans = chunks[:50] + entities[:50] + tokens[:100]\n",
    "    \n",
    "    # Find best match\n",
    "    best_score = 0.0\n",
    "    for span in all_spans:\n",
    "        if len(span) < 3:\n",
    "            continue\n",
    "        score = fuzz.token_sort_ratio(span, keyword_lower)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "    \n",
    "    result = (best_score >= threshold, best_score)\n",
    "    _fuzzy_match_cache[cache_key] = result\n",
    "    return result\n",
    "\n",
    "# Publication date extraction function\n",
    "def extract_publication_date(publication: Dict) -> Optional[str]:\n",
    "    # Extract publication date in yyyy-mm format\n",
    "    try:\n",
    "        # Try to get from 'published' field first\n",
    "        if 'published' in publication and 'date-parts' in publication['published']:\n",
    "            date_parts = publication['published']['date-parts'][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "        \n",
    "        # Try published-online\n",
    "        if 'published-online' in publication and 'date-parts' in publication['published-online']:\n",
    "            date_parts = publication['published-online']['date-parts'][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "        \n",
    "        # Try published-print\n",
    "        if 'published-print' in publication and 'date-parts' in publication['published-print']:\n",
    "            date_parts = publication['published-print']['date-parts'][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "        \n",
    "        # Try indexed\n",
    "        if 'indexed' in publication and 'date-parts' in publication['indexed']:\n",
    "            date_parts = publication['indexed']['date-parts'][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "        \n",
    "        # Try created\n",
    "        if 'created' in publication and 'date-parts' in publication['created']:\n",
    "            date_parts = publication['created']['date-parts'][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "        \n",
    "        # Try to parse issue date if year is present\n",
    "        if 'issued' in publication and 'date-parts' in publication['issued']:\n",
    "            date_parts = publication['issued']['date-parts'][0]\n",
    "            if len(date_parts) >= 2:\n",
    "                return f\"{date_parts[0]:04d}-{date_parts[1]:02d}\"\n",
    "            elif len(date_parts) >= 1:\n",
    "                return f\"{date_parts[0]:04d}-01\"  # Default to January if only year\n",
    "        \n",
    "        # Try publication year\n",
    "        if 'year' in publication:\n",
    "            return f\"{publication['year']:04d}-01\"  # Default to January\n",
    "            \n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting publication date: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Publication characteristics cache\n",
    "_characteristics_cache = {}\n",
    "def extract_publication_characteristics(publication: Dict) -> Dict[str, Any]:\n",
    "    # Extract publication characteristics with caching\n",
    "    cache_key = str(hash(json.dumps(publication, sort_keys=True)[:1000]))\n",
    "    if cache_key in _characteristics_cache:\n",
    "        return _characteristics_cache[cache_key]\n",
    "    \n",
    "    characteristics = {}\n",
    "    \n",
    "    # Extract title\n",
    "    title = \"\"\n",
    "    if 'title' in publication:\n",
    "        if isinstance(publication['title'], list) and publication['title']:\n",
    "            title = publication['title'][0]\n",
    "        else:\n",
    "            title = publication.get('title', '')\n",
    "    \n",
    "    # Extract abstract\n",
    "    abstract = publication.get('abstract', '')\n",
    "    \n",
    "    # Basic metrics\n",
    "    characteristics['title_length'] = len(title.split())\n",
    "    characteristics['abstract_length'] = len(abstract.split())\n",
    "    characteristics['total_length'] = characteristics['title_length'] + characteristics['abstract_length']\n",
    "    \n",
    "    # Keywords metrics\n",
    "    keywords = publication.get('keywords', [])\n",
    "    characteristics['keyword_count'] = len(keywords)\n",
    "    \n",
    "    # Process with spaCy if abstract is present\n",
    "    if abstract and len(abstract) > 10 and len(abstract) < 10000:\n",
    "        doc = nlp(abstract[:2000])\n",
    "        \n",
    "        # Part-of-speech distributions\n",
    "        pos_counts = Counter([token.pos_ for token in doc])\n",
    "        doc_len = len(doc) or 1\n",
    "        \n",
    "        characteristics['noun_ratio'] = pos_counts.get('NOUN', 0) / doc_len\n",
    "        characteristics['verb_ratio'] = pos_counts.get('VERB', 0) / doc_len\n",
    "        characteristics['adj_ratio'] = pos_counts.get('ADJ', 0) / doc_len\n",
    "        \n",
    "        # Named entity analysis\n",
    "        entity_types = [ent.label_ for ent in doc.ents]\n",
    "        entity_counter = Counter(entity_types)\n",
    "        \n",
    "        characteristics['entity_count'] = len(doc.ents)\n",
    "        characteristics['org_count'] = entity_counter.get('ORG', 0)\n",
    "        characteristics['person_count'] = entity_counter.get('PERSON', 0)\n",
    "        characteristics['date_count'] = entity_counter.get('DATE', 0)\n",
    "        \n",
    "        # Readability metrics\n",
    "        sentences = list(doc.sents)\n",
    "        if sentences:\n",
    "            sent_lengths = [len(sent) for sent in sentences]\n",
    "            characteristics['avg_sentence_length'] = np.mean(sent_lengths)\n",
    "            characteristics['sentence_count'] = len(sentences)\n",
    "        else:\n",
    "            characteristics['avg_sentence_length'] = 0\n",
    "            characteristics['sentence_count'] = 0\n",
    "    \n",
    "    # Publication year\n",
    "    if 'year' in publication:\n",
    "        characteristics['year'] = publication.get('year')\n",
    "    \n",
    "    # Author metrics\n",
    "    if 'authors' in publication:\n",
    "        authors = publication.get('authors', [])\n",
    "        if isinstance(authors, list):\n",
    "            characteristics['author_count'] = len(authors)\n",
    "        else:\n",
    "            characteristics['author_count'] = 1 if authors else 0\n",
    "    \n",
    "    _characteristics_cache[cache_key] = characteristics\n",
    "    return characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_embeddings_cache = {}\n",
    "_curated_models_cache = {}\n",
    "_model_keywords_cache = {}\n",
    "_model_descriptions_cache = {}\n",
    "\n",
    "def initialize_model_embeddings(model_descriptions: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "    # Create embeddings for model descriptions with batch processing\n",
    "    if _model_embeddings_cache and len(_model_embeddings_cache) == len(model_descriptions):\n",
    "        return _model_embeddings_cache\n",
    "    \n",
    "    embeddings = {}\n",
    "    batch_size = 32\n",
    "    \n",
    "    model_list = list(model_descriptions.items())\n",
    "    \n",
    "    for i in range(0, len(model_list), batch_size):\n",
    "        batch = model_list[i:i+batch_size]\n",
    "        models, texts = zip(*batch)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            batch_embeddings = st_model.encode(list(texts), convert_to_tensor=False)\n",
    "        \n",
    "        for j, model in enumerate(models):\n",
    "            embeddings[model] = batch_embeddings[j]\n",
    "    \n",
    "    _model_embeddings_cache.update(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def load_curated_models(curated_path: str) -> Dict[str, str]:\n",
    "    # Load curated DOI-model mappings\n",
    "    if curated_path in _curated_models_cache:\n",
    "        return _curated_models_cache[curated_path]\n",
    "    \n",
    "    try:\n",
    "        with open(curated_path) as f:\n",
    "            curated = json.load(f)\n",
    "        \n",
    "        mapping = {}\n",
    "        for entry in curated:\n",
    "            if 'doi' in entry and 'model' in entry:\n",
    "                normalized_doi = normalize_doi(entry['doi'])\n",
    "                if normalized_doi:\n",
    "                    mapping[normalized_doi] = entry['model']\n",
    "        \n",
    "        _curated_models_cache[curated_path] = mapping\n",
    "        return mapping\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading curated models from {curated_path}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def load_model_keywords(keywords_path: str) -> Dict[str, List[str]]:\n",
    "    # Load model keywords\n",
    "    if keywords_path in _model_keywords_cache:\n",
    "        return _model_keywords_cache[keywords_path]\n",
    "    \n",
    "    try:\n",
    "        with open(keywords_path) as f:\n",
    "            keywords = json.load(f)\n",
    "        \n",
    "        _model_keywords_cache[keywords_path] = keywords\n",
    "        return keywords\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model keywords from {keywords_path}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def load_model_descriptions(descriptions_path: str) -> Dict[str, str]:\n",
    "    # Load model descriptions\n",
    "    if descriptions_path in _model_descriptions_cache:\n",
    "        return _model_descriptions_cache[descriptions_path]\n",
    "    \n",
    "    try:\n",
    "        with open(descriptions_path) as f:\n",
    "            descriptions = json.load(f)\n",
    "        \n",
    "        _model_descriptions_cache[descriptions_path] = descriptions\n",
    "        return descriptions\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model descriptions from {descriptions_path}: {str(e)}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model thresholds\n",
    "MODEL_THRESHOLDS = {\n",
    "    'ECCO': 0.60,\n",
    "    'RAPID': 0.65,\n",
    "    'ISSM': 0.70,\n",
    "    'CMS-Flux': 0.70,\n",
    "    'CARDAMOM': 0.55,\n",
    "    'MOMO-CHEM': 0.95\n",
    "}\n",
    "\n",
    "# Affinity caches\n",
    "_keyword_affinity_cache = None\n",
    "_research_area_affinity_cache = None\n",
    "_division_affinity_cache = None\n",
    "\n",
    "def get_science_keyword_model_affinities():\n",
    "    # Get science keyword affinities\n",
    "    global _keyword_affinity_cache\n",
    "    \n",
    "    if _keyword_affinity_cache is not None:\n",
    "        return _keyword_affinity_cache\n",
    "    \n",
    "    try:\n",
    "        with open('./data_driven_affinities.json', 'r') as f:\n",
    "            _keyword_affinity_cache = json.load(f)\n",
    "            return _keyword_affinity_cache\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        print(\"Error loading affinity data. Using default empty affinities.\")\n",
    "        _keyword_affinity_cache = {}\n",
    "        return _keyword_affinity_cache\n",
    "\n",
    "def get_research_area_model_affinities():\n",
    "    # Get research area affinities \n",
    "    global _research_area_affinity_cache\n",
    "    \n",
    "    if _research_area_affinity_cache is not None:\n",
    "        return _research_area_affinity_cache\n",
    "    \n",
    "    _research_area_affinity_cache = {\n",
    "        \"Atmospheric/Ocean Indicators\": {\n",
    "            'ECCO': 1.7,\n",
    "            'MOMO-CHEM': 1.4,\n",
    "            'CMS-Flux': 1.1,\n",
    "            'ISSM': 1.1\n",
    "        },\n",
    "        \"Greenhouse Gases\": {\n",
    "            'CARDAMOM': 1.6,\n",
    "            'CMS-Flux': 1.8,\n",
    "            'MOMO-CHEM': 1.55,\n",
    "            'ECCO': 1.15\n",
    "        },\n",
    "        \"Ecosystems\": {\n",
    "            'CARDAMOM': 1.6,\n",
    "            'CMS-Flux': 1.2,\n",
    "            'ECCO': 1.25\n",
    "        },\n",
    "        \"Land Surface/Agriculture Indicators\": {\n",
    "            'CARDAMOM': 1.4,\n",
    "            'CMS-Flux': 1.3,\n",
    "            'ECCO': 1.1,\n",
    "            'ISSM': 1.4,\n",
    "            'RAPID': 1.4\n",
    "        },\n",
    "        \"Validation\": {\n",
    "            'CMS-Flux': 1.2,\n",
    "            'ECCO': 1.4,\n",
    "            'ISSM': 1.2,\n",
    "            'MOMO-CHEM': 1.15,\n",
    "            'RAPID': 1.25\n",
    "        },\n",
    "        \"Cryospheric Indicators\": {\n",
    "            'ECCO': 1.35,\n",
    "            'ISSM': 1.9\n",
    "        },\n",
    "        \"Air Quality\": {\n",
    "            'CMS-Flux': 1.4,\n",
    "            'MOMO-CHEM': 1.9\n",
    "        },\n",
    "        \"Floods\": {\n",
    "            'ISSM': 1.15,\n",
    "            'RAPID': 1.6\n",
    "        },\n",
    "        \"Environmental Impacts\": {\n",
    "            'MOMO-CHEM': 1.25\n",
    "        },\n",
    "        \"Severe Storms\": {\n",
    "            'ECCO': 1.2\n",
    "        },\n",
    "        \"Earthquakes\": {\n",
    "            'ECCO': 1.05\n",
    "        },\n",
    "        \"Droughts\": {\n",
    "            'CMS-Flux': 1.2,\n",
    "            'RAPID': 1.4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return _research_area_affinity_cache\n",
    "\n",
    "def get_division_model_affinities():\n",
    "    # Get division affinities\n",
    "    global _division_affinity_cache\n",
    "    \n",
    "    if _division_affinity_cache is not None:\n",
    "        return _division_affinity_cache\n",
    "    \n",
    "    _division_affinity_cache = {\n",
    "        'Earth Science': {\n",
    "            'ECCO': 1.3,\n",
    "            'RAPID': 1.2,\n",
    "            'CMS-Flux': 1.15,\n",
    "            'MOMO-CHEM': 1.1,\n",
    "            'ISSM': 1.15,\n",
    "            'CARDAMOM': 1.15\n",
    "        },\n",
    "        'Biological and Physical Sciences': {\n",
    "            'CARDAMOM': 1.2,\n",
    "            'CMS-Flux': 1.1,\n",
    "        },\n",
    "        'Heliophysics': {\n",
    "            'MOMO-CHEM': 1.15,\n",
    "        },\n",
    "        'Planetary Science': {\n",
    "            'ISSM': 1.1,\n",
    "        },\n",
    "        'Astrophysics': {}\n",
    "    }\n",
    "    \n",
    "    return _division_affinity_cache\n",
    "\n",
    "def get_model_specific_thresholds():\n",
    "    # Get model-specific thresholds\n",
    "    return MODEL_THRESHOLDS.copy()\n",
    "\n",
    "def analyze_threshold_performance(results: List[Dict], model_thresholds: Dict[str, float] = None, \n",
    "                                 overall_threshold: float = 0.4) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze model performance with custom thresholds\n",
    "    \n",
    "    Args:\n",
    "        results: List of publication results with ground truth\n",
    "        model_thresholds: Custom threshold values by model (defaults to MODEL_THRESHOLDS)\n",
    "        overall_threshold: Default threshold for models without specific threshold\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing performance metrics using the specified thresholds\n",
    "    \"\"\"\n",
    "    if model_thresholds is None:\n",
    "        model_thresholds = MODEL_THRESHOLDS.copy()\n",
    "    \n",
    "    # Extract publications with ground truth\n",
    "    publications_with_truth = [\n",
    "        result for result in results \n",
    "        if 'models' in result and result['models'] and 'confidence_scores' in result\n",
    "    ]\n",
    "    \n",
    "    if not publications_with_truth:\n",
    "        return {'error': 'No publications with ground truth for evaluation'}\n",
    "    \n",
    "    # Get all unique models\n",
    "    all_models = set()\n",
    "    for pub in publications_with_truth:\n",
    "        all_models.update(pub.get('models', []))\n",
    "        for model in pub.get('confidence_scores', {}).keys():\n",
    "            all_models.add(model)\n",
    "    all_models = sorted(list(all_models))\n",
    "    \n",
    "    # Collect performance metrics\n",
    "    model_metrics = {}\n",
    "    overall_metrics = {'tp': 0, 'fp': 0, 'fn': 0}\n",
    "    \n",
    "    for model in all_models:\n",
    "        # For each model, collect predictions using custom thresholds\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        # Apply model-specific threshold or fall back to overall\n",
    "        threshold = model_thresholds.get(model, overall_threshold)\n",
    "        \n",
    "        for pub in publications_with_truth:\n",
    "            is_true_match = 1 if model in pub.get('models', []) else 0\n",
    "            confidence = pub.get('confidence_scores', {}).get(model, 0)\n",
    "            is_predicted = 1 if confidence >= threshold else 0\n",
    "            \n",
    "            y_true.append(is_true_match)\n",
    "            y_pred.append(is_predicted)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tp = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1)\n",
    "        fp = sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1)\n",
    "        fn = sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        model_metrics[model] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'threshold': threshold\n",
    "        }\n",
    "        \n",
    "        # Accumulate for overall metrics\n",
    "        overall_metrics['tp'] += tp\n",
    "        overall_metrics['fp'] += fp\n",
    "        overall_metrics['fn'] += fn\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_precision = overall_metrics['tp'] / (overall_metrics['tp'] + overall_metrics['fp']) if (overall_metrics['tp'] + overall_metrics['fp']) > 0 else 0\n",
    "    overall_recall = overall_metrics['tp'] / (overall_metrics['tp'] + overall_metrics['fn']) if (overall_metrics['tp'] + overall_metrics['fn']) > 0 else 0\n",
    "    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'per_model': model_metrics,\n",
    "        'overall': {\n",
    "            'precision': overall_precision,\n",
    "            'recall': overall_recall,\n",
    "            'f1': overall_f1,\n",
    "            'tp': overall_metrics['tp'],\n",
    "            'fp': overall_metrics['fp'],\n",
    "            'fn': overall_metrics['fn']\n",
    "        },\n",
    "        'thresholds': {\n",
    "            'model_specific': model_thresholds,\n",
    "            'overall_default': overall_threshold\n",
    "        }\n",
    "    }\n",
    "\n",
    "def find_optimal_thresholds(results: List[Dict], threshold_range=None, step=0.05) -> Dict:\n",
    "    \"\"\"\n",
    "    Find optimal thresholds for each model based on F1 score\n",
    "    \n",
    "    Args:\n",
    "        results: List of publication results with ground truth\n",
    "        threshold_range: Optional range of thresholds to test (min, max)\n",
    "        step: Step size for threshold values\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing optimal thresholds for each model and overall\n",
    "    \"\"\"\n",
    "    if threshold_range is None:\n",
    "        threshold_range = (0.1, 0.95)\n",
    "    \n",
    "    # Generate threshold values to test\n",
    "    thresholds = np.arange(threshold_range[0], threshold_range[1] + step, step)\n",
    "    \n",
    "    # Extract publications with ground truth\n",
    "    publications_with_truth = [\n",
    "        result for result in results \n",
    "        if 'models' in result and result['models'] and 'confidence_scores' in result\n",
    "    ]\n",
    "    \n",
    "    if not publications_with_truth:\n",
    "        return {'error': 'No publications with ground truth for evaluation'}\n",
    "    \n",
    "    # Get all unique models\n",
    "    all_models = set()\n",
    "    for pub in publications_with_truth:\n",
    "        all_models.update(pub.get('models', []))\n",
    "        for model in pub.get('confidence_scores', {}).keys():\n",
    "            all_models.add(model)\n",
    "    all_models = sorted(list(all_models))\n",
    "    \n",
    "    # Find optimal thresholds for each model\n",
    "    optimal_thresholds = {}\n",
    "    \n",
    "    for model in all_models:\n",
    "        best_f1 = -1\n",
    "        best_threshold = 0.4  # Default\n",
    "        best_metrics = {}\n",
    "        \n",
    "        # Extract data for this model\n",
    "        model_data = []\n",
    "        for pub in publications_with_truth:\n",
    "            confidence = pub.get('confidence_scores', {}).get(model, 0)\n",
    "            is_true_match = 1 if model in pub.get('models', []) else 0\n",
    "            model_data.append((confidence, is_true_match))\n",
    "        \n",
    "        # Test each threshold\n",
    "        for threshold in thresholds:\n",
    "            # Calculate metrics at this threshold\n",
    "            tp = sum(1 for conf, true in model_data if true == 1 and conf >= threshold)\n",
    "            fp = sum(1 for conf, true in model_data if true == 0 and conf >= threshold)\n",
    "            fn = sum(1 for conf, true in model_data if true == 1 and conf < threshold)\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # Update if this is the best F1 score\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "                best_metrics = {\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1': f1,\n",
    "                    'tp': tp,\n",
    "                    'fp': fp,\n",
    "                    'fn': fn\n",
    "                }\n",
    "        \n",
    "        optimal_thresholds[model] = {\n",
    "            'threshold': float(best_threshold),\n",
    "            'f1': best_f1,\n",
    "            'metrics': best_metrics\n",
    "        }\n",
    "    \n",
    "    # Find optimal overall threshold\n",
    "    all_data = []\n",
    "    for pub in publications_with_truth:\n",
    "        for model in all_models:\n",
    "            confidence = pub.get('confidence_scores', {}).get(model, 0)\n",
    "            is_true_match = 1 if model in pub.get('models', []) else 0\n",
    "            all_data.append((confidence, is_true_match))\n",
    "    \n",
    "    best_overall_f1 = -1\n",
    "    best_overall_threshold = 0.4  # Default\n",
    "    best_overall_metrics = {}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        # Calculate metrics at this threshold\n",
    "        tp = sum(1 for conf, true in all_data if true == 1 and conf >= threshold)\n",
    "        fp = sum(1 for conf, true in all_data if true == 0 and conf >= threshold)\n",
    "        fn = sum(1 for conf, true in all_data if true == 1 and conf < threshold)\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        # Update if this is the best F1 score\n",
    "        if f1 > best_overall_f1:\n",
    "            best_overall_f1 = f1\n",
    "            best_overall_threshold = threshold\n",
    "            best_overall_metrics = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn\n",
    "            }\n",
    "    \n",
    "    # Create model threshold dictionary\n",
    "    model_threshold_dict = {model: data['threshold'] for model, data in optimal_thresholds.items()}\n",
    "    \n",
    "    return {\n",
    "        'per_model': optimal_thresholds,\n",
    "        'overall': {\n",
    "            'threshold': float(best_overall_threshold),\n",
    "            'f1': best_overall_f1,\n",
    "            'metrics': best_overall_metrics\n",
    "        },\n",
    "        'model_thresholds': model_threshold_dict\n",
    "    }\n",
    "\n",
    "# Semantic matching cache\n",
    "_semantic_match_cache = {}\n",
    "\n",
    "@torch.inference_mode()\n",
    "def semantic_match(publication_text: str, model_embeddings: Dict[str, np.ndarray], \n",
    "                  threshold: float = 0.5, model_thresholds: Dict[str, float] = None) -> Dict[str, float]:\n",
    "    # Semantic matching\n",
    "    cache_key = f\"{hash(publication_text)}_{hash(str(threshold))}\"\n",
    "    if cache_key in _semantic_match_cache:\n",
    "        return _semantic_match_cache[cache_key]\n",
    "    \n",
    "    if model_thresholds is None:\n",
    "        model_thresholds = get_model_specific_thresholds()\n",
    "    \n",
    "    pub_embedding = st_model.encode(publication_text, convert_to_tensor=False)\n",
    "    \n",
    "    model_names = list(model_embeddings.keys())\n",
    "    embeddings_array = np.array([model_embeddings[model] for model in model_names])\n",
    "    \n",
    "    similarities = cosine_similarity([pub_embedding], embeddings_array)[0]\n",
    "    \n",
    "    results = {}\n",
    "    for i, model in enumerate(model_names):\n",
    "        sim = similarities[i]\n",
    "        model_threshold = model_thresholds.get(model, threshold)\n",
    "        \n",
    "        if sim >= model_threshold:\n",
    "            results[model] = float(sim)\n",
    "    \n",
    "    _semantic_match_cache[cache_key] = results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c1a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "_matched_publications_cache = {}\n",
    "_improved_matching_cache = {}\n",
    "\n",
    "def match_models_improved(\n",
    "    publication: Dict, \n",
    "    curated_mapping: Dict[str, str],\n",
    "    model_keywords: Dict[str, List[str]],\n",
    "    model_embeddings: Dict[str, np.ndarray],\n",
    "    science_classifier,\n",
    "    context_manager=None,\n",
    "    ranker=None,\n",
    "    threshold: float = 0.65,\n",
    "    research_area_affinities=None,\n",
    "    division_affinities=None,\n",
    "    model_thresholds=None,\n",
    "    include_classifications: bool = True\n",
    ") -> Dict:\n",
    "    # Match publication to models\n",
    "    # Try to use cache\n",
    "    pub_key = None\n",
    "    if 'DOI' in publication and publication['DOI']:\n",
    "        pub_key = f\"doi:{normalize_doi(publication['DOI'])}\"\n",
    "    elif 'title' in publication:\n",
    "        if isinstance(publication['title'], list) and publication['title']:\n",
    "            pub_key = f\"title:{publication['title'][0]}\"\n",
    "        elif isinstance(publication['title'], str):\n",
    "            pub_key = f\"title:{publication['title']}\"\n",
    "    \n",
    "    if pub_key and pub_key in _improved_matching_cache:\n",
    "        return _improved_matching_cache[pub_key]\n",
    "    \n",
    "    # Load affinities if needed\n",
    "    if research_area_affinities is None:\n",
    "        research_area_affinities = get_research_area_model_affinities()\n",
    "        \n",
    "    if division_affinities is None:\n",
    "        division_affinities = get_division_model_affinities()\n",
    "    \n",
    "    if model_thresholds is None:\n",
    "        model_thresholds = get_model_specific_thresholds()\n",
    "    \n",
    "    # Initialize scores\n",
    "    all_models = list(model_embeddings.keys())\n",
    "    confidence_scores = {model: 0.0 for model in all_models}\n",
    "    confidence_sources = {model: [] for model in all_models}\n",
    "    \n",
    "    # Step 1: Check curated mapping\n",
    "    doi = normalize_doi(publication.get('DOI', ''))\n",
    "    if doi and doi in curated_mapping:\n",
    "        model = curated_mapping[doi]\n",
    "        confidence_scores[model] = 1.0\n",
    "        confidence_sources[model].append('curated_mapping')\n",
    "    \n",
    "    # Step 2: Extract text\n",
    "    title = \"\"\n",
    "    if 'title' in publication:\n",
    "        if isinstance(publication['title'], list) and publication['title']:\n",
    "            title = publication['title'][0]\n",
    "        else:\n",
    "            title = publication.get('title', '')\n",
    "            \n",
    "    abstract = publication.get('abstract', '')\n",
    "    publication_text = f\"{title} {abstract}\"\n",
    "    \n",
    "    # Early exit if curated match and minimal text\n",
    "    model = None\n",
    "    if doi and doi in curated_mapping:\n",
    "        model = curated_mapping[doi]\n",
    "        \n",
    "    if model and (len(publication_text) < 10):\n",
    "        matched_models = [model for model, confidence in confidence_scores.items() \n",
    "                        if confidence >= threshold]\n",
    "        \n",
    "        result = {\n",
    "            'matched_models': matched_models,\n",
    "            'confidence_scores': confidence_scores,\n",
    "            'confidence_sources': confidence_sources\n",
    "        }\n",
    "        \n",
    "        if pub_key:\n",
    "            _improved_matching_cache[pub_key] = result\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    # Step 3: Check keywords\n",
    "    keywords = publication.get('keywords', [])\n",
    "    text_for_keyword_matching = preprocess_text(publication_text)\n",
    "    \n",
    "    keyword_match_counts = {model: 0 for model in all_models}\n",
    "    \n",
    "    for model, model_kw_list in model_keywords.items():\n",
    "        # Skip if already high confidence\n",
    "        if model in confidence_scores and confidence_scores[model] >= 0.9:\n",
    "            continue\n",
    "            \n",
    "        for kw in model_kw_list:\n",
    "            # Check explicit keywords first\n",
    "            found_in_keywords = any(fuzzy_keyword_match(pub_kw, kw)[0] for pub_kw in keywords)\n",
    "            \n",
    "            if found_in_keywords:\n",
    "                keyword_match_counts[model] += 1\n",
    "            else:\n",
    "                # Check text if not found in keywords\n",
    "                match_found, _ = fuzzy_keyword_match(text_for_keyword_matching, kw)\n",
    "                if match_found:\n",
    "                    keyword_match_counts[model] += 1\n",
    "    \n",
    "    # Convert keyword matches to confidence\n",
    "    for model, count in keyword_match_counts.items():\n",
    "        if count > 0:\n",
    "            kw_confidence = min(0.95, 1 / (1 + np.exp(-0.5 * (count - 1))))\n",
    "            \n",
    "            if kw_confidence > 0.2:\n",
    "                confidence_scores[model] = max(confidence_scores[model], kw_confidence)\n",
    "                confidence_sources[model].append('keyword_match')\n",
    "    \n",
    "    # Step 4: Semantic matching\n",
    "    try:\n",
    "        if not any(score >= 0.9 for score in confidence_scores.values()):\n",
    "            semantic_matches = semantic_match(\n",
    "                publication_text, \n",
    "                model_embeddings,\n",
    "                threshold=0.4,\n",
    "                model_thresholds=model_thresholds\n",
    "            )\n",
    "            \n",
    "            for model, similarity in semantic_matches.items():\n",
    "                if similarity > 0.4:\n",
    "                    confidence_scores[model] = max(confidence_scores[model], similarity)\n",
    "                    confidence_sources[model].append('semantic_match')\n",
    "    except Exception as e:\n",
    "        print(f\"Semantic matching failed: {str(e)}\")\n",
    "    \n",
    "    # Step 5: Science classification signals\n",
    "    if not any(score >= 0.9 for score in confidence_scores.values()):\n",
    "        try:\n",
    "            science_results = science_classifier.classify(publication)\n",
    "            \n",
    "            # Apply science keyword affinities\n",
    "            keyword_affinities = get_science_keyword_model_affinities()\n",
    "            science_keywords = science_results.get('science_keywords', [])\n",
    "            for kw_entry in science_keywords:\n",
    "                keyword = kw_entry['label']\n",
    "                keyword_score = kw_entry['score']\n",
    "                \n",
    "                if keyword in keyword_affinities and keyword_score >= 0.3:\n",
    "                    for model, affinity in keyword_affinities[keyword].items():\n",
    "                        confidence = keyword_score * (affinity - 1.0)\n",
    "                        \n",
    "                        if confidence > 0.1:\n",
    "                            confidence_scores[model] = max(confidence_scores[model], confidence)\n",
    "                            confidence_sources[model].append(f'science_keyword:{keyword}')\n",
    "            \n",
    "            # Apply research area affinities\n",
    "            research_areas = science_results.get('research_areas', [])\n",
    "            for area_entry in research_areas:\n",
    "                area = area_entry['label']\n",
    "                area_score = area_entry['score']\n",
    "                \n",
    "                if area in research_area_affinities and area_score >= 0.3:\n",
    "                    for model, affinity in research_area_affinities[area].items():\n",
    "                        confidence = area_score * (affinity - 1.0)\n",
    "                        \n",
    "                        if confidence > 0.1:\n",
    "                            confidence_scores[model] = max(confidence_scores[model], confidence)\n",
    "                            confidence_sources[model].append(f'research_area:{area}')\n",
    "            \n",
    "            # Apply division affinities\n",
    "            division_entry = science_results.get('division')\n",
    "            if division_entry:\n",
    "                division = division_entry['label']\n",
    "                division_score = division_entry['score']\n",
    "                \n",
    "                if division in division_affinities and division_score >= 0.5:\n",
    "                    for model, affinity in division_affinities[division].items():\n",
    "                        confidence = division_score * (affinity - 1.0)\n",
    "                        \n",
    "                        if confidence > 0.05:\n",
    "                            confidence_scores[model] = max(confidence_scores[model], confidence)\n",
    "                            confidence_sources[model].append(f'division:{division}')\n",
    "        except Exception as e:\n",
    "            print(f\"Science classification failed: {str(e)}\")\n",
    "    \n",
    "    # Step 6: Context validation\n",
    "    if context_manager and not any(score >= 0.9 for score in confidence_scores.values()):\n",
    "        try:\n",
    "            context_scores = context_manager.get_context_scores(publication)\n",
    "            for model, score in context_scores.items():\n",
    "                if score > 0.4:\n",
    "                    context_confidence = score * 0.9\n",
    "                    confidence_scores[model] = max(confidence_scores[model], context_confidence)\n",
    "                    confidence_sources[model].append('context_validation')\n",
    "        except Exception as e:\n",
    "            print(f\"Context validation failed: {str(e)}\")\n",
    "    \n",
    "    # Step 7: Relevance ranking\n",
    "    if ranker and not any(score >= 0.9 for score in confidence_scores.values()):\n",
    "        try:\n",
    "            query = publication_text[:500]\n",
    "            top_models = [model for model, score in confidence_scores.items() if score > 0.3]\n",
    "            \n",
    "            if top_models:\n",
    "                rank_scores = ranker.batch_rank(query, top_models)\n",
    "                \n",
    "                for model, score in rank_scores.items():\n",
    "                    if score > 0.3:\n",
    "                        ranker_confidence = score * 0.98\n",
    "                        confidence_scores[model] = max(confidence_scores[model], ranker_confidence)\n",
    "                        confidence_sources[model].append('relevance_ranker')\n",
    "        except Exception as e:\n",
    "            print(f\"Relevance ranking failed: {str(e)}\")\n",
    "    \n",
    "    # Step 8: Apply hybrid boosts\n",
    "    candidate_models = [model for model, score in confidence_scores.items() if score >= 0.3]\n",
    "    for model in candidate_models:\n",
    "        sources = confidence_sources[model]\n",
    "        \n",
    "        # Boost for keyword + semantic matches\n",
    "        if 'keyword_match' in sources and 'semantic_match' in sources:\n",
    "            current_score = confidence_scores[model]\n",
    "            confidence_scores[model] = min(0.95, current_score * 1.05)\n",
    "            if 'hybrid' not in sources:\n",
    "                sources.append('hybrid')\n",
    "        \n",
    "        # Boost for science metadata consensus\n",
    "        science_sources = [s for s in sources if s.startswith(('science_keyword:', 'research_area:', 'division:'))]\n",
    "        if len(science_sources) >= 2:\n",
    "            current_score = confidence_scores[model]\n",
    "            confidence_scores[model] = min(0.95, current_score * 1.12)\n",
    "            if 'science_consensus' not in sources:\n",
    "                sources.append('science_consensus')\n",
    "    \n",
    "    # Filter by thresholds\n",
    "    matched_models = []\n",
    "    for model, confidence in confidence_scores.items():\n",
    "        model_threshold = model_thresholds.get(model, threshold)\n",
    "        if confidence >= model_threshold:\n",
    "            matched_models.append(model)\n",
    "    \n",
    "    # Add classifications\n",
    "    science_results = None\n",
    "    if include_classifications:\n",
    "        try:\n",
    "            # Use existing science results if available\n",
    "            if not science_results:\n",
    "                science_results = science_classifier.classify(publication)\n",
    "                \n",
    "            result_with_classifications = {\n",
    "                'matched_models': matched_models,\n",
    "                'confidence_scores': confidence_scores,\n",
    "                'confidence_sources': confidence_sources,\n",
    "                # Include all classification results\n",
    "                'classifications': {\n",
    "                    'research_areas': science_results.get('research_areas', []),\n",
    "                    'science_keywords': science_results.get('science_keywords', []),\n",
    "                    'division': science_results.get('division')\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add context terms if available\n",
    "            if context_manager:\n",
    "                try:\n",
    "                    prepared_text = science_classifier.get_instance()._prepare_text(publication)\n",
    "                    pub_profile = context_manager._get_pub_profile(prepared_text)\n",
    "                    result_with_classifications['context_terms'] = list(pub_profile['terms'])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error extracting context terms: {str(e)}\")\n",
    "            \n",
    "            # Add relevance scores if available\n",
    "            if ranker and publication_text:\n",
    "                try:\n",
    "                    query = publication_text[:500]\n",
    "                    relevance_scores = ranker.batch_rank(query, all_models)\n",
    "                    result_with_classifications['relevance_scores'] = relevance_scores\n",
    "                except Exception as e:\n",
    "                    print(f\"Error getting relevance scores: {str(e)}\")\n",
    "            \n",
    "            # Extract publication date \n",
    "            pub_date = extract_publication_date(publication)\n",
    "            if pub_date:\n",
    "                result_with_classifications['pubdate'] = pub_date\n",
    "                \n",
    "            # Cache result\n",
    "            if pub_key:\n",
    "                _improved_matching_cache[pub_key] = result_with_classifications\n",
    "                \n",
    "            return result_with_classifications\n",
    "        except Exception as e:\n",
    "            print(f\"Error including classifications: {str(e)}\")\n",
    "    \n",
    "    # Basic result without classifications\n",
    "    result = {\n",
    "        'matched_models': matched_models,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'confidence_sources': confidence_sources\n",
    "    }\n",
    "    \n",
    "    # Extract publication date \n",
    "    pub_date = extract_publication_date(publication)\n",
    "    if pub_date:\n",
    "        result['pubdate'] = pub_date\n",
    "    \n",
    "    # Cache result\n",
    "    if pub_key:\n",
    "        _improved_matching_cache[pub_key] = result\n",
    "        \n",
    "    return result\n",
    "\n",
    "def process_publication_batch(\n",
    "    publications: List[Dict], \n",
    "    curated_mapping: Dict[str, str], \n",
    "    model_keywords: Dict[str, List[str]], \n",
    "    model_embeddings: Dict[str, np.ndarray], \n",
    "    science_classifier,\n",
    "    context_manager,\n",
    "    ranker=None,\n",
    "    batch_size: int = 100,\n",
    "    include_classifications: bool = True\n",
    ") -> List[Dict]:\n",
    "    # Process publications in batches\n",
    "    results = []\n",
    "    total_pubs = len(publications)\n",
    "    num_batches = (total_pubs + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Load affinity\n",
    "    research_area_affinities = get_research_area_model_affinities()\n",
    "    division_affinities = get_division_model_affinities()\n",
    "    model_thresholds = get_model_specific_thresholds()\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_pubs)\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{num_batches} (publications {start_idx+1}-{end_idx}/{total_pubs})...\")\n",
    "        \n",
    "        batch_pubs = publications[start_idx:end_idx]\n",
    "        batch_results = []\n",
    "        \n",
    "        # Process each publication\n",
    "        for i, pub in enumerate(batch_pubs):\n",
    "            pub_idx = start_idx + i + 1\n",
    "            if pub_idx % 20 == 0:\n",
    "                print(f\"  Processing publication {pub_idx}/{total_pubs}...\")\n",
    "            \n",
    "            # Create cache\n",
    "            pub_key = None\n",
    "            if 'DOI' in pub and pub['DOI']:\n",
    "                pub_key = f\"doi:{normalize_doi(pub['DOI'])}\"\n",
    "            elif 'title' in pub:\n",
    "                if isinstance(pub['title'], list) and pub['title']:\n",
    "                    pub_key = f\"title:{pub['title'][0]}\"\n",
    "                elif isinstance(pub['title'], str):\n",
    "                    pub_key = f\"title:{pub['title']}\"\n",
    "            \n",
    "            # Check cache\n",
    "            if pub_key and pub_key in _matched_publications_cache:\n",
    "                cached_result = _matched_publications_cache[pub_key]\n",
    "                pub_copy = pub.copy()\n",
    "                pub_copy.update(cached_result)\n",
    "                batch_results.append(pub_copy)\n",
    "                continue\n",
    "            \n",
    "            # Match models\n",
    "            try:\n",
    "                match_result = match_models_improved(\n",
    "                    pub, \n",
    "                    curated_mapping, \n",
    "                    model_keywords, \n",
    "                    model_embeddings, \n",
    "                    science_classifier,\n",
    "                    context_manager,\n",
    "                    ranker,\n",
    "                    research_area_affinities=research_area_affinities,\n",
    "                    division_affinities=division_affinities,\n",
    "                    model_thresholds=model_thresholds,\n",
    "                    include_classifications=include_classifications\n",
    "                )\n",
    "                \n",
    "                # Add results to publication\n",
    "                pub_copy = pub.copy()\n",
    "                # Add all fields from match_result\n",
    "                for key, value in match_result.items():\n",
    "                    pub_copy[key] = value\n",
    "                \n",
    "                # Extract characteristics if needed\n",
    "                if ranker is not None:\n",
    "                    pub_copy['pub_characteristics'] = extract_publication_characteristics(pub)\n",
    "                \n",
    "                # Add to results\n",
    "                batch_results.append(pub_copy)\n",
    "                \n",
    "                # Cache results\n",
    "                if pub_key:\n",
    "                    cache_value = {\n",
    "                        'matched_models': match_result['matched_models'],\n",
    "                        'confidence_scores': match_result['confidence_scores'],\n",
    "                        'confidence_sources': match_result['confidence_sources']\n",
    "                    }\n",
    "                    if 'pub_characteristics' in pub_copy:\n",
    "                        cache_value['pub_characteristics'] = pub_copy['pub_characteristics']\n",
    "                    _matched_publications_cache[pub_key] = cache_value\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing publication {pub_idx}: {str(e)}\")\n",
    "                # Add publication without matches\n",
    "                pub_copy = pub.copy()\n",
    "                pub_copy['matched_models'] = []\n",
    "                pub_copy['confidence_scores'] = {}\n",
    "                pub_copy['confidence_sources'] = {}\n",
    "                batch_results.append(pub_copy)\n",
    "        \n",
    "        # Add batch results to overall results\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # Periodically clean caches\n",
    "        if batch_idx % 5 == 4:\n",
    "            _fuzzy_match_cache.clear()\n",
    "            _semantic_match_cache.clear()\n",
    "            \n",
    "            # Clean up memory\n",
    "            optimize_memory()\n",
    "        \n",
    "        print(f\"Completed batch {batch_idx+1}/{num_batches}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3bd2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_metrics(results: List[Dict], output_path_base: str = \"./metrics_visualization\"):\n",
    "    # Create visualizations\n",
    "\n",
    "    # Extract publications with ground truth\n",
    "    publications_with_truth = [\n",
    "        result for result in results \n",
    "        if 'models' in result and result['models'] and 'matched_models' in result\n",
    "    ]\n",
    "    \n",
    "    if not publications_with_truth:\n",
    "        print(\"No publications with ground truth for evaluation\")\n",
    "        return\n",
    "        \n",
    "    # Get all unique models\n",
    "    all_models = set()\n",
    "    for pub in publications_with_truth:\n",
    "        all_models.update(pub.get('models', []))\n",
    "        all_models.update(pub.get('matched_models', []))\n",
    "    all_models = sorted(list(all_models))\n",
    "    \n",
    "    # Dictionary to store all metrics\n",
    "    complete_metrics = {\n",
    "        'model_performance': {},\n",
    "        'source_analysis': {},\n",
    "        'classification_accuracy': {},\n",
    "        'confidence_analysis': {},\n",
    "        'temporal_analysis': {},\n",
    "        'threshold_analysis': {}  # New for threshold analysis\n",
    "    }\n",
    "    \n",
    "    # 1. Model Performance Metrics\n",
    "    model_metrics = {}\n",
    "    metrics = {\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "    \n",
    "    for model in all_models:\n",
    "        # For each model, collect binary predictions\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        for pub in publications_with_truth:\n",
    "            y_true.append(1 if model in pub.get('models', []) else 0)\n",
    "            y_pred.append(1 if model in pub.get('matched_models', []) else 0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        model_metrics[model] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'true_positives': sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 1),\n",
    "            'false_positives': sum(1 for t, p in zip(y_true, y_pred) if t == 0 and p == 1),\n",
    "            'false_negatives': sum(1 for t, p in zip(y_true, y_pred) if t == 1 and p == 0),\n",
    "            'match_count': sum(y_pred)\n",
    "        }\n",
    "        \n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['f1'].append(f1)\n",
    "    \n",
    "    # Calculate micro-average metrics\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    \n",
    "    for pub in publications_with_truth:\n",
    "        true_labels = [1 if model in pub.get('models', []) else 0 for model in all_models]\n",
    "        pred_labels = [1 if model in pub.get('matched_models', []) else 0 for model in all_models]\n",
    "        \n",
    "        all_y_true.extend(true_labels)\n",
    "        all_y_pred.extend(pred_labels)\n",
    "    \n",
    "    micro_precision = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    micro_recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    micro_f1 = f1_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    \n",
    "    complete_metrics['model_performance'] = {\n",
    "        'per_model': model_metrics,\n",
    "        'micro_average': {\n",
    "            'precision': micro_precision,\n",
    "            'recall': micro_recall,\n",
    "            'f1': micro_f1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create model performance visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Plot per-model metrics\n",
    "    x = np.arange(len(all_models))\n",
    "    width = 0.25\n",
    "    \n",
    "    # Skip if no models\n",
    "    if len(all_models) > 0:\n",
    "        bars1 = ax1.bar(x - width, metrics['precision'], width, label='Precision')\n",
    "        bars2 = ax1.bar(x, metrics['recall'], width, label='Recall')\n",
    "        bars3 = ax1.bar(x + width, metrics['f1'], width, label='F1 Score')\n",
    "        \n",
    "        ax1.set_xlabel('Models')\n",
    "        ax1.set_ylabel('Score')\n",
    "        ax1.set_title('Precision, Recall, and F1 Score by Model')\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(all_models, rotation=45, ha='right')\n",
    "        ax1.legend()\n",
    "        ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add values on bars\n",
    "        def add_labels(bars):\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{height:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        add_labels(bars1)\n",
    "        add_labels(bars2)\n",
    "        add_labels(bars3)\n",
    "    \n",
    "    # Plot micro-average metrics\n",
    "    micro_metrics = {\n",
    "        'Precision': micro_precision,\n",
    "        'Recall': micro_recall,\n",
    "        'F1 Score': micro_f1\n",
    "    }\n",
    "    \n",
    "    x2 = np.arange(len(micro_metrics))\n",
    "    bars = ax2.bar(x2, micro_metrics.values(), width=0.4)\n",
    "    \n",
    "    ax2.set_xlabel('Metrics')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Overall Micro-Average Metrics')\n",
    "    ax2.set_xticks(x2)\n",
    "    ax2.set_xticklabels(micro_metrics.keys())\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add values on top of the bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_model_performance.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    # 2. Confidence Analysis\n",
    "\n",
    "    # Extract confidence scores\n",
    "    confidence_data = {}\n",
    "    \n",
    "    for model in all_models:\n",
    "        confidence_scores = []\n",
    "        correct_predictions = []\n",
    "        \n",
    "        for pub in publications_with_truth:\n",
    "            is_true_match = model in pub.get('models', [])\n",
    "            is_predicted = model in pub.get('matched_models', [])\n",
    "            confidence = pub.get('confidence_scores', {}).get(model, 0)\n",
    "            \n",
    "            confidence_scores.append(confidence)\n",
    "            correct_predictions.append(1 if is_true_match == is_predicted else 0)\n",
    "        \n",
    "        confidence_data[model] = {\n",
    "            'scores': confidence_scores,\n",
    "            'correct_predictions': correct_predictions,\n",
    "            'mean_confidence': np.mean(confidence_scores) if confidence_scores else 0,\n",
    "            'median_confidence': np.median(confidence_scores) if confidence_scores else 0\n",
    "        }\n",
    "    \n",
    "    complete_metrics['confidence_analysis'] = confidence_data\n",
    "    \n",
    "    # Create confidence visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot mean confidence by model\n",
    "    mean_confidence = [confidence_data[model]['mean_confidence'] for model in all_models]\n",
    "    \n",
    "    if all_models:\n",
    "        ax1.bar(all_models, mean_confidence)\n",
    "        ax1.set_xlabel('Models')\n",
    "        ax1.set_ylabel('Mean Confidence Score')\n",
    "        ax1.set_title('Mean Confidence Score by Model')\n",
    "        ax1.set_xticklabels(all_models, rotation=45, ha='right')\n",
    "        ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot confidence distribution (boxplot)\n",
    "    if all_models:\n",
    "        confidence_values = [confidence_data[model]['scores'] for model in all_models]\n",
    "        ax2.boxplot(confidence_values, labels=all_models)\n",
    "        ax2.set_xlabel('Models')\n",
    "        ax2.set_ylabel('Confidence Score Distribution')\n",
    "        ax2.set_title('Confidence Score Distribution by Model')\n",
    "        ax2.set_xticklabels(all_models, rotation=45, ha='right')\n",
    "        ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_confidence_analysis.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    # 3. Source Analysis\n",
    "\n",
    "    # Analyze the sources used for matches\n",
    "    source_counts = {}\n",
    "    source_accuracy = {}\n",
    "    \n",
    "    for pub in publications_with_truth:\n",
    "        for model in all_models:\n",
    "            sources = pub.get('confidence_sources', {}).get(model, [])\n",
    "            is_true_match = model in pub.get('models', [])\n",
    "            is_predicted = model in pub.get('matched_models', [])\n",
    "            \n",
    "            for source in sources:\n",
    "                if source not in source_counts:\n",
    "                    source_counts[source] = 0\n",
    "                    source_accuracy[source] = {'correct': 0, 'total': 0}\n",
    "                \n",
    "                source_counts[source] += 1\n",
    "                \n",
    "                # Track accuracy\n",
    "                source_accuracy[source]['total'] += 1\n",
    "                if is_true_match == is_predicted:\n",
    "                    source_accuracy[source]['correct'] += 1\n",
    "    \n",
    "    # Calculate accuracy rates\n",
    "    for source in source_accuracy:\n",
    "        if source_accuracy[source]['total'] > 0:\n",
    "            source_accuracy[source]['accuracy'] = source_accuracy[source]['correct'] / source_accuracy[source]['total']\n",
    "        else:\n",
    "            source_accuracy[source]['accuracy'] = 0\n",
    "    \n",
    "    complete_metrics['source_analysis'] = {\n",
    "        'counts': source_counts,\n",
    "        'accuracy': source_accuracy\n",
    "    }\n",
    "    \n",
    "    # Create source analysis visualization\n",
    "    if source_counts:\n",
    "        # Prepare data\n",
    "        sources = list(source_counts.keys())\n",
    "        counts = [source_counts[s] for s in sources]\n",
    "        accuracies = [source_accuracy[s]['accuracy'] for s in sources]\n",
    "        \n",
    "        # Sort by count\n",
    "        sorted_data = sorted(zip(sources, counts, accuracies), key=lambda x: x[1], reverse=True)\n",
    "        sources = [x[0] for x in sorted_data]\n",
    "        counts = [x[1] for x in sorted_data]\n",
    "        accuracies = [x[2] for x in sorted_data]\n",
    "        \n",
    "        # Limit to top 15 sources for better visualization\n",
    "        if len(sources) > 15:\n",
    "            sources = sources[:15]\n",
    "            counts = counts[:15]\n",
    "            accuracies = accuracies[:15]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot counts\n",
    "        ax1.bar(sources, counts)\n",
    "        ax1.set_xlabel('Confidence Sources')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.set_title('Usage Count by Confidence Source')\n",
    "        ax1.set_xticklabels(sources, rotation=45, ha='right')\n",
    "        ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Plot accuracy\n",
    "        ax2.bar(sources, accuracies)\n",
    "        ax2.set_xlabel('Confidence Sources')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.set_title('Accuracy by Confidence Source')\n",
    "        ax2.set_xticklabels(sources, rotation=45, ha='right')\n",
    "        ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_path_base}_source_analysis.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    \n",
    "    # 4. Temporal Analysis\n",
    "\n",
    "    # Analyze performance over time by publication date\n",
    "    date_metrics = {}\n",
    "    \n",
    "    for pub in publications_with_truth:\n",
    "        pub_date = pub.get('pubdate')\n",
    "        if not pub_date or len(pub_date) < 7:  # Ensure proper format\n",
    "            continue\n",
    "            \n",
    "        year_month = pub_date  # Already in yyyy-mm format\n",
    "        \n",
    "        if year_month not in date_metrics:\n",
    "            date_metrics[year_month] = {\n",
    "                'total': 0,\n",
    "                'correct': 0,\n",
    "                'precision': [],\n",
    "                'recall': [],\n",
    "                'f1': []\n",
    "            }\n",
    "        \n",
    "        # Count publications\n",
    "        date_metrics[year_month]['total'] += 1\n",
    "        \n",
    "        # Calculate accuracy for this publication\n",
    "        true_models = set(pub.get('models', []))\n",
    "        pred_models = set(pub.get('matched_models', []))\n",
    "        \n",
    "        # True positives\n",
    "        tp = len(true_models.intersection(pred_models))\n",
    "        # False positives\n",
    "        fp = len(pred_models - true_models)\n",
    "        # False negatives\n",
    "        fn = len(true_models - pred_models)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        date_metrics[year_month]['precision'].append(precision)\n",
    "        date_metrics[year_month]['recall'].append(recall)\n",
    "        date_metrics[year_month]['f1'].append(f1)\n",
    "        \n",
    "        # Mark as correct if all matches are correct\n",
    "        if true_models == pred_models:\n",
    "            date_metrics[year_month]['correct'] += 1\n",
    "    \n",
    "    # Calculate average metrics by date\n",
    "    for date in date_metrics:\n",
    "        if date_metrics[date]['total'] > 0:\n",
    "            date_metrics[date]['accuracy'] = date_metrics[date]['correct'] / date_metrics[date]['total']\n",
    "            date_metrics[date]['avg_precision'] = np.mean(date_metrics[date]['precision'])\n",
    "            date_metrics[date]['avg_recall'] = np.mean(date_metrics[date]['recall'])\n",
    "            date_metrics[date]['avg_f1'] = np.mean(date_metrics[date]['f1'])\n",
    "    \n",
    "    complete_metrics['temporal_analysis'] = date_metrics\n",
    "    \n",
    "    # Create temporal analysis visualization\n",
    "    if date_metrics:\n",
    "        # Sort dates chronologically\n",
    "        sorted_dates = sorted(date_metrics.keys())\n",
    "        \n",
    "        if len(sorted_dates) > 1:  # Only plot if we have multiple dates\n",
    "            accuracies = [date_metrics[d]['accuracy'] for d in sorted_dates]\n",
    "            precisions = [date_metrics[d]['avg_precision'] for d in sorted_dates]\n",
    "            recalls = [date_metrics[d]['avg_recall'] for d in sorted_dates]\n",
    "            f1_scores = [date_metrics[d]['avg_f1'] for d in sorted_dates]\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            \n",
    "            ax.plot(sorted_dates, accuracies, 'o-', label='Accuracy')\n",
    "            ax.plot(sorted_dates, precisions, 's-', label='Precision')\n",
    "            ax.plot(sorted_dates, recalls, '^-', label='Recall')\n",
    "            ax.plot(sorted_dates, f1_scores, 'D-', label='F1 Score')\n",
    "            \n",
    "            ax.set_xlabel('Publication Date')\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_title('Performance Metrics by Publication Date')\n",
    "            ax.grid(True, linestyle='--', alpha=0.7)\n",
    "            ax.legend()\n",
    "            \n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_path_base}_temporal_analysis.png\", dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "    \n",
    "    # 5. Classification Accuracy Analysis\n",
    "\n",
    "    # Analyze how classification results correlate with model matches\n",
    "    research_area_stats = {}\n",
    "    science_keyword_stats = {}\n",
    "    division_stats = {}\n",
    "    \n",
    "    # Extract data\n",
    "    for pub in publications_with_truth:\n",
    "        # Check if we have classification data\n",
    "        classifications = pub.get('classifications', {})\n",
    "        if not classifications:\n",
    "            continue\n",
    "            \n",
    "        # Get match accuracy for this publication\n",
    "        true_models = set(pub.get('models', []))\n",
    "        pred_models = set(pub.get('matched_models', []))\n",
    "        is_correct = (true_models == pred_models)\n",
    "        \n",
    "        # Process research areas\n",
    "        for area in classifications.get('research_areas', []):\n",
    "            area_name = area.get('label', '')\n",
    "            area_score = area.get('score', 0)\n",
    "            \n",
    "            if area_name and area_score > 0.3:  # Only consider significant areas\n",
    "                if area_name not in research_area_stats:\n",
    "                    research_area_stats[area_name] = {'correct': 0, 'total': 0}\n",
    "                \n",
    "                research_area_stats[area_name]['total'] += 1\n",
    "                if is_correct:\n",
    "                    research_area_stats[area_name]['correct'] += 1\n",
    "        \n",
    "        # Process science keywords\n",
    "        for keyword in classifications.get('science_keywords', []):\n",
    "            kw_name = keyword.get('label', '')\n",
    "            kw_score = keyword.get('score', 0)\n",
    "            \n",
    "            if kw_name and kw_score > 0.3:  # Only consider significant keywords\n",
    "                if kw_name not in science_keyword_stats:\n",
    "                    science_keyword_stats[kw_name] = {'correct': 0, 'total': 0}\n",
    "                \n",
    "                science_keyword_stats[kw_name]['total'] += 1\n",
    "                if is_correct:\n",
    "                    science_keyword_stats[kw_name]['correct'] += 1\n",
    "        \n",
    "        # Process division\n",
    "        division = classifications.get('division', {})\n",
    "        if division:\n",
    "            div_name = division.get('label', '')\n",
    "            div_score = division.get('score', 0)\n",
    "            \n",
    "            if div_name and div_score > 0.5:  # Only consider significant divisions\n",
    "                if div_name not in division_stats:\n",
    "                    division_stats[div_name] = {'correct': 0, 'total': 0}\n",
    "                \n",
    "                division_stats[div_name]['total'] += 1\n",
    "                if is_correct:\n",
    "                    division_stats[div_name]['correct'] += 1\n",
    "    \n",
    "    # Calculate accuracy rates\n",
    "    for area in research_area_stats:\n",
    "        if research_area_stats[area]['total'] > 0:\n",
    "            research_area_stats[area]['accuracy'] = research_area_stats[area]['correct'] / research_area_stats[area]['total']\n",
    "    \n",
    "    for kw in science_keyword_stats:\n",
    "        if science_keyword_stats[kw]['total'] > 0:\n",
    "            science_keyword_stats[kw]['accuracy'] = science_keyword_stats[kw]['correct'] / science_keyword_stats[kw]['total']\n",
    "    \n",
    "    for div in division_stats:\n",
    "        if division_stats[div]['total'] > 0:\n",
    "            division_stats[div]['accuracy'] = division_stats[div]['correct'] / division_stats[div]['total']\n",
    "    \n",
    "    complete_metrics['classification_accuracy'] = {\n",
    "        'research_areas': research_area_stats,\n",
    "        'science_keywords': science_keyword_stats,\n",
    "        'divisions': division_stats\n",
    "    }\n",
    "    \n",
    "    # Create classification accuracy visualization\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15))\n",
    "    \n",
    "    # Filter out classifications with too few samples\n",
    "    def filter_and_sort(stats_dict, min_samples=5):\n",
    "        filtered = {k: v for k, v in stats_dict.items() if v['total'] >= min_samples}\n",
    "        return sorted(filtered.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n",
    "    \n",
    "    # Plot research area accuracy\n",
    "    sorted_areas = filter_and_sort(research_area_stats)\n",
    "    if sorted_areas:\n",
    "        area_names = [a[0] for a in sorted_areas]\n",
    "        area_accuracies = [a[1]['accuracy'] for a in sorted_areas]\n",
    "        area_counts = [a[1]['total'] for a in sorted_areas]\n",
    "        \n",
    "        # Limit to top 15 for better visualization\n",
    "        if len(area_names) > 15:\n",
    "            area_names = area_names[:15]\n",
    "            area_accuracies = area_accuracies[:15]\n",
    "            area_counts = area_counts[:15]\n",
    "            \n",
    "        bars = ax1.barh(area_names, area_accuracies)\n",
    "        ax1.set_xlabel('Accuracy')\n",
    "        ax1.set_ylabel('Research Area')\n",
    "        ax1.set_title('Match Accuracy by Research Area')\n",
    "        ax1.set_xlim(0, 1)\n",
    "        \n",
    "        # Add count labels to bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'n={area_counts[i]}', ha='left', va='center')\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, 'Insufficient research area data', \n",
    "                ha='center', va='center', transform=ax1.transAxes)\n",
    "    \n",
    "    # Plot science keyword accuracy\n",
    "    sorted_keywords = filter_and_sort(science_keyword_stats)\n",
    "    if sorted_keywords:\n",
    "        kw_names = [k[0] for k in sorted_keywords]\n",
    "        kw_accuracies = [k[1]['accuracy'] for k in sorted_keywords]\n",
    "        kw_counts = [k[1]['total'] for k in sorted_keywords]\n",
    "        \n",
    "        # Limit to top 15 for better visualization\n",
    "        if len(kw_names) > 15:\n",
    "            kw_names = kw_names[:15]\n",
    "            kw_accuracies = kw_accuracies[:15]\n",
    "            kw_counts = kw_counts[:15]\n",
    "            \n",
    "        bars = ax2.barh(kw_names, kw_accuracies)\n",
    "        ax2.set_xlabel('Accuracy')\n",
    "        ax2.set_ylabel('Science Keyword')\n",
    "        ax2.set_title('Match Accuracy by Science Keyword')\n",
    "        ax2.set_xlim(0, 1)\n",
    "        \n",
    "        # Add count labels to bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax2.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'n={kw_counts[i]}', ha='left', va='center')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'Insufficient science keyword data', \n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "    \n",
    "    # Plot division accuracy\n",
    "    sorted_divisions = filter_and_sort(division_stats)\n",
    "    if sorted_divisions:\n",
    "        div_names = [d[0] for d in sorted_divisions]\n",
    "        div_accuracies = [d[1]['accuracy'] for d in sorted_divisions]\n",
    "        div_counts = [d[1]['total'] for d in sorted_divisions]\n",
    "            \n",
    "        bars = ax3.barh(div_names, div_accuracies)\n",
    "        ax3.set_xlabel('Accuracy')\n",
    "        ax3.set_ylabel('Division')\n",
    "        ax3.set_title('Match Accuracy by Division')\n",
    "        ax3.set_xlim(0, 1)\n",
    "        \n",
    "        # Add count labels to bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            width = bar.get_width()\n",
    "            ax3.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'n={div_counts[i]}', ha='left', va='center')\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'Insufficient division data', \n",
    "                ha='center', va='center', transform=ax3.transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_classification_accuracy.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Threshold Analysis (New)\n",
    "    \n",
    "    # For each model, analyze different confidence thresholds\n",
    "    threshold_analysis = {}\n",
    "    \n",
    "    # Generate possible threshold values to test\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "    \n",
    "    for model in all_models:\n",
    "        threshold_metrics = {t: {'precision': 0, 'recall': 0, 'f1': 0, 'tp': 0, 'fp': 0, 'fn': 0} \n",
    "                             for t in thresholds}\n",
    "        \n",
    "        # Extract confidence scores and true labels for this model\n",
    "        confidence_scores = []\n",
    "        true_labels = []\n",
    "        \n",
    "        for pub in publications_with_truth:\n",
    "            score = pub.get('confidence_scores', {}).get(model, 0)\n",
    "            is_true_match = 1 if model in pub.get('models', []) else 0\n",
    "            \n",
    "            confidence_scores.append(score)\n",
    "            true_labels.append(is_true_match)\n",
    "        \n",
    "        # Calculate metrics at each threshold\n",
    "        for threshold in thresholds:\n",
    "            # Generate predictions using this threshold\n",
    "            predicted_labels = [1 if score >= threshold else 0 for score in confidence_scores]\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            tp = sum(1 for t, p in zip(true_labels, predicted_labels) if t == 1 and p == 1)\n",
    "            fp = sum(1 for t, p in zip(true_labels, predicted_labels) if t == 0 and p == 1)\n",
    "            fn = sum(1 for t, p in zip(true_labels, predicted_labels) if t == 1 and p == 0)\n",
    "            \n",
    "            # Precision, recall, and F1\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            threshold_metrics[threshold] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn\n",
    "            }\n",
    "        \n",
    "        # Find optimal threshold based on F1 score\n",
    "        f1_scores = [(t, metrics['f1']) for t, metrics in threshold_metrics.items()]\n",
    "        optimal_threshold = max(f1_scores, key=lambda x: x[1])[0] if f1_scores else 0.5\n",
    "        \n",
    "        threshold_analysis[model] = {\n",
    "            'metrics': threshold_metrics,\n",
    "            'optimal_threshold': optimal_threshold,\n",
    "            'optimal_f1': threshold_metrics[optimal_threshold]['f1'],\n",
    "            'current_threshold': MODEL_THRESHOLDS.get(model, 0.4)\n",
    "        }\n",
    "    \n",
    "    # Calculate overall threshold analysis\n",
    "    overall_threshold_metrics = {t: {'precision': 0, 'recall': 0, 'f1': 0} for t in thresholds}\n",
    "    \n",
    "    # Get all confidence scores and labels across all models\n",
    "    all_confidence_scores = []\n",
    "    all_true_labels = []\n",
    "    \n",
    "    for pub in publications_with_truth:\n",
    "        for model in all_models:\n",
    "            score = pub.get('confidence_scores', {}).get(model, 0)\n",
    "            is_true_match = 1 if model in pub.get('models', []) else 0\n",
    "            \n",
    "            all_confidence_scores.append(score)\n",
    "            all_true_labels.append(is_true_match)\n",
    "    \n",
    "    # Calculate metrics for each threshold\n",
    "    for threshold in thresholds:\n",
    "        predicted_labels = [1 if score >= threshold else 0 for score in all_confidence_scores]\n",
    "        \n",
    "        precision = precision_score(all_true_labels, predicted_labels, zero_division=0)\n",
    "        recall = recall_score(all_true_labels, predicted_labels, zero_division=0)\n",
    "        f1 = f1_score(all_true_labels, predicted_labels, zero_division=0)\n",
    "        \n",
    "        overall_threshold_metrics[threshold] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    \n",
    "    # Find optimal overall threshold\n",
    "    overall_f1_scores = [(t, metrics['f1']) for t, metrics in overall_threshold_metrics.items()]\n",
    "    overall_optimal_threshold = max(overall_f1_scores, key=lambda x: x[1])[0] if overall_f1_scores else 0.5\n",
    "    \n",
    "    threshold_analysis['overall'] = {\n",
    "        'metrics': overall_threshold_metrics,\n",
    "        'optimal_threshold': overall_optimal_threshold,\n",
    "        'optimal_f1': overall_threshold_metrics[overall_optimal_threshold]['f1'],\n",
    "        'current_threshold': 0.4  # Default overall threshold\n",
    "    }\n",
    "    \n",
    "    complete_metrics['threshold_analysis'] = threshold_analysis\n",
    "    \n",
    "    # Create threshold analysis visualizations\n",
    "    # 1. Model-specific threshold analysis\n",
    "    for model in all_models:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        model_thresholds = sorted(list(threshold_analysis[model]['metrics'].keys()))\n",
    "        precision_values = [threshold_analysis[model]['metrics'][t]['precision'] for t in model_thresholds]\n",
    "        recall_values = [threshold_analysis[model]['metrics'][t]['recall'] for t in model_thresholds]\n",
    "        f1_values = [threshold_analysis[model]['metrics'][t]['f1'] for t in model_thresholds]\n",
    "        \n",
    "        ax.plot(model_thresholds, precision_values, 'b-', label='Precision')\n",
    "        ax.plot(model_thresholds, recall_values, 'g-', label='Recall')\n",
    "        ax.plot(model_thresholds, f1_values, 'r-', label='F1 Score')\n",
    "        \n",
    "        # Mark current and optimal thresholds\n",
    "        current_threshold = threshold_analysis[model]['current_threshold']\n",
    "        optimal_threshold = threshold_analysis[model]['optimal_threshold']\n",
    "        \n",
    "        ax.axvline(x=current_threshold, color='gray', linestyle='--', \n",
    "                   label=f'Current Threshold ({current_threshold:.2f})')\n",
    "        ax.axvline(x=optimal_threshold, color='black', linestyle='-', \n",
    "                   label=f'Optimal Threshold ({optimal_threshold:.2f})')\n",
    "        \n",
    "        ax.set_xlabel('Confidence Threshold')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title(f'Threshold Analysis for {model}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_path_base}_threshold_{model}.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    # 2. Overall threshold analysis\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    overall_thresholds = sorted(list(threshold_analysis['overall']['metrics'].keys()))\n",
    "    precision_values = [threshold_analysis['overall']['metrics'][t]['precision'] for t in overall_thresholds]\n",
    "    recall_values = [threshold_analysis['overall']['metrics'][t]['recall'] for t in overall_thresholds]\n",
    "    f1_values = [threshold_analysis['overall']['metrics'][t]['f1'] for t in overall_thresholds]\n",
    "    \n",
    "    ax.plot(overall_thresholds, precision_values, 'b-', label='Precision')\n",
    "    ax.plot(overall_thresholds, recall_values, 'g-', label='Recall')\n",
    "    ax.plot(overall_thresholds, f1_values, 'r-', label='F1 Score')\n",
    "    \n",
    "    # Mark current and optimal thresholds\n",
    "    current_threshold = threshold_analysis['overall']['current_threshold']\n",
    "    optimal_threshold = threshold_analysis['overall']['optimal_threshold']\n",
    "    \n",
    "    ax.axvline(x=current_threshold, color='gray', linestyle='--', \n",
    "               label=f'Current Threshold ({current_threshold:.2f})')\n",
    "    ax.axvline(x=optimal_threshold, color='black', linestyle='-', \n",
    "               label=f'Optimal Threshold ({optimal_threshold:.2f})')\n",
    "    \n",
    "    ax.set_xlabel('Confidence Threshold')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Overall Threshold Analysis')\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_threshold_overall.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Comparative threshold analysis\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Create a bar chart comparing current vs. optimal thresholds\n",
    "    model_names = all_models\n",
    "    current_thresholds = [threshold_analysis[model]['current_threshold'] for model in model_names]\n",
    "    optimal_thresholds = [threshold_analysis[model]['optimal_threshold'] for model in model_names]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, current_thresholds, width, label='Current Threshold')\n",
    "    bars2 = ax.bar(x + width/2, optimal_thresholds, width, label='Optimal Threshold')\n",
    "    \n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Threshold Value')\n",
    "    ax.set_title('Current vs. Optimal Thresholds by Model')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_threshold_comparison.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Create a summary visualization\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Top left: Model F1 scores\n",
    "    if all_models:\n",
    "        f1_values = [model_metrics[model]['f1'] for model in all_models]\n",
    "        bars = axs[0, 0].bar(all_models, f1_values)\n",
    "        axs[0, 0].set_title('F1 Score by Model')\n",
    "        axs[0, 0].set_xlabel('Model')\n",
    "        axs[0, 0].set_ylabel('F1 Score')\n",
    "        axs[0, 0].set_xticklabels(all_models, rotation=45, ha='right')\n",
    "        axs[0, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axs[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                        f'{height:.2f}', ha='center', va='bottom')\n",
    "    else:\n",
    "        axs[0, 0].text(0.5, 0.5, 'No model data available', \n",
    "                     ha='center', va='center', transform=axs[0, 0].transAxes)\n",
    "    \n",
    "    # Top right: Sources accuracy (top 5)\n",
    "    if source_accuracy:\n",
    "        # Get top 5 most used sources\n",
    "        top_sources = sorted(source_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        top_source_names = [s[0] for s in top_sources]\n",
    "        \n",
    "        source_accuracies = [source_accuracy[s]['accuracy'] for s in top_source_names]\n",
    "        source_counts_plot = [source_counts[s] for s in top_source_names]\n",
    "        \n",
    "        bars = axs[0, 1].bar(top_source_names, source_accuracies)\n",
    "        axs[0, 1].set_title('Accuracy by Top 5 Confidence Sources')\n",
    "        axs[0, 1].set_xlabel('Source')\n",
    "        axs[0, 1].set_ylabel('Accuracy')\n",
    "        axs[0, 1].set_xticklabels(top_source_names, rotation=45, ha='right')\n",
    "        axs[0, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add count labels\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            axs[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                          f'n={source_counts_plot[i]}', ha='center', va='bottom')\n",
    "    else:\n",
    "        axs[0, 1].text(0.5, 0.5, 'No source data available', \n",
    "                     ha='center', va='center', transform=axs[0, 1].transAxes)\n",
    "    \n",
    "    # Bottom left: Classification accuracy\n",
    "    if research_area_stats or science_keyword_stats or division_stats:\n",
    "        # Create a summary of classification performance\n",
    "        classification_summary = {}\n",
    "        \n",
    "        # Average accuracy by type\n",
    "        if research_area_stats:\n",
    "            values = [s['accuracy'] for s in research_area_stats.values() if s['total'] >= 5]\n",
    "            if values:\n",
    "                classification_summary['Research Areas'] = np.mean(values)\n",
    "                \n",
    "        if science_keyword_stats:\n",
    "            values = [s['accuracy'] for s in science_keyword_stats.values() if s['total'] >= 5]\n",
    "            if values:\n",
    "                classification_summary['Science Keywords'] = np.mean(values)\n",
    "                \n",
    "        if division_stats:\n",
    "            values = [s['accuracy'] for s in division_stats.values() if s['total'] >= 5]\n",
    "            if values:\n",
    "                classification_summary['Divisions'] = np.mean(values)\n",
    "        \n",
    "        if classification_summary:\n",
    "            names = list(classification_summary.keys())\n",
    "            values = list(classification_summary.values())\n",
    "            \n",
    "            bars = axs[1, 0].bar(names, values)\n",
    "            axs[1, 0].set_title('Average Accuracy by Classification Type')\n",
    "            axs[1, 0].set_xlabel('Classification Type')\n",
    "            axs[1, 0].set_ylabel('Average Accuracy')\n",
    "            axs[1, 0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            \n",
    "            # Add labels\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                axs[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                              f'{height:.2f}', ha='center', va='bottom')\n",
    "        else:\n",
    "            axs[1, 0].text(0.5, 0.5, 'Insufficient classification data', \n",
    "                         ha='center', va='center', transform=axs[1, 0].transAxes)\n",
    "    else:\n",
    "        axs[1, 0].text(0.5, 0.5, 'No classification data available', \n",
    "                     ha='center', va='center', transform=axs[1, 0].transAxes)\n",
    "    \n",
    "    # Bottom right: Micro-average performance metrics\n",
    "    micro_metrics = {\n",
    "        'Precision': micro_precision,\n",
    "        'Recall': micro_recall,\n",
    "        'F1 Score': micro_f1\n",
    "    }\n",
    "    \n",
    "    bars = axs[1, 1].bar(micro_metrics.keys(), micro_metrics.values())\n",
    "    axs[1, 1].set_title('Overall Performance Metrics')\n",
    "    axs[1, 1].set_xlabel('Metric')\n",
    "    axs[1, 1].set_ylabel('Score')\n",
    "    axs[1, 1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axs[1, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                      f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path_base}_summary.png\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Save all metrics to JSON\n",
    "    with open(f\"{output_path_base}_complete.json\", 'w') as f:\n",
    "        # Convert NumPy values to Python types for JSON serialization\n",
    "        def convert_numpy(obj):\n",
    "            if isinstance(obj, np.integer):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, np.floating):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "            elif isinstance(obj, list):\n",
    "                return [convert_numpy(i) for i in obj]\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "        json.dump(convert_numpy(complete_metrics), f, indent=2)\n",
    "    \n",
    "    print(f\"Comprehensive metrics visualizations saved with base path: {output_path_base}\")\n",
    "    \n",
    "    # Return all metrics for further analysis\n",
    "    return complete_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7401e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Starting optimized science publication classifier...\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Initialize classifier\n",
    "    print(\"Initializing science classifier...\")\n",
    "    with torch.inference_mode():\n",
    "        science_classifier = ScienceClassifier.get_instance()\n",
    "    print(\"Science classifier initialized successfully.\")\n",
    "    \n",
    "    # Load configuration files\n",
    "    print(\"Loading configuration files...\")\n",
    "    curated_mapping = load_curated_models('./curated_publications.json')\n",
    "    model_keywords = load_model_keywords('./model_keywords.json')\n",
    "    model_descriptions = load_model_descriptions('./model_descriptions.json')\n",
    "    \n",
    "    # Initialize ranker\n",
    "    print(\"Initializing relevance ranker...\")\n",
    "    with torch.inference_mode():\n",
    "        ranker = RelevanceRanker(model_descriptions)\n",
    "    \n",
    "    # Initialize model embeddings\n",
    "    print(\"Initializing model embeddings...\")\n",
    "    with torch.inference_mode():\n",
    "        model_embeddings = initialize_model_embeddings(model_descriptions)\n",
    "    \n",
    "    # Clean memory\n",
    "    optimize_memory()\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test publications...\")\n",
    "    try:\n",
    "        with open('./labeled_test_data_plusmomo.json') as f:\n",
    "            test_data = json.load(f)\n",
    "        print(f\"Loaded {len(test_data)} test publications.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test data: {str(e)}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize context manager\n",
    "    print(\"Building context validation profiles...\")\n",
    "    with open('./curated_publications.json') as f:\n",
    "        full_curated = json.load(f)\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        context_manager = ModelContextManager(full_curated)\n",
    "    \n",
    "    # Process publications\n",
    "    print(\"\\nProcessing publications...\")\n",
    "    results = process_publication_batch(\n",
    "        test_data, \n",
    "        curated_mapping, \n",
    "        model_keywords, \n",
    "        model_embeddings, \n",
    "        science_classifier,\n",
    "        context_manager,\n",
    "        ranker=ranker\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    print(\"Saving results to results.json...\")\n",
    "    with open('results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Generate visualizations with threshold analysis\n",
    "    print(\"\\nGenerating visualizations with threshold analysis...\")\n",
    "    metrics = visualize_metrics(results)\n",
    "    \n",
    "    # Show TF-IDF model-specific terms\n",
    "    print(\"\\nModel-specific terminology based on TF-IDF analysis:\")\n",
    "    model_specific_terms = context_manager.get_model_specific_terms()\n",
    "    for model, terms in model_specific_terms.items():\n",
    "        print(f\"\\n{model} distinctive terms:\")\n",
    "        print(\", \".join(terms[:10]))  # Show top 10 terms\n",
    "    \n",
    "    # Find optimal thresholds\n",
    "    print(\"\\nFinding optimal thresholds...\")\n",
    "    optimal_thresholds = find_optimal_thresholds(results)\n",
    "    \n",
    "    print(\"\\nOptimal model-specific thresholds:\")\n",
    "    for model, data in optimal_thresholds['per_model'].items():\n",
    "        current = MODEL_THRESHOLDS.get(model, 0.4)\n",
    "        print(f\"{model}: {data['threshold']:.2f} (current: {current:.2f}, F1: {data['f1']:.2f})\")\n",
    "    \n",
    "    print(f\"\\nOptimal overall threshold: {optimal_thresholds['overall']['threshold']:.2f}\")\n",
    "    print(f\"Overall F1 score with optimal thresholds: {optimal_thresholds['overall']['f1']:.3f}\")\n",
    "    \n",
    "    # Compare performance with current vs. optimal thresholds\n",
    "    print(\"\\nComparing performance with current vs. optimal thresholds:\")\n",
    "    current_performance = analyze_threshold_performance(results)\n",
    "    optimal_performance = analyze_threshold_performance(\n",
    "        results, \n",
    "        model_thresholds=optimal_thresholds['model_thresholds'],\n",
    "        overall_threshold=optimal_thresholds['overall']['threshold']\n",
    "    )\n",
    "    \n",
    "    print(f\"Current F1: {current_performance['overall']['f1']:.3f}, \" + \n",
    "          f\"Optimal F1: {optimal_performance['overall']['f1']:.3f}, \" +\n",
    "          f\"Improvement: {(optimal_performance['overall']['f1'] - current_performance['overall']['f1']) * 100:.1f}%\")\n",
    "    \n",
    "    # Report completion\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nProcessing completed in {total_time:.2f}s ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"Average time per publication: {total_time/len(test_data):.4f}s\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Peak GPU memory usage: {torch.cuda.max_memory_allocated(0) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb2587",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
